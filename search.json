[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I’m Mat Miller, a Manager and Data Engineering Tech Lead in the Engineering and Construction industry. I earned my engineering degree from Purdue University in 2009 and began my career working on large energy and industrial projects. I’ve always been passionate about technology, so I shifted my focus to building technology solutions to make projects more efficient and deliver better outcomes.\nWith experience spanning both business operations and technology, I am able to effectively bridge the gap between both worlds. This allows me to anticipate potential problems, catch missed requirements early, and avoid miscommunication between business operations and the development team. The result is delivering technology solutions more rapidly with higher quality that better address the business needs.\nAfter recognizing the transformative potential of AI in 2016, I started focusing on the field and enrolled in the fast.ai course. I have since completed several AI projects and participated in Kaggle competitions. With a strong foundation in AI, I am eager to help develop solutions that address business needs, leveraging recent technological advancements, while ensuring they remain realistic based on the current state of the technology.\nLet me know if you’d like to discuss how my skills could benefit your projects and initiatives."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html",
    "href": "posts/2023-06-10_transformers/notebook.html",
    "title": "Transformers From Scratch",
    "section": "",
    "text": "In this blog we’re going to walk through creating and training a transformer from scratch. We’ll go through each foundational element step by step and explain what is happening along the way. This blog is written in a Jupyter notebook which you can download and use to run the code yourself as you follow along. Running the code as you follow along and changing it to see how the output changes will help you learn the concepts better than reading alone. While this is a lengthy topic, please don’t be too alarmed with the length of the notebook or the amount of code. Most of it is copied from previous cells as we build up the transformer. Rather than just showing the code that was changed which would have shortened things up considerably, I chose to copy all required code down to the next cell to allow this entire notebook to be run from top to bottom. This should make it easier to run as well as allow you to experiment with each new concept as we go.\nI’ll be closely following Andrej Karpathy’s YouTube video ‘Let’s build GPT: from scratch, in code, spelled out.’. We’ll be heavily referencing the Attention Is All You Need paper as we go. If you would rather you can download the final version of the code from Andrej’s repo. The dataset we’ll be using for this can downloaded here. You may also find referencing Andrej’s nanoGPT repo helpful as well."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#initial-code-setup",
    "href": "posts/2023-06-10_transformers/notebook.html#initial-code-setup",
    "title": "Transformers From Scratch",
    "section": "7.1 Initial Code Setup",
    "text": "7.1 Initial Code Setup\nTo start with we’re going to modify our BigramLanguageModel to be a TransformerLanguageModel class.\n\nembedding_dim = 32 #The vector size of the token embeddings. Andrej used n_embed as the variable name.\n\nWe’re going to add an embedding dimension, change up the token embedding table and modify the token embedding lookup and logits calculation as we work towards modifying this class into a true transformer. We’ll iteratively test as we go to make sure it is still able to train correctly. Please read through the below code taking note of the comments explaining the changes being made.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size=vocab_size, embedding_dim=embedding_dim):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        #A basic linear layer to pass our token embeddings through. This is a preliminary step, not the final network.\n        #Note the input size in the embedding_dim and the output size is the number of tokens or vocab size.\n        #This is because we are going to be predicting the probability for every token in the vocab that it is the next token.\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n\n        #This will be our lookup table for all of the token embeddings. We'll have an entry for each token (aka vocab size)... \n        #...and each embedding will be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel - Lookup token embeddings\n        logits = self.language_model_head_linear_layer(token_embeddings) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Get predictions\n            logits, loss = self(idx) #This is calling `forward`\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are the token indicies (int).\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx #TODO: Stopped Here        \n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.3278) Validation Loss: tensor(4.3231)\nStep: 200 Training Loss: tensor(2.5421) Validation Loss: tensor(2.5626)\nStep: 400 Training Loss: tensor(2.4982) Validation Loss: tensor(2.5163)\nStep: 600 Training Loss: tensor(2.4936) Validation Loss: tensor(2.5354)\nStep: 800 Training Loss: tensor(2.4983) Validation Loss: tensor(2.5067)\nStep: 1000 Training Loss: tensor(2.5025) Validation Loss: tensor(2.5045)\nStep: 1200 Training Loss: tensor(2.4831) Validation Loss: tensor(2.5028)\nStep: 1400 Training Loss: tensor(2.4866) Validation Loss: tensor(2.5157)\nStep: 1600 Training Loss: tensor(2.4927) Validation Loss: tensor(2.5120)\nStep: 1800 Training Loss: tensor(2.4899) Validation Loss: tensor(2.5120)\nStep: 2000 Training Loss: tensor(2.4804) Validation Loss: tensor(2.5071)\nStep: 2200 Training Loss: tensor(2.4841) Validation Loss: tensor(2.5178)\nStep: 2400 Training Loss: tensor(2.4940) Validation Loss: tensor(2.4883)\nStep: 2600 Training Loss: tensor(2.4956) Validation Loss: tensor(2.5065)\nStep: 2800 Training Loss: tensor(2.4799) Validation Loss: tensor(2.5138)\nStep: 2999 Training Loss: tensor(2.4870) Validation Loss: tensor(2.5165)\n\n\n\nCExthy bridcowindakis s, bth\n\nHAPORDurayoule.\nS:\nO:\nIS:\nThachangs ar bthar usqur, vethar dilasoate arche my.\n\nHD:\n\nYom o mur\nYowhthetof isth bot mil ndill, bes ireeesenghin lat Heridrovets, and Win nghir.\nThabousel lind me l.\nHAser ce wiry ptupr aisspllwhy.\nHAntoul noroopetelaves\nMPOLI swod mothakleo Windo whth eiiby we ath dourive wee, ired t so mo she te\n\nAN ad nterurt f sor; irist m:\n\nThiny aleronth, af Pre?\n\nWISo myay INouk!\nKENoby sarardsal thes ghesthinin cour ay aney RDUES:\nI fr t ce.\nJ\nCPU times: user 17 s, sys: 438 ms, total: 17.4 s\nWall time: 17.4 s\n\n\nWe need to also encode the token position so we’ll need to add another embedding table for that which will be learned as well."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#building-up-to-self-attention",
    "href": "posts/2023-06-10_transformers/notebook.html#building-up-to-self-attention",
    "title": "Transformers From Scratch",
    "section": "7.2 Building Up To Self Attention",
    "text": "7.2 Building Up To Self Attention\nWe’ll go through the simple cumulative token average again using matrix multiplication and modify it over time to be self attention.\n\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\ntril\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\nweights = torch.zeros(T,T,dtype=torch.float)\nweights = weights.masked_fill(tril == 0, float('-inf'))\nweights = torch.softmax(weights, dim=-1)\nweights\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\n\nout = weights @ x\nout.shape\n\ntorch.Size([32, 8, 32])\n\n\nWhen using the cumulative mean the weights are fixed, but ideally we want the weights to variable and learned so each token can interact with other tokens a varying amount based on learned paramters of what is most important. Some tokens will find other tokens more interesting than others and we want that to be data dependent and learned through training.\nThe example Andrej gives is “maybe if I’m a vowel token I am more interested in past consonant tokens and I want the [consonant] data to flow to me, this is the problem that self attention solves”. The way that self attention solves this is that every single node or token will emit 2 vectors, a query and key vector. The query vector roughly represents “what am I looking for” and the key vector roughly represents “what do I contain”. The way we then get the affinities between each token is by performing a dot product (matrix multiplication) of all the query vectors against all of the key vectors. So for a given query vector, the dot product is calculated against all of the key vectors and the results of that become the weights that are used for each token. This is the weight variable we used above except now instead of being a fixed average, it varies per token and is learned. If the key and query are aligned they will produce a larger value when the dot product is taken between them which leads to a larger value in the weight matrix.\nLet’s take the above example and modify it to implement self attention.\nFirst we need to define our head size. We will use 16. This will be the side dimension of a matrix where each query and key vector are matrix multiplied together. To get the query and key vectors from the token embeddings we first need to run the token embedding through a linear layer for the query and key which will generate a vector of size head_size.\n\n#version 4: self attention\n#setup\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\n\nhead_size = 16 #Self attention head size\n\n#Learned vector to extract key vector from token embedding vector\nkey_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract query vector from token embedding vector\nquery_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Extract query and key values for every token in the batch in parallel\nkey = key_layer(x) # (B,T,head_size)\nquery = query_layer(x) # (B,T,head_size) #TODO:\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\nNow we will calculate the affinities (weights) between each token in each sequence by matrix multiplying all of the queries and keys. If we simply try to calculate query @ key it will fail because the shapes are not correct to be able to do matrix multiplication. In our case both key and query are of shape (B,T,head_size) which are incompatible shapes to be matrix multiplied together. We need to transpose, or rotate, the key in the T and head_size dimension so they can be matrix multiplied. We cannot simply use the .T transpose because it would transpose in the batch dimension as well which we do not want so instead we’ll specify which dimensions to transpose which we can do by calling key.transpose(-2, -1) which will transpose the last 2 dimensions.\n\n#Calculate affinity (weights)\n#(B,T,head_size) @ (B, head_size, T) which is (32,8,16) @ (32,16,8) -&gt; (B, T, T) which is (32,8,8)\nweights = query @ key.transpose(-2, -1) \nweights.shape\n\ntorch.Size([32, 8, 8])\n\n\n\nweights[0]\n\ntensor([[ 0.2746, -1.2489,  0.5325,  0.2690,  0.1749, -0.7806,  0.1727,  1.3460],\n        [ 1.1833,  1.3634,  0.8982, -0.2749, -0.6459, -0.9106, -0.1111, -1.6019],\n        [-0.5018, -0.0807, -1.0500, -0.7615,  0.3193, -0.1342,  0.3969, -1.8405],\n        [ 0.5840,  0.6687, -0.5924,  0.4017,  0.3058, -0.6051, -0.0642, -0.6784],\n        [-0.2627, -2.5447,  1.5607, -1.5866, -0.6412, -1.3504, -0.0761, -2.3587],\n        [-0.8941, -2.0453, -1.5232,  1.5689, -0.5483,  1.9307, -0.6665, -1.7059],\n        [-0.2565,  0.5645, -0.7255, -0.2768, -0.3469,  0.7811, -0.5242, -0.9621],\n        [ 0.7061,  1.1336,  0.4749,  0.7493,  0.5897,  1.3509,  0.7673, -0.4065]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNow we have weights that are calculated based on each token’s affinity to every other token. We then apply the same filtering that we did previously with our cumulative mean so we simply remove the line where the weights were set to zero. This will allow us to finally apply a learned weighting to each previous token embedding.\n\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\n\n# weights = torch.zeros(T,T,dtype=torch.float) #Removed now that weights are calculated\nweights = weights.masked_fill(tril == 0, float('-inf')) #Masks future tokens\nweights = torch.softmax(weights, dim=-1) #Provides even distribution (weights that add up to 1)\n\nout = weights @ x\nout.shape\n\ntorch.Size([32, 8, 32])\n\n\nYou can see the weights below. Notice they are no longer uniform. They can now be indivual and learned from the data.\n\nweights[0] #TODO\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4551, 0.5449, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3224, 0.4912, 0.1864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3096, 0.3370, 0.0955, 0.2580, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1213, 0.0124, 0.7510, 0.0323, 0.0831, 0.0000, 0.0000, 0.0000],\n        [0.0314, 0.0099, 0.0167, 0.3685, 0.0444, 0.5291, 0.0000, 0.0000],\n        [0.1066, 0.2423, 0.0667, 0.1045, 0.0974, 0.3009, 0.0816, 0.0000],\n        [0.1168, 0.1792, 0.0927, 0.1220, 0.1040, 0.2227, 0.1242, 0.0384]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNow each token will be able to calculate its affinity to all other tokens. You can see in the example by looking at the bottom row, that the 8th token has a high affinity for the 6th token because it has the highest value: \n[0.1168, 0.1792, 0.0927, 0.1220, 0.1040,0.2227, 0.1242, 0.0384]\nThere is one more part of self attention we need to look at. That is that when we aggregate the tokens out = weights @ x we don’t aggregate the tokens exactly, we aggregate the value, so in the same way that we calculate key and query via passing the token embedding through a linear layer, we will do the same to get the value.\n\n#version 4: self attention\n#setup\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\n\nhead_size = 16 #Self attention head size\n\n#Learned vector to extract key vector from token embedding vector\nkey_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract query vector from token embedding vector\nquery_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract value vector from token embedding vector\nvalue_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) #NEW\n\n#Extract query, key and value for every token in the batch in parallel\nkey = key_layer(x) # (B,T,head_size)\nquery = query_layer(x) # (B,T,head_size)\nvalue = value_layer(x) # (B,T,head_size) #NEW\n\n#Calculate affinity (weights)\n#(B,T,head_size) @ (B, head_size, T) which is (32,8,16) @ (32,16,8) -&gt; (B, T, T) which is (32,8,8)\nweights = query @ key.transpose(-2, -1) \nweights.shape\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\ntorch.Size([32, 8, 8])\n\n\nAnd now instead of calculating the output by matrix multiplying the weights by x we multiply the weights by value.\n\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\n\nweights = weights.masked_fill(tril == 0, float('-inf')) #Masks future tokens\nweights = torch.softmax(weights, dim=-1) #Provides even distribution (weights that add up to 1)\n\n# out = weights @ x\nout = weights @ value #NEW (B, T, T) @ (B, T, head_size) = (B, T, head_size)\nout.shape\n\ntorch.Size([32, 8, 16])\n\n\nNotice how the shape of out has changed from torch.Size([32, 8, 32]) to torch.Size([32, 8, 16]) now that the we are using value which is of length 16 instead of the token embedding x which was of length 32.\nYou can think of the token embedding x as private information of the token and it must be passed through the linear layers to get the query, key and value. You can think of it as the token embedding x has all the information about the token and:\nquery: represents the things that the token is interested in or wants.\nkey: represents the things the token has.\nvalue: represents, if you find the token interesting, the information the token wants to communicate.\nAdditional Notes on Attention: link 1) Attention is a communication mechanism. You can think of it as if you had nodes in a directed graph:\n\nEach node has a vector of information (token embedding) and it gets a weighted sum of all of the nodes that point to it. This is done in a data dependent manner, so it depends on what data is stored in each node at any point in time. Our graph does not look quite like the example. Instead, our graph has 8 nodes. The first node is pointed to by only itself, our second node is pointed to by the first node and itself, our third node is pointed to by our first and second nodes as well as itself and so on. This structure is common in auto-regressive scenarious. \nAuto-regressive in this context refers to a type of model that generates sequences by modeling the probability of each item in the sequence given its preceding items. In other words, autoregressive language models generate predictions step by step, where each prediction is dependent on the ones that came before it.\nIn principal attention can be applied to any arbitrary directed graph as it is just a communication mechanism between nodes.\n\nThere is no notion of space or position. Attention simply acts over a set of vectors in this graph. The nodes have no idea of where they are positioned within the space which is why we need to encode them positionally which gives them information that anchors them to a specific position. i.e. inherently the nodes, representing characters in our example, don’t know what position they occur in relative to the other nodes which is why we need to positionally encode them. You can contrast this with convolutional neural networks where the data and network inherently are modeled spatially. For example CNN’s are regularly used for computer vision applications. In these applications adjacent pixels are fed into the CNN where convolutional filters act in space preserving the spatial information about the data.\n\nAttention, in contrast with CNN’s, has no notion of space, so space or position or location need to be encoded into the nodes through some other mechanism, which in our case is a positional encoding vector. This position vector will be added to the token prior to it being processed through the linear layers.\nAdditional Notes: link * Each example across batch dimensions is processed completely independently. Information from an item in a batch does not affect information in another item within the batch. Different items within a batch never talk to eachother. * In an encoder network (block), you do not filter out future tokens, only in a decoder network. This means that in an encoder network, these lines from our previous example would be removed:\ntril = torch.tril(torch.ones(T,T,dtype=torch.long)) \nweights = weights.masked_fill(tril == 0, float('-inf'))\n\nThere are many instances where you want all of the nodes to talk to each other, such as in sentiment analysis for example, because later on in the network you are making a simple prediction on whether the text is positive or negative. Another example would be vision transformers where you want all image patches to talk to each other. In these instances you use an encoder block which does not have masking in contrast to the decoder block which is what we have been focusing on here. * There are different types of attention. What we’re looking at now is self attention. The reason this is self-attention is because the data comes from the same source (x). Attention can be much more general than self attention, in that the source of the data can be from a different source. For example in encoder decoder networks, the queries could be produced from x but the the keys and values could come from a completely different source, for example from different encoder blocks that we would want to condition on. A real world example of this could be translating from one language to another, where the original or input language comes from an separate encoder network. The encoder network provides the keys and values and the decoder network provides the queries. This is called cross attention and is where there is a separate set of nodes we would like to pull information from into our node. Self attention, again, is where we are pulling keys, queries and values from one set of nodes.\n\nSo far we’ve implemented most of the attenion equation from the original Attention is all you need paper. \nAttention(Q,K,V) = softmax((Q*K^T)/(sqrt(dk))*V Where: Q = Query, K = Key, V = Value, dk = dimension of the Q and K or ‘head’.\nThe piece we are missing is dividing by sqrt(dk) which makes this ‘scaled self attention’. To do this we need to divide weights by sqrt(dk) or the dimension of the Q,K head. This makes it so when Q,K are unit variance, weights will be unit variance too which is important so softmax will remain diffused and not be saturated too much, i.e. the dot products betweek Q and K can become very large which pushes the gradients through the softmax to become very small which negatively impact training. This is why we want to scale them first before taking the softmax.\nLet’s look at a real example of this:\nWhere q and k are a gausian or normal distributions so the mean of the values is 0 and the standard deviation is 1. When you compute the matrix multiplication between them you will notice that the variance of weights is quite high.\n\ntorch.manual_seed(TORCH_SEED)\nq = torch.randn(B,T,head_size)\nk = torch.randn(B,T,head_size)\nprint('Mean of q:',q.mean(),'Variance of q:',q.var(),'Mean of k:',k.mean(),'Variance of k:',k.var())\nweights = q @ k.transpose(-2,-1)\nprint('Shape of weights:',weights.shape,'Mean of weights:',weights.mean(),'Variance of weights:',weights.var(),\n      '\\nMin of weights:',weights.min(),'Max of weights:',weights.max())\n\nMean of q: tensor(0.0021) Variance of q: tensor(0.9985) Mean of k: tensor(-0.0129) Variance of k: tensor(1.0255)\nShape of weights: torch.Size([32, 8, 8]) Mean of weights: tensor(-0.0302) Variance of weights: tensor(17.3386) \nMin of weights: tensor(-16.3490) Max of weights: tensor(13.1295)\n\n\n\nweights[0,0]\n\ntensor([-3.9763,  1.5713, -1.4034, -2.8632,  4.2487,  1.1146, -6.5130, -2.7662])\n\n\nNow if you divide the dot product of q and k by the square root of the head_size you can see that it returns the variance of weights back to 1 instead of approximately 17 prior to scaling.\n\nimport math\nweights = (q @ k.transpose(-2,-1)) / math.sqrt(head_size) #TODO Output size is (B,T,T) (32,8,8)\nprint('Shape of weights:',weights.shape,'Mean of weights:',weights.mean(),'Variance of weights:',weights.var(),\n      '\\nMin of weights:',weights.min(),'Max of weights:',weights.max())\n\nShape of weights: torch.Size([32, 8, 8]) Mean of weights: tensor(-0.0075) Variance of weights: tensor(1.0837) \nMin of weights: tensor(-4.0872) Max of weights: tensor(3.2824)\n\n\n\nweights[0,0]\n\ntensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])\n\n\nWe’ll create a very basic function to plot the tensors to help visualize the results.\n\nimport matplotlib.pyplot as plt\ndef plot_1d_tensor(x):\n    print(x)\n    plt.bar(range(len(x)), x)\n\nAgain, the reason that scaling weights is important is because of the subsequent softmax that is applied. When large values are input into softmax it causes the gradients to be small and the output of the softmax to converge toward one-hot vectors. First we’ll start out with one of the example weights that has already been divided by math.sqrt(head_size).\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915]), dim=-1))\n\ntensor([0.0465, 0.1862, 0.0885, 0.0614, 0.3636, 0.1661, 0.0247, 0.0630])\n\n\n\n\n\n\n\n\n\nYou can see the the output of softmax here is diffuse. None of the output values are overly large or small. If you multiply these same values by math.sqrt(head_size), effectively undoing scaling we applied, you will see that the results after softmax are less evenly distributed or diffuse.\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])*math.sqrt(head_size), dim=-1))\n\ntensor([2.3960e-04, 6.1487e-02, 3.1406e-03, 7.2937e-04, 8.9464e-01, 3.8940e-02,\n        1.8965e-05, 8.0382e-04])\n\n\n\n\n\n\n\n\n\nIf you push it even further you can see that the second item in the vector continues to grow even though the value of each element, relative to eachother has not changed.\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])*head_size, dim=-1))\n\ntensor([5.1446e-15, 2.2311e-05, 1.5187e-10, 4.4175e-13, 9.9997e-01, 3.5890e-06,\n        2.0192e-19, 6.5168e-13])\n\n\n\n\n\n\n\n\n\nAs the input values to the softmax continue to grow the result of the softmax continues to converge to a one-hot encoded vector, which is where one of the values in the vector is 1 and all the rest are 0’s. In effect this would make it so 1 node will only draw information from one other node, which is generally not what we want. This is especially a problem during initialization of the network before training, as it can be difficult for the network to recover from this during training."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#continuing-model-definition",
    "href": "posts/2023-06-10_transformers/notebook.html#continuing-model-definition",
    "title": "Transformers From Scratch",
    "section": "7.3 Continuing model definition",
    "text": "7.3 Continuing model definition\nNow we’re going to create a Head module where we’ll implement a single self attention head which we’ll use in our transformer, replacing the bigram model. You can reference the video link here to follow along if you would like.\n\nclass Head(nn.Module):\n    \"\"\" one self attention head \"\"\"\n    def __init__(self, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.head_size = head_size\n        self.context_length = context_length\n        self.key_layer   = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.query_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.value_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones((self.context_length, self.context_length))))\n    \n    def forward(self, x):\n        B,T,C = x.shape\n        assert T &lt;= self.context_length #check that x.shape matches pre-defined dims\n        assert C == self.embedding_dim\n        q = self.query_layer(x) #(B,T,C) (batch size, context length, head_size\n        k = self.key_layer(x) #(B,T,C)\n        v = self.value_layer(x) #(B,T,C)\n        \n        #compute scores based on affinities\n        weights = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 # (B,T,C) @ (B,C,T) -&gt; (B,T,T)\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) #(B,T,T)\n        weights = F.softmax(input=weights, dim=-1) #(B,T,T)\n        \n        #perform weighted aggragation of the values\n        out = weights @ v # (B,T,T) @ (B,T,C) -&gt; (B,T,C)\n        return out\n\n# Head()(x)\n\nThe register_buffer method is utilized to incorporate the tril matrix as a part of the model’s state. This integration ensures that tril is consistently saved and loaded with the model, maintaining uniform behavior across various runs and settings. Crucially, being a buffer, tril is excluded from gradient calculations and is not included as a parameter during model optimization, thereby rendering it a non-trainable component of the model.\nTo make visualizing the training loss easier we’ll create a simple function to plot them.\n\ndef plot_losses(losses):\n    train_losses = [o['train'] for o in losses if o.get('train') is not None]\n    valid_losses = [o['valid'] for o in losses if o.get('valid') is not None]\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(valid_losses, label='Validation Loss')\n    plt.ylabel('Loss')\n    plt.title('Losses')\n    plt.legend()\n    plt.show()\n\nNow we’ll add our new Head implementation to the TransformerLanguageModel class and train a model to ensure everything is working as well as to get a baseline of the results. Note we are also adding a token_position_embedding_table to encode the token positions. This learned looked up value will be added to the token_embeddings.\n\nlearning_rate = 1e-3 # decreate the learning rate because self attention cannot tolerate very high learning rates.\nmax_iters = 5000\n\n\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.self_attention_head_linear_layer = Head(head_size=head_size, embedding_dim=embedding_dim, context_length=context_length)\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.head_size, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.self_attention_head_linear_layer(x) #apply one head of self attention\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n        \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.1743) Validation Loss: tensor(4.1712)\nStep: 200 Training Loss: tensor(3.1199) Validation Loss: tensor(3.1343)\nStep: 400 Training Loss: tensor(2.8712) Validation Loss: tensor(2.8892)\nStep: 600 Training Loss: tensor(2.7071) Validation Loss: tensor(2.7260)\nStep: 800 Training Loss: tensor(2.6324) Validation Loss: tensor(2.6392)\nStep: 1000 Training Loss: tensor(2.5896) Validation Loss: tensor(2.5849)\nStep: 1200 Training Loss: tensor(2.5460) Validation Loss: tensor(2.5497)\nStep: 1400 Training Loss: tensor(2.5158) Validation Loss: tensor(2.5259)\nStep: 1600 Training Loss: tensor(2.5000) Validation Loss: tensor(2.5051)\nStep: 1800 Training Loss: tensor(2.4885) Validation Loss: tensor(2.4980)\nStep: 2000 Training Loss: tensor(2.4632) Validation Loss: tensor(2.4858)\nStep: 2200 Training Loss: tensor(2.4572) Validation Loss: tensor(2.4797)\nStep: 2400 Training Loss: tensor(2.4632) Validation Loss: tensor(2.4467)\nStep: 2600 Training Loss: tensor(2.4587) Validation Loss: tensor(2.4553)\nStep: 2800 Training Loss: tensor(2.4338) Validation Loss: tensor(2.4533)\nStep: 3000 Training Loss: tensor(2.4402) Validation Loss: tensor(2.4562)\nStep: 3200 Training Loss: tensor(2.4409) Validation Loss: tensor(2.4492)\nStep: 3400 Training Loss: tensor(2.4249) Validation Loss: tensor(2.4487)\nStep: 3600 Training Loss: tensor(2.4376) Validation Loss: tensor(2.4395)\nStep: 3800 Training Loss: tensor(2.4166) Validation Loss: tensor(2.4278)\nStep: 4000 Training Loss: tensor(2.4102) Validation Loss: tensor(2.4275)\nStep: 4200 Training Loss: tensor(2.4191) Validation Loss: tensor(2.4384)\nStep: 4400 Training Loss: tensor(2.4178) Validation Loss: tensor(2.4217)\nStep: 4600 Training Loss: tensor(2.4077) Validation Loss: tensor(2.4109)\nStep: 4800 Training Loss: tensor(2.4062) Validation Loss: tensor(2.4189)\nStep: 4999 Training Loss: tensor(2.4043) Validation Loss: tensor(2.4176)\n\n\n\n\n\n\n\n\n\n\nAnd thef tridcowind tis n, ber\n\nHiset bobe toe.\nS:\nO-' my dalatanss:\nWant he uw hathe.\nWar dthas ate awice my.\n\nHaldaru zorou wabuts, tof is hy me mil ndill, aes iree sen cin lat Het drovets, and Win ng:\nWilerabous lplind peallllishe onchiry:\nAugr aiss hawty.\n\n'Thake norodpeeelaves\nMomy.\nWhod mothake onWindo whe Ceiiby, wout, fourive wees ired thoous\nAr-x's uhe kad nterthirf so;\nAngis! m:\nE nge male ont ffaf Pre?\n\nWISo myat houre!\n\nWidby ak\nSadsal thes ghe thidin cour ay aney Iry ts chan th voul\n\n\nNext we’ll add multi-head attention which is just computing multiple attention heads together in parallel and then concatenating the results. \n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads:int, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        \n        self.heads = nn.ModuleList([\n            Head(head_size=self.head_size, embedding_dim=self.embedding_dim, context_length=self.context_length) \n            for _ in range(self.num_heads)\n        ])\n        \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1) #Note the concat is in the last 'C' dimension =&gt; (B,T,C*num_heads)\n            \n\nNow let’s add our newly created multi-head attention back into our Model.\n\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        # self.self_attention_head_linear_layer = Head(head_size=head_size, embedding_dim=embedding_dim, context_length=context_length)\n        #4 heads of 8 dimensional self attention.\n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        # x = self.self_attention_head_linear_layer(x) #apply one head of self attention\n        x = self.multi_self_attention_heads_layer(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2248) Validation Loss: tensor(4.2250)\nStep: 200 Training Loss: tensor(3.0112) Validation Loss: tensor(3.0132)\nStep: 400 Training Loss: tensor(2.7330) Validation Loss: tensor(2.7487)\nStep: 600 Training Loss: tensor(2.6190) Validation Loss: tensor(2.6244)\nStep: 800 Training Loss: tensor(2.5537) Validation Loss: tensor(2.5700)\nStep: 1000 Training Loss: tensor(2.5222) Validation Loss: tensor(2.5220)\nStep: 1200 Training Loss: tensor(2.4785) Validation Loss: tensor(2.4870)\nStep: 1400 Training Loss: tensor(2.4509) Validation Loss: tensor(2.4563)\nStep: 1600 Training Loss: tensor(2.4205) Validation Loss: tensor(2.4278)\nStep: 1800 Training Loss: tensor(2.3966) Validation Loss: tensor(2.4144)\nStep: 2000 Training Loss: tensor(2.3658) Validation Loss: tensor(2.3828)\nStep: 2200 Training Loss: tensor(2.3729) Validation Loss: tensor(2.3910)\nStep: 2400 Training Loss: tensor(2.3579) Validation Loss: tensor(2.3466)\nStep: 2600 Training Loss: tensor(2.3544) Validation Loss: tensor(2.3499)\nStep: 2800 Training Loss: tensor(2.3267) Validation Loss: tensor(2.3427)\nStep: 3000 Training Loss: tensor(2.3259) Validation Loss: tensor(2.3410)\nStep: 3200 Training Loss: tensor(2.3180) Validation Loss: tensor(2.3313)\nStep: 3400 Training Loss: tensor(2.3070) Validation Loss: tensor(2.3142)\nStep: 3600 Training Loss: tensor(2.3024) Validation Loss: tensor(2.3078)\nStep: 3800 Training Loss: tensor(2.2728) Validation Loss: tensor(2.3038)\nStep: 4000 Training Loss: tensor(2.2630) Validation Loss: tensor(2.2855)\nStep: 4200 Training Loss: tensor(2.2825) Validation Loss: tensor(2.2850)\nStep: 4400 Training Loss: tensor(2.2734) Validation Loss: tensor(2.2868)\nStep: 4600 Training Loss: tensor(2.2629) Validation Loss: tensor(2.2753)\nStep: 4800 Training Loss: tensor(2.2425) Validation Loss: tensor(2.2706)\nStep: 4999 Training Loss: tensor(2.2440) Validation Loss: tensor(2.2609)\n\n\n\n\n\n\n\n\n\n\nAnd they tridcowd,\nThis so be madises bube to tavegr-'t theall ands:\nWant he us hat tot?\nWedtlas anes wice my.\n\nHDER:\nAt onoth\nYouts, tof is hy me mil nowlit,\nWheirwe sen cin lat Het drov the and the nown iserans!\n lolind teall thus, cocrivy prugh aiss hewty.\nHllings kne\nTo thig I whom.\n\nWhoul to ake onWinso whre piiby we atit,\nCrive winghience poo mo the thu the danterupt fis are;\nDe! muf thre male of,\nTo fis.\nFe I So myakny, be!\nWhied is:\nSadsal the E'd st huin couk ay andy Iry to cof my carey\n\n\nAs you can see there is quite an improvement in the loss, going from Validation Loss: tensor(2.4176) with a single attention head to Validation Loss: tensor(2.2609) with our multi-attention head that has 4 heads. Note, these losses may vary somewhat between training runs. The results are still nonsense, but are looking closer to the training text than previous attempts. The reason that multi-headed attention works better than a single self attention block is that it is helpful to have multiple communication channels between tokens so they can each be looking for different things over different communication channels. As an example one communication channel make be looking back at vowels or consonants while another might be looking for the previous space.\nIf you look at this transformer block diagram, you can see that we’ve implemented quite a bit of it so far. \nWe’ve implemented the output embeddings, positional embeddings, (the lower) masked multi-head attention, and the final linear and softmax layers. We are going to skip the multi-head attention block in the middle as that is only needed if your model has an encoder block, which ours does not. This leaves the feed forward network to implement which is just a simple multi layer perceptron. In addition the entire block between the positional encodings and final linear layer can be stacked on top of itself multiple times signified by Nx.\nHere is the equation for the feed forward network, which is a simple multi layer perceptron:  \n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"A simple linear network followed by a non-linearity\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        return self.ffn(x)\n\nNote: In the equation it defines a (linear layer), (relu), and (linear layer). We’ll add the final linear layer later. Now let’s add our FFN to our Transformer Model.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        #4 heads of 8 dimensional self attention.\n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.multi_self_attention_heads_layer(x) # (B,T,C)\n        x = self.feed_forward_network(x) # (B,T,C) NEW\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2022) Validation Loss: tensor(4.2019)\nStep: 200 Training Loss: tensor(2.9494) Validation Loss: tensor(2.9685)\nStep: 400 Training Loss: tensor(2.6759) Validation Loss: tensor(2.6864)\nStep: 600 Training Loss: tensor(2.5779) Validation Loss: tensor(2.5799)\nStep: 800 Training Loss: tensor(2.5171) Validation Loss: tensor(2.5197)\nStep: 1000 Training Loss: tensor(2.4739) Validation Loss: tensor(2.4704)\nStep: 1200 Training Loss: tensor(2.4210) Validation Loss: tensor(2.4257)\nStep: 1400 Training Loss: tensor(2.4079) Validation Loss: tensor(2.4105)\nStep: 1600 Training Loss: tensor(2.3843) Validation Loss: tensor(2.3845)\nStep: 1800 Training Loss: tensor(2.3682) Validation Loss: tensor(2.3731)\nStep: 2000 Training Loss: tensor(2.3387) Validation Loss: tensor(2.3475)\nStep: 2200 Training Loss: tensor(2.3342) Validation Loss: tensor(2.3500)\nStep: 2400 Training Loss: tensor(2.3180) Validation Loss: tensor(2.3127)\nStep: 2600 Training Loss: tensor(2.3176) Validation Loss: tensor(2.3160)\nStep: 2800 Training Loss: tensor(2.2881) Validation Loss: tensor(2.3087)\nStep: 3000 Training Loss: tensor(2.2834) Validation Loss: tensor(2.3059)\nStep: 3200 Training Loss: tensor(2.2796) Validation Loss: tensor(2.2901)\nStep: 3400 Training Loss: tensor(2.2719) Validation Loss: tensor(2.2743)\nStep: 3600 Training Loss: tensor(2.2675) Validation Loss: tensor(2.2681)\nStep: 3800 Training Loss: tensor(2.2428) Validation Loss: tensor(2.2751)\nStep: 4000 Training Loss: tensor(2.2294) Validation Loss: tensor(2.2524)\nStep: 4200 Training Loss: tensor(2.2468) Validation Loss: tensor(2.2545)\nStep: 4400 Training Loss: tensor(2.2373) Validation Loss: tensor(2.2437)\nStep: 4600 Training Loss: tensor(2.2310) Validation Loss: tensor(2.2448)\nStep: 4800 Training Loss: tensor(2.2182) Validation Loss: tensor(2.2522)\nStep: 4999 Training Loss: tensor(2.2135) Validation Loss: tensor(2.2291)\n\n\n\n\n\n\n\n\n\n\nWher bef bridcowf,\nThe lay ble\nbairet bube to tave O-' my dalllauss:\nWant he us he hertbar dilth anes with my thand a wizorm he offs, to fit her! Varl nowlit,\nWheiree sen cin lat Heacliov the and the nown!\nFerablesel lind teall thull cechir speave aiss hewty.\nHETBHUSIRCBETI:\nAlave whom\nIll, demet aklecal-'so wher piichs withe dour warce hidend thoouse the the the danderthirf son; igis! muf thre ifled at tise Pried my of.\n\nHKINGLER:\nWidby and adsal ther grest hoin cour ay aney Iry thel fronf veay\nCPU times: user 1min 15s, sys: 445 ms, total: 1min 15s\nWall time: 1min 15s\n\n\nOur loss has improved again from Validation Loss: tensor(2.2854) now to Validation Loss: tensor(2.2720) now that we’ve added the feed forward network.\nNext we need to create a Block module that incorporates everthing within the block on the transformer architecture diagram (grey box) which will then allow us to stack them.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        \n    def forward(self, x):\n        return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        \n\nNow we can add our new Transformer Block to our model and start stacking it.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        # self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4)\n        # self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n        ) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        # x = self.multi_self_attention_heads_layer(x) # (B,T,C)\n        # x = self.feed_forward_network(x) # (B,T,C)\n        x = self.transformer_blocks(x) #NEW\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2116) Validation Loss: tensor(4.2078)\nStep: 200 Training Loss: tensor(3.2643) Validation Loss: tensor(3.2907)\nStep: 400 Training Loss: tensor(3.1541) Validation Loss: tensor(3.1676)\nStep: 600 Training Loss: tensor(3.0360) Validation Loss: tensor(3.0239)\nStep: 800 Training Loss: tensor(2.8569) Validation Loss: tensor(2.8526)\nStep: 1000 Training Loss: tensor(2.7738) Validation Loss: tensor(2.7607)\nStep: 1200 Training Loss: tensor(2.6645) Validation Loss: tensor(2.6827)\nStep: 1400 Training Loss: tensor(2.6202) Validation Loss: tensor(2.6159)\nStep: 1600 Training Loss: tensor(2.5581) Validation Loss: tensor(2.5613)\nStep: 1800 Training Loss: tensor(2.5231) Validation Loss: tensor(2.5388)\nStep: 2000 Training Loss: tensor(2.5020) Validation Loss: tensor(2.5028)\nStep: 2200 Training Loss: tensor(2.4899) Validation Loss: tensor(2.4974)\nStep: 2400 Training Loss: tensor(2.4812) Validation Loss: tensor(2.4668)\nStep: 2600 Training Loss: tensor(2.4656) Validation Loss: tensor(2.4641)\nStep: 2800 Training Loss: tensor(2.4574) Validation Loss: tensor(2.4511)\nStep: 3000 Training Loss: tensor(2.4306) Validation Loss: tensor(2.4413)\nStep: 3200 Training Loss: tensor(2.4016) Validation Loss: tensor(2.4273)\nStep: 3400 Training Loss: tensor(2.3946) Validation Loss: tensor(2.3999)\nStep: 3600 Training Loss: tensor(2.3738) Validation Loss: tensor(2.3823)\nStep: 3800 Training Loss: tensor(2.3844) Validation Loss: tensor(2.3700)\nStep: 4000 Training Loss: tensor(2.3571) Validation Loss: tensor(2.3570)\nStep: 4200 Training Loss: tensor(2.3426) Validation Loss: tensor(2.3729)\nStep: 4400 Training Loss: tensor(2.3394) Validation Loss: tensor(2.3696)\nStep: 4600 Training Loss: tensor(2.3300) Validation Loss: tensor(2.3343)\nStep: 4800 Training Loss: tensor(2.3263) Validation Loss: tensor(2.3400)\nStep: 4999 Training Loss: tensor(2.3301) Validation Loss: tensor(2.3403)\n\n\n\n\n\n\n\n\n\n\nAnd thik bry cowd,\nThis bor thibe sou bobe to:\nave rud my thichanss:\nWarth fou qor, ve bar dilth afe aw cramy.\n\nHhy ar mereou waow somtof is he ce mil nowlincaes ireees, hein latiser lilv the and the non ond wans!\n\nAplind pealltliser cechiry: tur hais's, why hou to u nor\nTo thigh sond:\nIl wo to thake o Windo wher eiibk we ati dourive we hidend thoo mowr-x'd und kad nonrtf he sor; iris! mef thin inled,\nThe af Pre?\n\nKIS\nINUSH:\nNube!\n\nGiyd is:\nards beace Eghes bidin cou afar tey ir-ltome fronf ve y\nCPU times: user 2min 56s, sys: 421 ms, total: 2min 57s\nWall time: 2min 56s\n\n\nAs you can see the accuracy actually got worse. Given our new much more powerful model, this is not something that we want. As the depth of models increase they can become harder to train. Fortunately there are a few things that we can do about that. link\nFirst we can implement skip connections, also known as residual connections, which are depicted on the transformer architecture diagram as black lines that bypass the masked multi-head attention block and feed into the add and norm block. You can also see one bypassing the FFN. The idea for these originally came from deep residual networks paper. In this case we are going to add the input data back to the output of the blocks that are being skipped. When you use addition, the gradients are evenly distributed between both the skip branch and the block branch. An alternative that is sometimes used is a simple concatenation of the input and output of the skipped block. \nWhen we initialize the network before training we typically want to start off with very low weights for the branches that go through the blocks so the blocks contribute very little to the overall loss. This way the gradient signal makes its way through the entire network. Then during training the network will slowly increase the weights and participation of the blocks.\nNow let’s implement the skip connections in our TransfomerBlock module.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        \n    def forward(self, x):\n        # return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        x = x + self.multi_self_attention_heads_layer(x) # adding input back to the output of each block for skip connection. NEW\n        x = x + self.feed_forward_network(x) # adding input back to the output of each block for skip connection. NEW\n        return x\n        \n\nWe also need to add a projection layer to our MultiHeadAttention module as well as the feed forward network. This is a simple linear layer.\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads:int, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        \n        self.heads = nn.ModuleList([\n            Head(head_size=self.head_size, embedding_dim=self.embedding_dim, \n                 context_length=self.context_length) \n            for _ in range(self.num_heads)])\n        \n        self.projection_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=True) #NEW\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.projection_layer(out) #NEW\n        return out\n            \n\nIn the FFN rather than adding the same projection layer parameter we’ll simply just add an additional linear layer to the existing sequential module. Also we are going to fan out and then back in by a factor of 4 between the linear layers in the FFN to add additional computation.\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"A simple linear network followed by a non-linearity\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim*4),#Updated\n            nn.ReLU(),\n            nn.Linear(in_features=self.embedding_dim*4, out_features=self.embedding_dim) #NEW\n        )\n    def forward(self, x):\n        return self.ffn(x)\n\nNow let’s train the network again to see how we end up.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n        )\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.transformer_blocks(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.6328) Validation Loss: tensor(4.6313)\nStep: 200 Training Loss: tensor(2.5782) Validation Loss: tensor(2.5969)\nStep: 400 Training Loss: tensor(2.4491) Validation Loss: tensor(2.4365)\nStep: 600 Training Loss: tensor(2.3560) Validation Loss: tensor(2.3455)\nStep: 800 Training Loss: tensor(2.2816) Validation Loss: tensor(2.2922)\nStep: 1000 Training Loss: tensor(2.2414) Validation Loss: tensor(2.2609)\nStep: 1200 Training Loss: tensor(2.2245) Validation Loss: tensor(2.2473)\nStep: 1400 Training Loss: tensor(2.1878) Validation Loss: tensor(2.2126)\nStep: 1600 Training Loss: tensor(2.1557) Validation Loss: tensor(2.1949)\nStep: 1800 Training Loss: tensor(2.1444) Validation Loss: tensor(2.1952)\nStep: 2000 Training Loss: tensor(2.1448) Validation Loss: tensor(2.1569)\nStep: 2200 Training Loss: tensor(2.1297) Validation Loss: tensor(2.1741)\nStep: 2400 Training Loss: tensor(2.0952) Validation Loss: tensor(2.1558)\nStep: 2600 Training Loss: tensor(2.0832) Validation Loss: tensor(2.1392)\nStep: 2800 Training Loss: tensor(2.0740) Validation Loss: tensor(2.1216)\nStep: 3000 Training Loss: tensor(2.0602) Validation Loss: tensor(2.1131)\nStep: 3200 Training Loss: tensor(2.0669) Validation Loss: tensor(2.1428)\nStep: 3400 Training Loss: tensor(2.0427) Validation Loss: tensor(2.0881)\nStep: 3600 Training Loss: tensor(2.0371) Validation Loss: tensor(2.1069)\nStep: 3800 Training Loss: tensor(2.0253) Validation Loss: tensor(2.1075)\nStep: 4000 Training Loss: tensor(2.0300) Validation Loss: tensor(2.1037)\nStep: 4200 Training Loss: tensor(2.0191) Validation Loss: tensor(2.0958)\nStep: 4400 Training Loss: tensor(2.0207) Validation Loss: tensor(2.0896)\nStep: 4600 Training Loss: tensor(1.9983) Validation Loss: tensor(2.0888)\nStep: 4800 Training Loss: tensor(1.9998) Validation Loss: tensor(2.0826)\nStep: 4999 Training Loss: tensor(1.9828) Validation Loss: tensor(2.0681)\n\n\n\n\n\n\n\n\n\n\n\nKING RIVAR:\nI will to lay ble\n\nHAPOMENBELA:\nAnd thruans that hands:\nWaither us his vet?\n\nMEXENDEL:\nWarch, my feans' to zokn he oursertef it her than welll butes is eesen cin latistlivilv the do kine nown is wace!\n lill dise littius, on him speage aissell, yet lord.\nI mame, this down'st you, thee killo Wicho dhat evings to thed suis Then, it he poorter,-; the day danter firf sorre;\nI therf threy fleront than Pried by of.\n\nHENNG ERLANCE:\nYO:\nArd all his a for huin cour ay and your to-chan the!\n\nJ\nCPU times: user 3min 17s, sys: 490 ms, total: 3min 18s\nWall time: 3min 17s\n\n\nThis looks much better than our last run without the residual layers which had a loss of Validation Loss: tensor(2.4430) and it also beats the previous run before that had a los of Validation Loss: tensor(2.2720) with a final loss of Validation Loss: tensor(2.0940). Also, as you can see the text output, while still gibberish, is much better than in all previous runs.\nThe second trick that helps with training deep neural nets, in addition to residual blocks, is the Norm as depicted on the block which in our case is layer norm. Let’s implement and add that. link\n\nclass LayerNorm:\n    def __init__(self, dim:int, eps:float=1e-5):\n        self.dim = dim\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n    \n    def __call__(self, x):\n        x_mean = x.mean(dim=1, keepdim=True) # layer mean\n        x_variance = x.var(dim=1, keepdim=True) # layer variance\n        x_hat = (x - x_mean) / torch.sqrt(x_variance + self.eps) # normalize to the unit variance\n        self.out = self.gamma * x_hat + self.beta\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nSince the original attention is all you need paper came out, it has become more common to apply the norm prior to the blocks instead of after them with the add as is depicted on the transformer architecture diagram. We will follow what common practice is today. Also instead of using the layer norm we developed, we will use the Pytorch version instead.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        self.layer_norm_1 = nn.LayerNorm(normalized_shape=self.embedding_dim) #NEW\n        self.layer_norm_2 = nn.LayerNorm(normalized_shape=self.embedding_dim) #NEW\n        \n    def forward(self, x):\n        # return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        x = x + self.multi_self_attention_heads_layer(self.layer_norm_1(x)) # added layer norm. UPDATED\n        x = x + self.feed_forward_network(self.layer_norm_2(x)) # added layer norm. UPDATED\n        return x\n        \n\nThese layer norms are applied to each token embedding to ensure they start off having a unit gausian at initialization, but because of the trainable parameters, this may change during training. \nWe also need to add a layer norm after the last transformer block and before the last linear layer. Now let’s train the model again and see how it does.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            nn.LayerNorm(embedding_dim), #NEW\n        )\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.transformer_blocks(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.3103) Validation Loss: tensor(4.3100)\nStep: 200 Training Loss: tensor(2.6644) Validation Loss: tensor(2.6888)\nStep: 400 Training Loss: tensor(2.4590) Validation Loss: tensor(2.4470)\nStep: 600 Training Loss: tensor(2.3602) Validation Loss: tensor(2.3479)\nStep: 800 Training Loss: tensor(2.2801) Validation Loss: tensor(2.2854)\nStep: 1000 Training Loss: tensor(2.2313) Validation Loss: tensor(2.2563)\nStep: 1200 Training Loss: tensor(2.2185) Validation Loss: tensor(2.2377)\nStep: 1400 Training Loss: tensor(2.1741) Validation Loss: tensor(2.2103)\nStep: 1600 Training Loss: tensor(2.1425) Validation Loss: tensor(2.1853)\nStep: 1800 Training Loss: tensor(2.1290) Validation Loss: tensor(2.1792)\nStep: 2000 Training Loss: tensor(2.1295) Validation Loss: tensor(2.1381)\nStep: 2200 Training Loss: tensor(2.1140) Validation Loss: tensor(2.1594)\nStep: 2400 Training Loss: tensor(2.0825) Validation Loss: tensor(2.1407)\nStep: 2600 Training Loss: tensor(2.0727) Validation Loss: tensor(2.1325)\nStep: 2800 Training Loss: tensor(2.0618) Validation Loss: tensor(2.1148)\nStep: 3000 Training Loss: tensor(2.0459) Validation Loss: tensor(2.1033)\nStep: 3200 Training Loss: tensor(2.0515) Validation Loss: tensor(2.1216)\nStep: 3400 Training Loss: tensor(2.0321) Validation Loss: tensor(2.0743)\nStep: 3600 Training Loss: tensor(2.0179) Validation Loss: tensor(2.0913)\nStep: 3800 Training Loss: tensor(2.0171) Validation Loss: tensor(2.0952)\nStep: 4000 Training Loss: tensor(2.0151) Validation Loss: tensor(2.0876)\nStep: 4200 Training Loss: tensor(1.9998) Validation Loss: tensor(2.0803)\nStep: 4400 Training Loss: tensor(2.0134) Validation Loss: tensor(2.0872)\nStep: 4600 Training Loss: tensor(1.9862) Validation Loss: tensor(2.0807)\nStep: 4800 Training Loss: tensor(1.9923) Validation Loss: tensor(2.0776)\nStep: 4999 Training Loss: tensor(1.9644) Validation Loss: tensor(2.0590)\n\n\n\n\n\n\n\n\n\n\nWill be Roridce.\n\nSTAOLOLIO:\nKI a set bube to takegry.\n\nMBROKING\nMy LANGANGENV KINCE:\nthat dight ane away, my feans' to zormuse off Lroof is here vail; dight,\nWhiiree,\nYou, will is therev the do;\nWhe now oir wans!\nAl lind teal.\n-huch courly speap; airse, why.\nHerents norfore elguls;\nProtle, demees kneoul-wou what eiich o' maits, rive ceessience poor gier; thume known,\nrefter so;\nAngatt must wity ale of whith Pried by of.\n\nHKING ESTEL:\nPrisar adaid the Edwart hiin courchard ny ity to chan the whi\nCPU times: user 3min 38s, sys: 526 ms, total: 3min 38s\nWall time: 3min 38s\n\n\nThe loss is now down to Validation Loss: tensor(2.0630) from Validation Loss: tensor(2.0940) during the last run.\n\n%reset -f\n#| output: true"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "",
    "text": "Open in Google Colab\n\n\nIn this post we’re going to create and optimize GPU CUDA kernels to accelerate an embarrasingly parallel workload and achieve a 8.4 million X speedup over baseline. Everything you need to create and run CUDA kernels is contained within this blog post which is actually a Jupyter notebook that you can run interactively. This is a fully self contained example workload that can be run for free with no setup required on Google Colab. All you need to do is click the Open in Google Colab button at the top of this post.\nYou should run this notebook as you go in Google Colab or any other properly configured CUDA environment. No extra setup is required if you use Google Colab, but if you run in another environment you are responsible for setting it up which is beyond the scope of this post. Learning by doing is more effective than reading alone. Hacking around with the code and seeing how that affects the outcomes helps cement the concepts being taught and builds intuition.\nYou will see several methods for computing the workload covered in this post. We will benchmark each method so you observe how quickly they run and begin to build an intuition on much you might be able to accelerate your own workloads. As a rule of thumb you can typically excpect a 10-1,000x speedup when accelerating workloads on GPU’s. Each step will be explained and you will learn tips and tricks throughout the post.\nThe problem we will be solving is based on a real problem I ran across recently. We had a 3D model with hundreds of thousands of objects. The objects were divided into sets. We needed to figure out which objects from set1 overlapped with objects in set2. Objects in set1 contained metadata which needed to be transferred to all overlapping objects from set2. Each set contained approximately 200,000 objects. Every object from set1 needed to be checked for overlap against every object in set2 which equated to 200,000 * 200,000 = 40 Billon checks that needed to be completed. We had the min and max x,y,z bounding box coordinates for every object which is what we used to check for overlap. When the problem was brought to me an initial attempt was made to solve it by looping through each set of objects in pandas and checking for overlapping objects. The problem was this approach was going to take almost a month to run. This was a perfect embarassingly parallel workload and happened to perfectly coincide with the CUDA MODE class I was taking. It was the perfect opportunity to implement my newly learned CUDA skills on real world problem. Code is included in this notebook to create a synthetic dataset that matches this problem.\nWe will follow the same path I took when trying out different methods to solve this problem. We’ll start out with the basic pandas for loop code that was brought to me initially and I’ll show you a simple trick to speed up pandas for loops. Next we’ll look at a solution using numpy and broadcasting. This is one of the simplest ways to parallelize workloads on CPU’s. This reduced the runtime from 777 hours with our initial pandas for loop to 8 minutes. From a practical perspective this was an acceptable amount of time to wait for the answer, but I was curious how much faster I could make this a custom CUDA kernel. The results were surprising to say the least. The next easy step was to take the numpy broadcasting solution and conver it to pytorch and run it on the GPU. This brought the runtime down to 15 seconds, a 32x improvement. At this point I was skeptical how much better a custom CUDA kernel would really be able to speed this up. After all pytorch is an incredibly optimized library for performing AI training, would I really be able to beat it. It turns out the answer was yes! Not that my CUDA code is better than pytorch’s, it’s definitely not, however my code that was written for this specific problem ended up being a lot faster than the broadcasting solution in pytorch. I’ll show you how to write and run C++ CPU and CUDA code using the pytorch load_inline compiler solution. I’ll also show you how create compiled CPU and CUDA solutions using numba and compare and contrast the 2 options and explain why I like numba better.\nA special thanks to Mark Saroufim and Andreas Köpf for creating the CUDA MODE lecture series and Jeremy Howard for beautifully demonstrating 2 dead simple ways to get started with CUDA programming using PyTorch load_inline and Numba.\n\nPyTorch load_inline lecture and notebook\nNumba lecture and notebook\n\nBefore we jump into the code I’ll show you the benchmarking results to hopefully get you excited about the speedups you’re going to learn how to implement.\n\n\nCode\n#Compiled benchmark results:\n\n#    task,                              gpu,       duration, uom,\ndata = [\n    ('pandas_iterrows',                 't4_2',    777,      'h'),\n    ('pandas_itertuples',               't4_2',    15,       'h'),\n    ('numpy_cpu_broadcasting',          't4_2',    7.87,     'm'),\n    ('torch_gpu_broadcasting',          't4_2',    0.25,     'm'),\n    ('cpp_gpu_tli',                     't4_2',    369,      'ms'),\n    ('cpp_cpu_1_thread_tli',            't4_2',    3.78,     'm'),\n    ('cpp_cpu_omp_tli',                 't4_2',    2.25,     'm'),\n    ('numba_cpu_f32',                   't4_2',    1.17,     'm'),\n    ('numba_cpu_i32',                   't4_2',    1.02,     'm'),\n    ('numba_gpu_0',                     't4_2',    401,      'ms'),\n    ('numba_gpu_smem1_f32',             't4_2',    345,      'ms'),\n    ('numba_gpu_smem2_f32',             't4_2',    333,      'ms'),\n    ('numba_gpu_smem3_f16',             't4_2',    331,      'ms'),\n\n    ('pandas_iterrows',                 'a100_12', 427,      'h'),\n    ('pandas_itertuples',               'a100_12', 12.2,     'h'),\n    ('numpy_cpu_broadcasting',          'a100_12', 6.28,     'm'),\n    ('torch_gpu_broadcasting',          'a100_12', 0.053,    'm'),\n    ('cpp_gpu_tli',                     'a100_12', 85,       'ms'),\n    ('cpp_cpu_1_thread_tli',            'a100_12', 1.8,      'm'),\n    ('cpp_cpu_omp_tli',                 'a100_12', 1.2,      'm'),\n    ('numba_cpu_f32',                   'a100_12', 0.17,     'm'),\n    ('numba_cpu_i32',                   'a100_12', 0.16,     'm'),\n    ('numba_gpu_0',                     'a100_12', 122,      'ms'),\n    ('numba_gpu_smem1_f32',             'a100_12', 114,      'ms'),\n    ('numba_gpu_smem2_f32',             'a100_12', 115,      'ms'),\n    ('numba_gpu_smem3_f16',             'a100_12', 110,      'ms'),\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see our runtime went from hours to minutes to milliseconds as we added first CPU and then GPU acceleration. Keep on reading to see the implementations of each implementation."
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#colab-setup",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#colab-setup",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "2.1 Colab Setup",
    "text": "2.1 Colab Setup\nGetting started in Google Colab is fortunately very easy! Just load this notebook in Google Colab and select a GPU enabled runtime. A free Nvidia T4 instance will work just fine, though it’s a bit slow. All of the base benchmarks in this notebook were obtained using a free T4 instance on Colab.  As of May 2024 Colab comes with all of the required dependencies such as CUDA, pytorch, numpy, pandas, numba, tqdm, ipywidgets, etc except for Ninja and wurlitzer which you can install by running the command below.\n\n!pip install Ninja wurlitzer\n\nRequirement already satisfied: Ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\nRequirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (3.1.0)"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#local-setup-not-recommended-for-beginners",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#local-setup-not-recommended-for-beginners",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "2.2 Local Setup (Not Recommended for Beginners)",
    "text": "2.2 Local Setup (Not Recommended for Beginners)\nThese are the steps I followed to set up my environment for this project. They are not comprehensive and may not be optimal for your situation. You may use them as a reference but please modify them as necessary to fit your situation. If you get stuck, I suggest using Google Colab instead of trying to run this locally because it is easier to get started with.\n\nSetup new conda environment: conda create -n numba python=3.9\nActivate your new environment: conda activate numba\npip install -U setuptools pip ninja\nconda install -c conda-forge cupy cuda-version=12.1\nconda install nvidia::cuda-python\nconda install nvidia/label/cuda-12.1.0::cuda\nInstall conda prereqs: conda install numba\nInstall pytorch: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\nInstall pip pre-reqs: pip install pandas tqdm ipywidgets wurlitzer\n(Optional) Setup the conda env as a jupyter kernel: conda install ipykernel and ipython kernel install --user --name=numba"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#numpy-on-cpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#numpy-on-cpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "7.1 Numpy on CPU",
    "text": "7.1 Numpy on CPU\nVectorizing with NumPy allows the CPU to perform these comparisons in parallel. Additionally, NumPy is implemented in C, which should provide a dramatic speedup.\n\ndef check_overlap_np(set1, set2, offset):\n    # Directly use broadcasting for comparison without explicit expansion\n    min_overlap = set1[:, None, :3] &lt;= set2[None, :, 3:]\n    max_overlap = set1[:, None, 3:] &gt;= set2[None, :, :3]\n\n    # Perform logical AND operation and find all-axis overlap\n    overlap = np.logical_and(min_overlap, max_overlap).all(axis=2)\n\n    # Extract indices of overlapping boxes\n    overlap_indices = np.argwhere(overlap) + np.array([offset, 0])\n\n    # Convert to list of tuples (index in boxes1, index in boxes2)\n    overlap_pairs = list(map(tuple, overlap_indices))\n\n    return overlap_pairs\n\n\nprint(hardware_runtime_info)\n%time check_overlap_np(set1_np[1:4001],set2_np,0)[:5]\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5.98 s, sys: 3.38 s, total: 9.35 s\nWall time: 9.44 s\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 6.2 s, sys: 1.38 s, total: 7.58 s\nWall time: 7.54 s\n----------------------------------------------------------------------------------------------------\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n2.0 % of data checked\n0.1311111111111111 hours to check the entire dataset\n7.866666666666665 min to check the entire dataset\n5932.203389830509 times speedup vs pandas iterrows\n114.40677966101697 times speedup vs pandas itertuples\n\n\nNow we’re talking—a 5,932x speedup! We can now process the entire dataset in about 8 minutes, a significant improvement over the original estimate of 777 hours. For this particular dataset, we only need to perform the analysis once, so running in 8 minutes is sufficient. However, if each set were 10 times larger, the processing time would increase 100-fold, bringing us back to an impractical duration again. Let’s see how much of a speedup we can achieve by moving these arrays to the GPU while using the same algorithm."
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#torch-on-gpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#torch-on-gpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "7.2 Torch on GPU",
    "text": "7.2 Torch on GPU\nWe’ll start off by rewriting our check_overlap function in pytorch.\n\ndef check_overlap_torch(set1, set2, offset):\n    # Ensure inputs are on the same device (e.g., CUDA GPU)\n    device = set1.device\n\n    # Directly use broadcasting for comparison without explicit expansion\n    min_overlap = set1[:, None, :3] &lt;= set2[None, :, 3:]\n    max_overlap = set1[:, None, 3:] &gt;= set2[None, :, :3]\n\n    # Perform logical AND operation and find all-axis overlap\n    overlap = torch.logical_and(min_overlap, max_overlap).all(dim=2)\n\n    # Extract indices of overlapping boxes\n    overlap_indices = torch.nonzero(overlap) + torch.tensor([offset, 0], device=device)\n\n    # Convert to list of tuples (index in boxes1, index in boxes2)\n    overlap_pairs = list(map(tuple, overlap_indices.to('cpu').tolist()))\n\n    return overlap_pairs\n\nWe’ll run it on the first 4,000 items from set 1 against all items in set 2 just like we did with the numpy arrays on CPU to get an apple to apple comparison.\n\nprint(hardware_runtime_info)\n%timeit -n 5 check_overlap_torch(set1_tt[1:4001], set2_tt, 0)\ncheck_overlap_torch(set1_tt[1:4001], set2_tt, 0)[:5]\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n304 ms ± 2.35 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n56.3 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n----------------------------------------------------------------------------------------------------\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n2.0 % of data checked\n0.004222222222222222 hours to check the entire dataset\n15.2 sec to check the entire dataset\n184,211.0 times speedup vs pandas iterrows\n31.0 times speedup vs numpy CPU broadcasting\n\n\nOk, this is another huge speedup. We’re now at a 184,211x speedup over the orignal implementation and a 31x speedup over the comparable CPU implementation. Since it’s so fast now, let’s actually compute this against the entire dataset.\n\n%%time\nprint(hardware_runtime_info)\noverlaps = []\nfor i in range(0,len(set1_tt),4000):\n    overlaps += check_overlap_torch(set1_tt[i:i+4000], set2_tt, i)\noverlaps += check_overlap_torch(set1_tt[i+4000:], set2_tt, i+4000)\nprint(f\"Overlaps found: {len(overlaps):,}\")\noverlaps[:5]\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nOverlaps found: 396,137\nCPU times: user 15.9 s, sys: 51.8 ms, total: 16 s\nWall time: 16 s\n\n\n[(0, 0), (0, 35920), (1, 0), (1, 1), (2, 1)]\n\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 14.5 s, sys: 50.4 ms, total: 14.5 s\nWall time: 16 s\n[(0, 0), (0, 35920), (1, 0), (1, 1), (2, 1)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3.17 s, sys: 29.4 ms, total: 3.2 s\nWall time: 3.19 s\n----------------------------------------------------------------------------------------------------\nYou might be wondering why we’re processing this in chunks of 4,000 rows instead of all at once. This approach is more memory-efficient and results in a faster runtime. Without chunking, you’d run out of memory.  Next, we’ll create a matrix using these results, formatted for future custom CUDA kernel examples. This matrix will have dimensions len(set1) x 6. Each row in this matrix corresponds to an index from set1, and we store up to six matches, with each integer value representing the index of a matching row from set2. We will use output_test_tt from now on to validate the correctness of the results produced by future kernels.\n\noutput_test_tt = torch.ones((len(set1_tt),6),dtype=int)*-1\noutput_test_tt_y = {i:0 for i in range(len(set1_tt))}\nfor set1_idx, set2_idx in overlaps:\n  output_test_tt[set1_idx,output_test_tt_y[set1_idx]] = set2_idx\n  output_test_tt_y[set1_idx] += 1\noutput_test_tt\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]])"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#setup-and-boilerplate",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#setup-and-boilerplate",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "8.1 Setup and Boilerplate",
    "text": "8.1 Setup and Boilerplate\nNow we’ll write our own custom CUDA kernel and compile it using PyTorch’s load_inline function. We’ll be using the fantastic CUDA example from Jeremy Howard’s CUDA MODE Lecture 3 presentation and notebook. For a more detailed introduction to CUDA with PyTorch, I recommend checking out these resources.  First we’ll enable CUDA_LAUNCH_BLOCKING to get better compilation error messages and load the wurlitzer plugin, which allows c++ compilation messages to be surfaced up to python and into the notebook.\n\n# os.environ['CUDA_LAUNCH_BLOCKING']='1' # Uncomment this line for better error messages - useful for debugging.\n\n\n%load_ext wurlitzer\n\nload_cuda is a helper function for compiling custom cuda kernel’s inline using torch’s load_inline function.\n\ndef load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n  return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n                     extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")\n\ncuda_begin contains the boilerplate imports and utility functions we’ll use for compiling our custom CUDA kernels. We import the PyTorch torch extension for integrating torch in our C++ code, stdio for printing, and CUDAException for error handling and reporting. Additionally, we create a few helper functions to ensure that the tensors passed to our custom CUDA kernel are on the GPU and contiguous in memory. We also include a simple helper function for ceiling division to calculate the number of blocks to launch.\n\ncuda_begin = r'''\n#include &lt;torch/extension.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;c10/cuda/CUDAException.h&gt;\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\ninline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n'''"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-cuda-kernel-on-gpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-cuda-kernel-on-gpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "8.2 C++ CUDA Kernel on GPU",
    "text": "8.2 C++ CUDA Kernel on GPU\nThe main advantage of using GPUs over CPUs is their ability to perform a large amount of work in parallel and largely independently. While CPU cores are typically faster than GPU cores, a powerful modern CPU might have 64 cores or 128 threads, whereas a modern GPU has thousands of ‘cores’ and can execute thousands of threads in parallel. This parallelism allows GPUs to outperform CPUs significantly on embarrassingly parallel workloads. The speedup varies depending on the workload and the specific CPU and GPU used, but generally, you can expect a speedup ranging from 10x to 1000x.   When designing your CUDA program, you divide your workload into blocks and threads. A block is a group of threads, and blocks can be defined as 1, 2, or 3 dimensional to map to your workload for easier comprehension. Utilizing additional block dimensions does not affect performance. We will use a single block dimension, with a maximum of 2^31 blocks for dimension 0 (x) and 2^16 for dimensions 1 (y) and 2 (z). Each block can have a maximum of 1024 threads. It is typically beneficial to use a multiple of 32 when assigning the number of threads per block, as this is the size of a ‘warp’ in modern Nvidia GPUs. While the explanation of what a warp is is beyond the scope of this notebook, know that using a thread count divisible by 32 is almost always faster than not using a multiple of 32.   You might be wondering why ‘blocks’ are important and why Nvidia introduced this concept in CUDA. Blocks are crucial for several reasons. All threads within a block are executed on the same streaming multiprocessor (SM) and have access to shared memory, which is approximately 10 times faster than global memory. For example, an RTX 3090 has 48KB of shared memory per block and 82 SMs, with a total of 10,496 CUDA cores. Similarly, a T4 has 48KB of shared memory per block and 40 SMs, with a total of 2,560 CUDA cores.  With that brief introduction, let’s dive into the code. First, we need to define our custom CUDA kernel. The __global__ keyword specifies that the function is a CUDA kernel to be executed on the GPU. CUDA kernels are executed in parallel on the GPU and do not return anything. Instead, you need to pass a variable to the kernel where the output can be stored. In our function, we need to pass pointers to set1 (s1), set2 (s2), and the output tensor (out), as well as the lengths of s1 and s2, and the maximum number of s2 indices that can be stored per s1 item in out. Please take a moment to read through the thoroughly and verbosely commented CUDA kernel below to understand its functionality.\n\ncuda_src = cuda_begin + r'''\n// CUDA kernel function\n__global__ void find_overlap_kernel(float* s1, float* s2, int s1len, int s2len, int* out, int out_idx_max) {\n\n    // Get the global thread index by using the block index, blockDim (threads per block) and thread index within the block.\n    // Since we will be using this index 'i' to index into the 's1' array which is 'flattened' 2D matrix we with a dimension\n    // n x 6 where n is the number of items in s1, we need to multiply our global thread index by 6 to offset the global index\n    // to be able to properly index into 's1'. The items in 's1' and 's2' are the min - x,y,z and max - x,y,z values of the\n    // 3D model object bounding boxes.\n    int i = (blockIdx.x * blockDim.x + threadIdx.x) * 6;\n\n    //Initialize the index into the output array to 0 of the maximum 'out_idx_max'.\n    int out_idx = 0;\n\n    // Guard - check that the global thread index is within the range of the 's1' array. This is required because we\n    // it is common that the number of 'blocks' x 'thread per block' is not exactly equal to the number of items to\n    // be processed so we effectively need to tell these threads to do nothing if they are out of range. This also\n    // prevents illegal memory access.\n    if (i &lt; s1len * 6) {\n\n        // We need to now loop through every item in 's2' and check if the current item in 's1' overlaps with it.\n        for (int j = 0; j &lt; s2len; j++) {\n\n            // We need to apply the same offset for 's2' as we did for 's1' for the min/max x,y,z values.\n            int jf = j * 6;\n\n            // Check if the current item in 's1' overlaps with the current item in 's2'. We do this by checking if the min x,y,z\n            // values of 's1' are less than the max x,y,z values of 's2' and the max x,y,z values of 's1' are greater than the min x,y,z\n            // values of 's2'. If all of these conditions are true then we have an overlap.\n            if (s1[i+0] &lt;= s2[jf+3] && s1[i+1] &lt;= s2[jf+4] && s1[i+2] &lt;= s2[jf+5] && s1[i+3] &gt;= s2[jf+0] && s1[i+4] &gt;= s2[jf+1] && s1[i+5] &gt;= s2[jf+2]) {\n\n                // If we have an overlap then we need to store the index of 's2' in the output array. The output array is of size\n                // 's1len x out_idx_max' where for convenience we chose 'out_idx_max' to be 6 so we could reuse 'i' for both the index\n                // into 's1' and the index into 'out'.\n                out[i+out_idx] = j;\n\n                // Increment the index into the output array if 'out_idx' is less than the parameter 'out_idx_max'. We are not handling\n                // the case where there are more than 'out_idx_max' overlaps for a given 's1' item in this example, but using our synthetic dataset\n                // we should not hit this case.\n                if (out_idx &lt; out_idx_max) {\n                    out_idx += 1;\n                }\n            }\n        }\n    }\n}\n\n// CUDA kernel launch function. This c++ function launches the CUDA kernel and is run on the CPU. We need to pass it the s1 and s2 torch tensors.\n// It will validate the input tensors, get the lengths of the inpute tensors, create the empty output tensor and then launch the CUDA kernel.\ntorch::Tensor find_overlap(torch::Tensor s1, torch::Tensor s2) {\n    //printf(\"Starting...\\n\");\n\n    // Validate the input tensors are on the GPU and contiguous. 'CHECK_INPUT' is a helper function defined in the CUDA kernel boilerplate.\n    CHECK_INPUT(s1);\n    CHECK_INPUT(s2);\n\n    // Get the lengths of the input tensors.\n    int s1len = s1.size(0);\n    int s2len = s2.size(0);\n\n    // Set the maximum number of overlaps to record for each box in s1. In this example 6 is the ONLY value that will work correctly.\n    int match_limit = 6;\n\n    // Create the output tensor creation options specifying the data type (int) and device (same as s1 - GPU).\n    auto options = torch::TensorOptions().dtype(torch::kInt).device(s1.device());\n\n    // Create the output tensor with the specified options and initialize it with -1 to indicate no overlap.\n    torch::Tensor out = torch::ones({s1len, match_limit}, options) * -1;\n\n    // Set the number of threads per block\n    int threads = 256;\n\n    // Calculate the number of blocks needed by using the ceiling division function defined in the CUDA kernel boilerplate.\n    // NOTE: This will end up creating more threads than necessary to process each item in s1 which is why we need to implement the guard\n    // in the CUDA kernel.\n    int blocks = cdiv(s1len,threads);\n\n    // Launch the CUDA kernel specifying the number of blocks and threads inside the &lt;&lt; &gt;&gt; brackets. This is special sytax that is required\n    // for CUDA kernels. Then we pass in the pointers to the s1, s2 and out tensors as well as the length of s1, s2, and the max number of\n    // matched s2 indicies per s1 item that can be stored in 'out' as values.\n    find_overlap_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(s1.data_ptr&lt;float&gt;(), s2.data_ptr&lt;float&gt;(), s1len, s2len, out.data_ptr&lt;int&gt;(), match_limit);\n\n    // Check for CUDA errors. This is required because the CUDA threads are executed asynchronously and we need to wait for them to finish and check\n    // whether there were any errors.\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n\n    // Synchronize the CUDA threads. This is required because the CUDA threads are executed asynchronously and we need to wait for them all to finish before\n    // returning the output tensor to ensure all threads have completed and the output is fully populated.\n    cudaDeviceSynchronize();\n\n    // Return the output tensor.\n    return out;\n}\n'''\n\nAdd the find_overlap c++ function declaration to the cpp_sources in the torch load_inline function. The function logic is defined above in the cuda_src and is the c++ function run on the CPU that calls our custom CUDA kernel.\n\ncpp_src = \"torch::Tensor find_overlap(torch::Tensor s1, torch::Tensor s2);\"\n\nCompile the CUDA kernel using our load_cuda utility function, which calls PyTorch’s load_inline with some predefined defaults. We’ll time the compilation step, which typically takes 1-2 minutes. When writing your own CUDA kernel, this wait time can significantly slow down your iteration speed. Later in this post, we’ll demonstrate other methods that compile much faster and allow for quicker iterations, which is highly recommended. It took me many iterations and a lot of waiting for compilation to get this code right.\n\nprint(hardware_runtime_info)\n%time module = load_cuda(cuda_src, cpp_src, ['find_overlap'], verbose=True)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.01 s, sys: 105 ms, total: 1.11 s\nWall time: 1min 33s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/inline_ext...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext.so\nLoading extension module inline_ext...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 680 ms, sys: 109 ms, total: 788 ms\nWall time: 1min 15s\n----------------------------------------------------------------------------------------------------\nNow we’ll inspect the newly compiled module. We can see that it has our newly defined find_overlap function, which is now available to call from Python. While inline_compile is relatively slow, it simplifies writing and running C++ CUDA code from a Python Jupyter notebook, which is quite convenient!\n\ndir(module)\n\n['__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'find_overlap']\nWe’ll quickly inspect the type, shape, and layout of the set1 and set2 tensors that we’ll use to test our new CUDA kernel. It’s always a good idea to examine your inputs and outputs when writing code, and Jupyter notebooks make this process easy.\n\nset1_tt.dtype, set2_tt.dtype, set1_tt.shape, set2_tt.shape, set1_tt.layout, set2_tt.layout\n\n(torch.float32,\n torch.float32,\n torch.Size([200000, 6]),\n torch.Size([200000, 6]),\n torch.strided,\n torch.strided)\nFinally, we’ll time our new CUDA kernel. It takes 369 ms to run which is a 7,600,000 (7.6 Millon) times speedup over our original implementation and a 41 x improvement over our previous broadcasted pure pytorch CUDA implementation.\n\nprint(hardware_runtime_info)\n%timeit -n 10 out = module.find_overlap(set1_tt, set2_tt).cpu()\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n369 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n85.3 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.0001025 hours to check the entire dataset\n0.369 sec to check the entire dataset\n7,588,076.0 times speedup vs pandas iterrows\n41.0 times speedup vs pytorch gpu broadcasting\n\n\nFinally, we’ll calculate the results one last time and store them in the out variable. This will allow us to compare against our previously calculated results to ensure they match and confirm that we have not introduced any logic errors.  Checking the output is a crucial step. While implementing some kernels for this post, I observed large speedups that initially seemed promising. However, I later realized that these ‘performance gains’ were due to omitting a significant portion of the computation. This underscores the importance of thorough validation.\n\nout = module.find_overlap(set1_tt, set2_tt)\nout\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\nFinally, we’ll check to make sure the results match, which they do.\n\ntorch.all(out==output_test_tt.cuda())\n\ntensor(True, device='cuda:0')"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-single-threaded-for-loop-on-cpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-single-threaded-for-loop-on-cpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "8.3 C++ Single Threaded For Loop on CPU",
    "text": "8.3 C++ Single Threaded For Loop on CPU\nAfter implementing a working custom CUDA kernel, I was curious to see how fast a naive single-threaded (nested for loop) CPU-only C++ implementation would be. My aim with this implementation is purely for benchmarking purposes, to help build an intuition on the types of speedups you might expect with different implementations.\n\ncuda_src = cuda_begin + r'''\ntorch::Tensor find_overlap_cpu(torch::Tensor s1, torch::Tensor s2) {\n    CHECK_CONTIGUOUS(s1);\n    CHECK_CONTIGUOUS(s2);\n\n    int s1len = s1.size(0);\n    int s2len = s2.size(0);\n    int match_limit = 6; // Maximum number of overlaps to record for each box in s1\n\n    // Create an output tensor initialized with -1 to indicate no overlap\n    auto options = torch::TensorOptions().dtype(torch::kInt).device(torch::kCPU);\n    torch::Tensor out = torch::empty({s1len, match_limit}, options).fill_(-1);\n\n    // Direct access to tensor data\n    const float* s1_ptr = s1.data_ptr&lt;float&gt;();\n    const float* s2_ptr = s2.data_ptr&lt;float&gt;();\n    int* out_ptr = out.data_ptr&lt;int&gt;();\n\n    // Nested loops to check for overlaps\n    for (int i = 0; i &lt; s1len; i++) {\n        int out_idx = 0; // Index to track number of overlaps found for the current s1 box\n        for (int j = 0; j &lt; s2len; j++) {\n            // Calculate flat indices for i-th and j-th boxes\n            int iflat = i * 6;\n            int jflat = j * 6;\n\n            // Overlap check condition\n            if (s1_ptr[iflat + 0] &lt;= s2_ptr[jflat + 3] && s1_ptr[iflat + 1] &lt;= s2_ptr[jflat + 4] &&\n                s1_ptr[iflat + 2] &lt;= s2_ptr[jflat + 5] && s1_ptr[iflat + 3] &gt;= s2_ptr[jflat + 0] &&\n                s1_ptr[iflat + 4] &gt;= s2_ptr[jflat + 1] && s1_ptr[iflat + 5] &gt;= s2_ptr[jflat + 2]) {\n\n                // Record overlap index if within limit\n                if (out_idx &lt; match_limit) {\n                    out_ptr[i * match_limit + out_idx] = j;\n                    out_idx++;\n                }\n            }\n        }\n\n        // Mark as no overlap found if out_idx is still 0 after checking all s2 boxes\n        // if (out_idx == 0) {\n        //     out_ptr[i * match_limit] = -2;\n        // }\n    }\n\n    return out;\n}\n'''\n\n\ncpp_src = \"torch::Tensor find_overlap_cpu(torch::Tensor s1, torch::Tensor s2);\"\n\n\nprint(hardware_runtime_info)\n%time module = load_cuda(cuda_src, cpp_src, ['find_overlap_cpu'], verbose=True)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.05 s, sys: 110 ms, total: 1.16 s\nWall time: 1min 36s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 1 and re-building as inline_ext_v1...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v1...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v1.so\nLoading extension module inline_ext_v1...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 702 ms, sys: 108 ms, total: 810 ms\nWall time: 1min 13s\n----------------------------------------------------------------------------------------------------\nFor our CPU implementation, we need to create tensors on the CPU instead of using the GPU tensors we used previously for our CUDA kernel.\n\nset1_ttc = torch.tensor(set1_np/1000,dtype=torch.float32).contiguous()\nset2_ttc = torch.tensor(set2_np/1000,dtype=torch.float32).contiguous()\n\n\nprint(hardware_runtime_info)\n%time out = module.find_overlap_cpu(set1_ttc, set2_ttc)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 39s, sys: 1.17 s, total: 3min 40s\nWall time: 3min 47s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 30s, sys: 1.51 s, total: 3min 32s\nWall time: 3min 28s\n----------------------------------------------------------------------------------------------------\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.06305555555555556 hours to check the entire dataset\n227 sec to check the entire dataset\n12,335.0 times speedup vs pandas iterrows\n615.1761517615 times SLOWDOWN vs Custom GPU CUDA kernel torch_inline\n\n\nAnd finally test that the output is correct.\n\ntorch.all(out==output_test_tt)\n\ntensor(True)"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-omp-parallelization-on-cpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#c-omp-parallelization-on-cpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "8.4 C++ OMP Parallelization on CPU",
    "text": "8.4 C++ OMP Parallelization on CPU\nNext, we’ll try parallelizing on the CPU using C++ OMP (OpenMP) parallelization to see how fast we can get on the CPU. The runtimes will vary SIGNIFICANTLY depending on the number of cores available and their speed.  To add OMP parallelization, we need to include a few additional compiler flags in our load_cuda function, which is why we’re rewriting it here.\n\ndef load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n  return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n                     extra_cflags=[\"-fopenmp\"],\n                     extra_cuda_cflags=[\"-O2\"] if opt else [],\n                     extra_ldflags=[\"-fopenmp\"],\n                     verbose=verbose, name=\"inline_ext\")\n\nChoose the number of threads that makes sense for your CPU. I have added a quick function below to determine the number of threads your CPU supports if you don’t already know.\n\nimport multiprocessing\n\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\ncores\n\n2\n\n\n\ncuda_src = r\"\"\ncpp_src = cuda_begin + r\"\"\"\ntorch::Tensor find_overlap_cpu_omp(torch::Tensor s1, torch::Tensor s2) {\n    CHECK_CONTIGUOUS(s1);\n    CHECK_CONTIGUOUS(s2);\n\n    // Set the number of OpenMP threads\n    omp_set_num_threads(2); // Set this to an appropriate value for your cpu\n\n    int s1len = s1.size(0);\n    int s2len = s2.size(0);\n    int match_limit = 6; // Maximum number of overlaps to record for each box in s1\n\n    auto options = torch::TensorOptions().dtype(torch::kInt).device(torch::kCPU);\n    torch::Tensor out = torch::empty({s1len, match_limit}, options).fill_(-1);\n\n    const float* s1_ptr = s1.data_ptr&lt;float&gt;();\n    const float* s2_ptr = s2.data_ptr&lt;float&gt;();\n    int* out_ptr = out.data_ptr&lt;int&gt;();\n\n    #pragma omp parallel for\n    for (int i = 0; i &lt; s1len; i++) {\n        int out_idx = 0;\n        for (int j = 0; j &lt; s2len; j++) {\n            int iflat = i * 6;\n            int jflat = j * 6;\n\n            if (s1_ptr[iflat + 0] &lt;= s2_ptr[jflat + 3] && s1_ptr[iflat + 1] &lt;= s2_ptr[jflat + 4] &&\n                s1_ptr[iflat + 2] &lt;= s2_ptr[jflat + 5] && s1_ptr[iflat + 3] &gt;= s2_ptr[jflat + 0] &&\n                s1_ptr[iflat + 4] &gt;= s2_ptr[jflat + 1] && s1_ptr[iflat + 5] &gt;= s2_ptr[jflat + 2]) {\n\n                if (out_idx &lt; match_limit) {\n                    out_ptr[i * match_limit + out_idx] = j;\n                    out_idx++;\n                }\n            }\n        }\n\n        // if (out_idx == 0) {\n        //     out_ptr[i * match_limit] = -2;\n        // }\n    }\n\n    return out;\n}\n\"\"\"\n\n\nprint(hardware_runtime_info)\n%time module = load_cuda(cuda_src, cpp_src, ['find_overlap_cpu_omp'], verbose=True)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 598 ms, sys: 54.3 ms, total: 652 ms\nWall time: 53 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 2 and re-building as inline_ext_v2...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v2...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v2.so\nLoading extension module inline_ext_v2...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\n%time out = module.find_overlap_cpu_omp(set1_ttc, set2_ttc)\n\nBelow are the results of several different runs on different CPU’s with different numbers of threads.  Benchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5min 41s, sys: 1.35 s, total: 5min 42s\nWall time: 3min 32s\n3min 32s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n8.67 s - 18 threads\n6.91 s - 36 threads\n9.28 s - # threads not set\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 37s, sys: 927 ms, total: 3min 38s\nWall time: 1min 48s\n1min 48s - 2 threads\nTODO: Try 12 threads\n\ntorch.all(out==output_test_tt)\n\ntensor(True)\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.058888888888888886 hours to check the entire dataset\n212 sec to check the entire dataset\n13,208.0 times speedup vs pandas iterrows\n1.070754717 times speedup vs c++ for loop CPU load_inline\n\n\nA trick I discovered in later experiments when writing CUDA kernels was to load the values from the set pointers into dedicated variables before performing the comparisons. This yielded faster runtimes. I applied the same technique to the CPU OMP implementation, and it improved performance here as well by loading the s1 values into variables first.\n\ncuda_src = r\"\"\ncpp_src = cuda_begin + r\"\"\"\ntorch::Tensor find_overlap_cpu_omp(torch::Tensor s1, torch::Tensor s2) {\n    CHECK_CONTIGUOUS(s1);\n    CHECK_CONTIGUOUS(s2);\n\n    omp_set_num_threads(2); // Adjust this to an optimal value for your cpu\n\n    int s1len = s1.size(0);\n    int s2len = s2.size(0);\n    int match_limit = 6;\n\n    auto options = torch::TensorOptions().dtype(torch::kInt).device(torch::kCPU);\n    torch::Tensor out = torch::empty({s1len, match_limit}, options).fill_(-1);\n\n    const float* s1_ptr = s1.data_ptr&lt;float&gt;();\n    const float* s2_ptr = s2.data_ptr&lt;float&gt;();\n    int* out_ptr = out.data_ptr&lt;int&gt;();\n\n    #pragma omp parallel for\n    for (int i = 0; i &lt; s1len; i++) {\n        int out_idx = 0;\n        // Explicit variables for s1 items\n        float s1_0 = s1_ptr[i * 6 + 0];\n        float s1_1 = s1_ptr[i * 6 + 1];\n        float s1_2 = s1_ptr[i * 6 + 2];\n        float s1_3 = s1_ptr[i * 6 + 3];\n        float s1_4 = s1_ptr[i * 6 + 4];\n        float s1_5 = s1_ptr[i * 6 + 5];\n\n        for (int j = 0, jmult6 = 0; j &lt; s2len; j++, jmult6 += 6) {\n            // Directly use the precomputed index for s2 access\n            if (s1_0 &lt;= s2_ptr[jmult6 + 3] && s1_1 &lt;= s2_ptr[jmult6 + 4] &&\n                s1_2 &lt;= s2_ptr[jmult6 + 5] && s1_3 &gt;= s2_ptr[jmult6 + 0] &&\n                s1_4 &gt;= s2_ptr[jmult6 + 1] && s1_5 &gt;= s2_ptr[jmult6 + 2]) {\n\n                if (out_idx &lt; match_limit) {\n                    out_ptr[i * match_limit + out_idx] = j;\n                    out_idx++;\n                }\n            }\n        }\n\n        if (out_idx == 0) {\n            out_ptr[i * match_limit] = -2;\n        }\n    }\n\n    return out;\n}\n\"\"\"\n\n\nprint(hardware_runtime_info)\n%time module = load_cuda(cuda_src, cpp_src, ['find_overlap_cpu_omp'], verbose=True)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 594 ms, sys: 62.5 ms, total: 656 ms\nWall time: 54 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 3 and re-building as inline_ext_v3...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v3...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v3.so\nLoading extension module inline_ext_v3...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 292 ms, sys: 56.7 ms, total: 349 ms\nWall time: 32.2 s\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\n%time out = module.find_overlap_cpu_omp(set1_ttc, set2_ttc)\n\nBelow are the results of several different runs on different CPU’s with different numbers of threads.  Benchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 27s, sys: 1.11 s, total: 3min 28s\nWall time: 2min 15s\n2min 15s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n6.45 s - 18 threads\n4.58 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 2min 26s, sys: 504 ms, total: 2min 27s\nWall time: 1min 12s\n1min 12s - 2 threads\n----------------------------------------------------------------------------------------------------\n\ntorch.all(out==output_test_tt)\n\ntensor(True)\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.0375 hours to check the entire dataset\n135 sec to check the entire dataset\n20,741.0 times speedup vs pandas iterrows\n1.5703703704 times speedup vs c++ OMP CPU load_inline\n\n\nAs you can see, setting the set1 pointer values to variables before performing the comparisons leads to a significant performance improvement!  Now, let’s try the same with the set2 values that are being compared against. This is unlikely to improve performance, as it doesn’t decrease the number of lookups like it does with set1 in the outer loop.\n\ncuda_src = r\"\"\ncpp_src = cuda_begin + r\"\"\"\ntorch::Tensor find_overlap_cpu_omp(torch::Tensor s1, torch::Tensor s2) {\n    CHECK_CONTIGUOUS(s1);\n    CHECK_CONTIGUOUS(s2);\n\n    omp_set_num_threads(2); // Assuming this is the optimal number of threads for your system\n\n    int s1len = s1.size(0);\n    int s2len = s2.size(0);\n    int match_limit = 6;\n\n    auto options = torch::TensorOptions().dtype(torch::kInt).device(torch::kCPU);\n    torch::Tensor out = torch::empty({s1len, match_limit}, options).fill_(-1);\n\n    const float* s1_ptr = s1.data_ptr&lt;float&gt;();\n    const float* s2_ptr = s2.data_ptr&lt;float&gt;();\n    int* out_ptr = out.data_ptr&lt;int&gt;();\n\n    #pragma omp parallel for\n    for (int i = 0; i &lt; s1len; i++) {\n        int out_idx = 0;\n        // Directly initialize variables for s1 items\n        float s1_0 = s1_ptr[i * 6 + 0];\n        float s1_1 = s1_ptr[i * 6 + 1];\n        float s1_2 = s1_ptr[i * 6 + 2];\n        float s1_3 = s1_ptr[i * 6 + 3];\n        float s1_4 = s1_ptr[i * 6 + 4];\n        float s1_5 = s1_ptr[i * 6 + 5];\n\n        for (int j = 0; j &lt; s2len; j++) {\n            // Directly initialize variables for s2 items\n            float s2_0 = s2_ptr[j * 6 + 0];\n            float s2_1 = s2_ptr[j * 6 + 1];\n            float s2_2 = s2_ptr[j * 6 + 2];\n            float s2_3 = s2_ptr[j * 6 + 3];\n            float s2_4 = s2_ptr[j * 6 + 4];\n            float s2_5 = s2_ptr[j * 6 + 5];\n\n            if (s1_0 &lt;= s2_3 && s1_1 &lt;= s2_4 && s1_2 &lt;= s2_5 &&\n                s1_3 &gt;= s2_0 && s1_4 &gt;= s2_1 && s1_5 &gt;= s2_2) {\n\n                if (out_idx &lt; match_limit) {\n                    out_ptr[i * match_limit + out_idx] = j;\n                    out_idx++;\n                }\n            }\n        }\n\n        if (out_idx == 0) {\n            out_ptr[i * match_limit] = -2;\n        }\n    }\n\n    return out;\n}\n\"\"\"\n\n\nprint(hardware_runtime_info)\n%time module = load_cuda(cuda_src, cpp_src, ['find_overlap_cpu_omp'], verbose=True)\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 601 ms, sys: 59.6 ms, total: 661 ms\nWall time: 52.8 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 4 and re-building as inline_ext_v4...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v4...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v4.so\nLoading extension module inline_ext_v4...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 309 ms, sys: 47.6 ms, total: 357 ms\nWall time: 31.5 s\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\n%time out = module.find_overlap_cpu_omp(set1_ttc, set2_ttc)\n\nBelow are the results of several different runs on various CPUs with different numbers of threads.  Benchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 9min 30s, sys: 2.43 s, total: 9min 33s\nWall time: 5min 31s\n5min 31s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n10.7 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 5min 34s, sys: 1.4 s, total: 5min 35s\nWall time: 2min 46s\n2min 46s - 2 threads\nTODO Try 12 threads\n----------------------------------------------------------------------------------------------------\nAs you can see caching the set2 values actually made the performance worse.\n\ntorch.all(out==output_test_tt)\n\ntensor(True)"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#cpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#cpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "9.1 CPU",
    "text": "9.1 CPU\nFirst, we’ll start with a CPU implementation in Numba to establish a baseline. We’ll use float32 numbers initially and explore whether using different numeric types can improve performance for this problem.\n\n9.1.1 float32\nConvert the previously loaded int32 Numpy arrays to float32.\n\nset1_npf = np.array(set1_np/1000,dtype=np.float32)\nset2_npf = np.array(set2_np/1000,dtype=np.float32)\nset1_npf\n\narray([[367.541, 229.104, 201.559, 367.701, 229.264, 201.719],\n       [367.541, 229.104, 204.812, 367.701, 229.264, 204.972],\n       [370.369, 229.104, 204.812, 370.529, 229.264, 204.972],\n       ...,\n       [348.4  , 306.836, 229.795, 348.56 , 306.996, 229.955],\n       [348.4  , 306.836, 231.066, 348.56 , 306.996, 231.226],\n       [348.4  , 306.836, 239.387, 348.56 , 306.996, 239.547]],\n      dtype=float32)\n\n\nVerify the shape and types of the new arrays.\n\nset1_npf.shape, set1_npf.dtype, set2_npf.shape, set2_npf.dtype\n\n((200000, 6), dtype('float32'), (200000, 6), dtype('float32'))\n\n\nNow we’ll create our CPU implementation. Notice that we are defining the types of the input and output parameters for the function. By specifying these types, Numba can compile this function to be much faster than a standard Python function.  Take note of the three different comparison implementations below. I’ve commented out the first two iterations so you can see what I started with. These initial implementations were much slower than the final version. Implementation details can significantly impact your function’s performance, so it’s worth experimenting with different options.\n\n@njit(int32[:, :](float32[:, :], float32[:, :], int64), parallel=True, fastmath=True)\ndef find_overlap_cpu_numba(s1, s2, match_limit=6):\n    s1len = s1.shape[0]\n    s2len = s2.shape[0]\n    out = np.empty((s1len, match_limit), dtype=np.int32)\n    out.fill(-1)  # This is the corrected line\n    # Your parallel loop logic here\n    for i in prange(s1len):\n        s1l = s1[i]\n        s1l0 = s1l[0]\n        s1l1 = s1l[1]\n        s1l2 = s1l[2]\n        s1l3 = s1l[3]\n        s1l4 = s1l[4]\n        s1l5 = s1l[5]\n        out_idx = 0\n        for j in range(s2len):\n            s2l = s2[j]\n            s2l0 = s2l[0]\n            s2l1 = s2l[1]\n            s2l2 = s2l[2]\n            s2l3 = s2l[3]\n            s2l4 = s2l[4]\n            s2l5 = s2l[5]\n            #My original implementation of the overlap check was over 100x slower!!!\n            # if (s1l[:3] &lt;= s2[j, 3:]).all() and (s1l[3:] &gt;= s2[j, :3]).all(): #Iteration 1\n            # if s1l0 &lt;= s2l[3] and s1l1 &lt;= s2l[4] and s1l2 &lt;= s2l[5] \\\n            #     and s1l3 &gt;= s2l[0] and s1l4 &gt;= s2l[1] and s1l5 &gt;= s2l[2]: #Iteration 2\n            if s1l0 &lt;= s2l3 and s1l1 &lt;= s2l4 and s1l2 &lt;= s2l5 \\\n                and s1l3 &gt;= s2l0 and s1l4 &gt;= s2l1 and s1l5 &gt;= s2l2: #Iteration 3\n\n                out[i,out_idx] = j\n                if out_idx &lt; match_limit:\n                    out_idx += 1\n                # else:\n                #     out[i,match_limit-1] = -2\n    return out\n\n\nprint(hardware_runtime_info)\n%time out = find_overlap_cpu_numba(set1_npf, set2_npf,6)\nout\n\nBenchmark Results:\nImplementations:\n\nIteration 1\nif (s1l[:3] &lt;= s2[j, 3:]).all() and (s1l[3:] &gt;= s2[j, :3]).all():\n\nIteration 2\nif s1l0 &lt;= s2l[3] and s1l1 &lt;= s2l[4] and s1l2 &lt;= s2l[5] and s1l3 &gt;= s2l[0] and s1l4 &gt;= s2l[1] and s1l5 &gt;= s2l[2]:\n\nIteration 3\ns2l0 = s2l[0]\ns2l1 = s2l[1]\ns2l2 = s2l[2]\ns2l3 = s2l[3]\ns2l4 = s2l[4]\ns2l5 = s2l[5]\n\nif s1l0 &lt;= s2l3 and s1l1 &lt;= s2l4 and s1l2 &lt;= s2l5 and s1l3 &gt;= s2l0 and s1l4 &gt;= s2l1 and s1l5 &gt;= s2l2:\n\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 42s, sys: 193 ms, total: 1min 42s\nWall time: 1min 10s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n\nIteration 1\nCPU times: user 1h 4min 37s, sys: 798 ms, total: 1h 4min 38s&lt;br&gt;\nWall time: 2min 16s\n\nIteration 2\nCPU times: user 7min 29s, sys: 95.5 ms, total: 7min 29s&lt;br&gt;\nWall time: 14.4 s\n\nIteration 3\nCPU times: user 52.2 s, sys: 12.2 ms, total: 52.2 s&lt;br&gt;\nWall time: 1.99 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 55s, sys: 53.8 ms, total: 1min 55s\nWall time: 10.1 s\n----------------------------------------------------------------------------------------------------\n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.019444444444444445 hours to check the entire dataset\n70 sec to check the entire dataset\n40,000.0 times speedup vs pandas iterrows\n1.9285714286 times speedup vs c++ OMP set1 cache CPU load_inline\n\n\nAs you can see, the three successive implementation iterations yielded a compounded speedup of 9.4x and 7x. I believe the speedup between implementations 1 and 2 is because the NumPy library does not have to be called, but I’m unsure why there was such a significant speedup between implementations 2 and 3. I suspect it has to do with caching, but I’m not certain.\n\n#create a copy of the output to use it for future validations.\nout_key = output_test_tt\n\nAs always, we check out output to make sure it’s correct.\n\ntorch.all(torch.tensor(out)==output_test_tt)\n\ntensor(True)\n\n\n9.1.2 int32\nNext we’ll try out using the same exact logic only swapping float32 for int32 to see if it makes a difference.  First we’ll create our int32 numpy arrays.\n\nset1_npi = np.array(set1_np,dtype=np.int32)\nset2_npi = np.array(set2_np,dtype=np.int32)\nset1_npi\n\narray([[367541, 229104, 201559, 367701, 229264, 201719],\n       [367541, 229104, 204812, 367701, 229264, 204972],\n       [370369, 229104, 204812, 370529, 229264, 204972],\n       ...,\n       [348400, 306836, 229795, 348560, 306996, 229955],\n       [348400, 306836, 231066, 348560, 306996, 231226],\n       [348400, 306836, 239387, 348560, 306996, 239547]], dtype=int32)\n\n\nAnd compile a new function that has the exact same logic as before, but the new type for the inputs as we discussed.\n\n@njit(int32[:, :](int32[:, :], int32[:, :], int64), parallel=True, fastmath=True)\ndef find_overlap_cpu_numba_int(s1, s2, match_limit=6):\n    s1len = s1.shape[0]\n    s2len = s2.shape[0]\n    out = np.empty((s1len, match_limit), dtype=np.int32)\n    out.fill(-1)  # This is the corrected line\n    # Your parallel loop logic here\n    for i in prange(s1len):\n        s1l = s1[i]\n        s1l0 = s1l[0]\n        s1l1 = s1l[1]\n        s1l2 = s1l[2]\n        s1l3 = s1l[3]\n        s1l4 = s1l[4]\n        s1l5 = s1l[5]\n        out_idx = 0\n        for j in range(s2len):\n            s2l = s2[j]\n            s2l0 = s2l[0]\n            s2l1 = s2l[1]\n            s2l2 = s2l[2]\n            s2l3 = s2l[3]\n            s2l4 = s2l[4]\n            s2l5 = s2l[5]\n            #My original implementation of the overlap check was over 100x slower!!!\n            # if (s1l[:3] &lt;= s2[j, 3:]).all() and (s1l[3:] &gt;= s2[j, :3]).all(): #Iteration 1\n            # if s1l0 &lt;= s2l[3] and s1l1 &lt;= s2l[4] and s1l2 &lt;= s2l[5] \\\n            #     and s1l3 &gt;= s2l[0] and s1l4 &gt;= s2l[1] and s1l5 &gt;= s2l[2]: #Iteration 2\n            if s1l0 &lt;= s2l3 and s1l1 &lt;= s2l4 and s1l2 &lt;= s2l5 \\\n                and s1l3 &gt;= s2l0 and s1l4 &gt;= s2l1 and s1l5 &gt;= s2l2: #Iteration 3\n\n                out[i,out_idx] = j\n                if out_idx &lt; match_limit:\n                    out_idx += 1\n                else:\n                    out[i,match_limit-1] = -2\n    return out\n\n\nprint(hardware_runtime_info)\n%time out = find_overlap_cpu_numba_int(set1_npi, set2_npi, 6)\nout\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1min 47s, sys: 265 ms, total: 1min 48s\nWall time: 1min 1s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\nCPU times: user 49.1 s, sys: 16.1 ms, total: 49.1 s&lt;br&gt;\nWall time: 2.02 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 1min 51s, sys: 24.1 ms, total: 1min 51s\nWall time: 9.51 s\nSwapping input types did not make any difference in the runtime.\n\ntorch.all(torch.tensor(out)==output_test_tt)\n\ntensor(True)"
  },
  {
    "objectID": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#gpu",
    "href": "posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.html#gpu",
    "title": "CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks",
    "section": "9.2 GPU",
    "text": "9.2 GPU\n\n9.2.1 Plotting\nWe need to start performing sweeps of hyperparameters, such as the number of threads per block, to determine the optimal values. Plotting the results will help us visualize which settings work best and how much impact changing these settings has. We’ll create a function to plot the number of threads per block on the x-axis and runtime on the y-axis. Due to the significant variance in runtimes, we’ve added a parameter to limit the y-axis range to better visualize the best results. Additionally, we may want to visualize other variables besides threads per block, so if we specify a series key, it will create a separate plot color for each value, such as the number of items per thread.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_timeit_results(results, limit_y_axis=False, series_key=None):\n    \"\"\"\n    Plots the runtime of a function for different values of n, given a list of average runtimes.\n\n    Parameters:\n    - results: List of dictionaries with runtime data and other relevant parameters.\n    - limit_y_axis: Boolean, optional (default False). If True, limits the Y-axis range\n      from 10% below the minimum average runtime to twice the minimum average runtime.\n\n    Returns:\n    - None\n    \"\"\"\n    #get the averate timeit results in ms\n    timeit_results_ms = [o['timeit_results'].average*1000 for o in results]\n\n    #get the threads per block list\n    threads_per_block = [o['threads_per_block'] for o in results]\n\n    #get the min timeit result\n    min_results_ms = min(timeit_results_ms)\n\n    #get the index of the min timeit result\n    min_results_ms_idx = timeit_results_ms.index(min_results_ms)\n\n    #get the threads per block of the min value\n    min_results_threads_per_block = results[min_results_ms_idx]['threads_per_block']\n\n    # Get the indexes of each series_key value if a series key is provided\n    if series_key is not None:\n        series_key_values = dict()\n        for idx, result in enumerate(results):\n            if result[series_key] not in series_key_values:\n                series_key_values[result[series_key]] = list()\n            series_key_values[result[series_key]].append(idx)\n\n    #get any remaining runtime parameter values to add to the chart title\n    extra_params_str = ', '.join([f'{k}: {v}' for k, v in results[min_results_ms_idx].items()\n      if k not in ['timeit_results', 'threads_per_block']])\n\n    # Creating the plot\n    plt.figure(figsize=(10, 6))\n    if series_key is None:\n        plt.plot(threads_per_block, timeit_results_ms, marker='o', linestyle='-', color='b')\n    else:\n        for series_key_value, result_idxs in series_key_values.items():\n            plt.plot([threads_per_block[i] for i in result_idxs], [timeit_results_ms[i] for i in result_idxs],\n                     marker='o', linestyle='-', label=series_key_value)\n        plt.legend(title=series_key if series_key else None)\n\n    # Adding title and labels\n    plt.title(f'''Runtime of function for Different Threads Per Block\n{hardware_runtime_info}\nMin Time (ms) {min_results_ms} @ TPB: {min_results_threads_per_block} {extra_params_str}''')\n    plt.xlabel('Threads Per Block')\n    plt.ylabel('Time Taken (ms)')\n\n    # Limiting Y-axis if requested\n    if limit_y_axis:\n        lower_bound = min_results_ms * 0.9  # 10% below the minimum\n        upper_bound = min_results_ms * 1.2    # 20% above the minimum\n        plt.ylim(lower_bound, upper_bound)\n\n    # Display the plot\n    plt.grid(True)\n    plt.show()\n\n\n\n\n9.2.2 Basic CUDA Kernel\nNow let’s implement our first CUDA kernel with Numba. When defining the Numba CUDA kernel, use the @cuda.jit decorator. When calling this function, you need to pass the required CUDA kernel arguments in square brackets before passing the function arguments in parentheses: kernel_func[grid_dim, block_dim, stream, dyn_shared_mem_size](standard_function_arguments). grid_dim and block_dim are always required, but additional CUDA arguments, such as shared memory size, can also be passed, which we’ll cover shortly. For further information, please refer to the Numba documentation.  This function should look familiar compared to the C++ CUDA kernel implementations from before. You’ll notice that you can utilize some Pythonic implementation styles, which helps make the code more compact and readable.\n\n@cuda.jit\ndef find_overlap_gpu_numba_kernel(s1, s2, out):\n    cbidx, cbdim, tidx = cuda.blockIdx.x, cuda.blockDim.x, cuda.threadIdx.x\n    s1idx = cbidx * cbdim + tidx\n\n    if s1idx &lt; len(s1):\n        1+1\n        s10, s11, s12, s13, s14, s15 = s1[s1idx]\n        outidx = 0\n        for s2idx in range(s2.shape[0]):\n            s2l = s2[s2idx]\n            if s10 &lt;= s2l[3] and s11 &lt;= s2l[4] and s12 &lt;= s2l[5] and \\\n                s13 &gt;= s2l[0] and s14 &gt;= s2l[1] and s15 &gt;= s2l[2]:\n                if outidx &lt; 6:\n                    out[s1idx,outidx] = s2idx\n                    outidx += 1\n        if outidx == 6:\n            out[s1idx,5] = -2\n\nNext, we’ll define our Python wrapper function to handle tasks similar to those managed by the C++ CPU functions earlier, such as determining the number of blocks.\n\ndef find_overlap_gpu_numba(s1, s2, out, tpb=256):\n    blocks = math.ceil(len(s1)/tpb)\n    find_overlap_gpu_numba_kernel[blocks, tpb](ca(s1), ca(s2), ca(out))\n    return out.cpu()\n\nNext we’ll create a simple helper function to create an initialized output array. We’ll typically keep this initialization outside of the benchmark runs to make it easier to compare just the kernel runtimes.\n\ninit_out = lambda: torch.ones(set1_tt.shape,dtype=torch.int32).contiguous().cuda()*-1\nout = init_out()\nout\n\ntensor([[-1, -1, -1, -1, -1, -1],\n        [-1, -1, -1, -1, -1, -1],\n        [-1, -1, -1, -1, -1, -1],\n        ...,\n        [-1, -1, -1, -1, -1, -1],\n        [-1, -1, -1, -1, -1, -1],\n        [-1, -1, -1, -1, -1, -1]], device='cuda:0', dtype=torch.int32)\n\n\nNow let’s run our new numba kernel and see how well it performs.\n\nprint(hardware_runtime_info)\n%timeit -n 10 find_overlap_gpu_numba(set1_tt, set2_tt, out)\n\nIsn’t it nice that we don’t have to wait long for it to compile? This allows for much faster iteration speed. Next, I’ll implement some more advanced techniques, which would have been much more challenging to implement using torch.load_inline due to the slow iteration speed. It took me quite a few tries to get the more advanced implementations right.\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n400 ms ± 4.68 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\nTODO XXXX\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n143 ms ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nNow we’ll take a look at the output and check that it’s correct.\n\nout\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n\ntorch.all(out==output_test_tt.cuda())\n\ntensor(True, device='cuda:0')\nLet’s try another experiment with 1,024 threads per block.\n\nprint(hardware_runtime_info)\nout = init_out()\n%timeit -n 10 find_overlap_gpu_numba(set1_tt, set2_tt, out, tpb=1024)\nassert torch.all(out==output_test_tt.cuda())\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n401 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 83.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nIt looks like it doesn’t make any difference.\nNow let’s do a sweep to confirm what the optimal number of threads per block is.\n\nresults0 = []\nprint(hardware_runtime_info)\nfor tpbn in tqdm(range(1,64+1)):\n  out = init_out()\n  timeit_results = %timeit -o -q -r 1 -n 10 find_overlap_gpu_numba(set1_tt, set2_tt, out, tpb=tpbn*16)\n  assert torch.all(out==output_test_tt.cuda())\n  # print(tpbn,timeit_results)\n  results0.append({'timeit_results':timeit_results, 'threads_per_block':tpbn*16})\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n\n\n\n\n\n \n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n0.00011111111111111112 hours to check the entire dataset\n0.4 sec to check the entire dataset\n7,000,000.0 times speedup vs pandas iterrows\n175.0 times speedup vs numba cpu float32\n0.9225 times speedup vs custom gpu cuda kernel torch load_inline\n\n\n\n\n9.2.3 CUDA Kernel Using Shared Memory v1\nNext, we’ll utilize shared memory to try and speed up our CUDA kernel. Shared memory is ultra-fast memory shared between all threads in a block, and it is approximately 10 times faster than global GPU memory. The Nvidia T4 and RTX 3090 have 48KB of shared memory available per thread block, compared to 16GB and 24GB, respectively, of global memory VRAM in each card.\nIn our CUDA kernel implementation, each thread computes all comparisons for one element from set1 against all elements of set2 by looping over all elements in set2. This means that in our original CUDA kernel implementation, each thread has to load all elements of set2 from global memory. To speed this up, we can break set2 into chunks so that each chunk fits into shared memory and each element in the chunk can be fetched from global memory only once per block by a single thread.\nWe’ll add another loop to our kernel to loop through each set2 chunk. In the outer loop, each thread will load a single value of set2 into shared memory, and then in the inner loop, we’ll compare the element from set1 with each element from set2 in shared memory. If our kernel is memory-bound, this should speed up our overall runtime.\n\ndef find_overlap_gpu_numba_smem(s1, s2, out, tpb=256, s2cachesize=256):\n    dyn_shared_mem_size = s2cachesize * 6 * 4\n    blocks = math.ceil(len(s1)/tpb)\n    find_overlap_gpu_numba_kernel_smem[blocks, tpb, 0, dyn_shared_mem_size](ca(s1), ca(s2), ca(out), s2cachesize)\n    return out.cpu()\n\n\n@cuda.jit\ndef find_overlap_gpu_numba_kernel_smem(s1, s2, out, s2cachesize):\n    cbidx, cbdim, tidx = cuda.blockIdx.x,cuda.blockDim.x,cuda.threadIdx.x\n    s1idx = cbidx * cbdim + tidx\n\n    s1len = len(s1)\n    s2len = len(s2)\n\n    if s1idx &lt; len(s1):\n        s10, s11, s12, s13, s14, s15 = s1[s1idx]\n    else:\n        s10, s11, s12, s13, s14, s15 = np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0)\n    outidx = 0\n\n    #Allocate our shared memory\n    smem = cuda.shared.array(0, dtype=np.float32)\n\n    #Loop through the set2 chunks\n    for s2idx_beg in range(0,math.ceil(len(s2)/cbdim)*cbdim,cbdim):\n\n        #Load Shared Memory\n        if s2idx_beg+tidx &lt; s2len:\n            for i in range(6):\n                smem[tidx*6+i] = s2[s2idx_beg+tidx,i]\n        #We need to synchronize all the threads across the block to ensure they are all done loading their values into shared memory before we proceed.\n        cuda.syncthreads()\n\n        if s1idx &lt; len(s1):\n            #Loop through all items that were loaded into shared memory\n            for s2idx_global in range(s2idx_beg, min(s2idx_beg+cbdim,s2len)):\n                s2idx_local = (s2idx_global - s2idx_beg)*6\n                s2l = smem[s2idx_local:s2idx_local+6]\n                if s10 &lt;= s2l[3] and s11 &lt;= s2l[4] and s12 &lt;= s2l[5] and \\\n                    s13 &gt;= s2l[0] and s14 &gt;= s2l[1] and s15 &gt;= s2l[2]:\n                    if outidx &lt; 6:\n                        out[s1idx,outidx] = s2idx_global\n                        outidx += 1\n            if outidx == 6:\n                out[s1idx,5] = -2\n        # We again need to sync threads because all threads must be done doing\n        # their checks prior to loading the next set2 chunk\n        cuda.syncthreads()\n\n\nout = init_out()\nprint(hardware_runtime_info)\n%timeit -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=512, s2cachesize=512)\nassert torch.all(out==output_test_tt.cuda())\nout\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n345 ms ± 15.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n129 ms ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\nout = init_out()\nn = 20\n%timeit -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=n*32, s2cachesize=n*32)\nassert torch.all(out==output_test_tt.cuda())\nout\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n399 ms ± 3.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n114 ms ± 38.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\nout = init_out()\nn = 32\n%timeit -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=n*32, s2cachesize=n*32)\nassert torch.all(out==output_test_tt.cuda())\nout\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n338 ms ± 5.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 120 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n\nresults1 = []\nprint(hardware_runtime_info)\nfor n in tqdm(range(1,64+1)):\n  out = init_out()\n  timeit_results = %timeit -o -q -r 1 -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=n*16, s2cachesize=n*16)\n  assert torch.all(out==output_test_tt.cuda())\n  # print(n,timeit_results)\n  results1.append({'timeit_results':timeit_results, 'threads_per_block':n*16})\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% XXXXXXXXXXXXXXXXXXXXXX 64/64 [04:09&lt;00:00,  3.55s/it]\n \n\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100 % of data checked\n9.25e-05 hours to check the entire dataset\n0.333 sec to check the entire dataset\n8,408,408.0 times speedup vs pandas iterrows\n1.1081081081 times speedup vs custom gpu cuda kernel torch load_inline\n1.2012012012 times speedup vs numba gpu cuda kernel float32\n\n\nAs you can see using shared memory gave us a nice 20% speed improvement over our previous numba kernel that did not utilize shared memory.\n\n\n9.2.4 CUDA Kernel Using Shared Memory v2\nNext we’ll try a slight variation where we test if loading multiple set2 elements into shared memory per thread is faster (cache_items_per_thread) than only loading a single element per thread as in our v1 example.\n\n@cuda.jit\ndef find_overlap_gpu_numba_kernel_smem(s1, s2, out, cache_items_per_thread):\n    cbidx, cbdim, tidx = cuda.blockIdx.x,cuda.blockDim.x,cuda.threadIdx.x\n    s1idx = cbidx * cbdim + tidx\n\n    s1len = len(s1)\n    s2len = len(s2)\n\n    zf = np.float32(0.0)\n\n    if s1idx &lt; len(s1):\n        s10, s11, s12, s13, s14, s15 = s1[s1idx]\n    else:\n        s10, s11, s12, s13, s14, s15 = zf, zf, zf, zf, zf, zf\n    outidx = 0\n\n    smem = cuda.shared.array(0, dtype=np.float32)\n    for s2idx_beg in range(0,math.ceil(len(s2)/(cbdim*cache_items_per_thread))*cbdim*cache_items_per_thread+1,\n                           cbdim*cache_items_per_thread):\n\n        #Load Shared Memory\n        if s2idx_beg+tidx &lt; s2len:\n            #Loop through the number of cache_items_per_thread - This is the main difference\n            #with our v1 implementation where we always loaded 1 item per thread.\n            for i in range(cache_items_per_thread):\n                if s2idx_beg+tidx*cache_items_per_thread+i &lt; s2len:\n                    for j in range(6):\n                        smem[(tidx*cache_items_per_thread+i)*6+j] = s2[s2idx_beg+tidx*cache_items_per_thread+i,j]\n        cuda.syncthreads()\n\n        if s1idx &lt; len(s1):\n\n            for s2idx_global in range(s2idx_beg, min(s2idx_beg+cbdim*cache_items_per_thread,s2len)):\n                s2idx_local = (s2idx_global - s2idx_beg)*6\n                s2l = smem[s2idx_local:s2idx_local+6]\n\n                if s10 &lt;= s2l[3] and s11 &lt;= s2l[4] and s12 &lt;= s2l[5] and \\\n                    s13 &gt;= s2l[0] and s14 &gt;= s2l[1] and s15 &gt;= s2l[2]:\n\n                    if outidx &lt; 6:\n                        out[s1idx,outidx] = s2idx_global\n                        outidx += 1\n\n        cuda.syncthreads()\n\n\ndef find_overlap_gpu_numba_smem(s1, s2, out, tpb=256, cache_items_per_thread=1):\n    dyn_shared_mem_size = (tpb*cache_items_per_thread) * 6 * 4\n    blocks = math.ceil(len(s1)/tpb)\n    find_overlap_gpu_numba_kernel_smem[blocks, tpb, 0, dyn_shared_mem_size](ca(s1), ca(s2), ca(out), cache_items_per_thread)\n    return out.cpu()\n\nFirst we’ll test it out to make sure it works and produces the correct result.\n\nout = init_out()\nprint(hardware_runtime_info)\n%timeit -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=39*16, cache_items_per_thread=3)\nassert torch.all(out==output_test_tt.cuda())\nout\n\nBenchmark Results:\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n417 ms ± 17.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n152 ms ± 16.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n\nprint(hardware_runtime_info)\nresults2 = []\nbest_result = None\nfor tpbn in tqdm(range(4,64+1)): #Lower than 4 takes forever and is not fastest\n    for ciptn in tqdm(range(1,2048//(tpbn*16)+1)):\n        out = init_out()\n        #verify this is less than the shared memory size\n        if tpbn*16 * ciptn &gt; 2048:\n            pass\n        else:\n            timeit_results = %timeit -o -q -r 1 -n 10 find_overlap_gpu_numba_smem(set1_tt, set2_tt, out, tpb=tpbn*16, cache_items_per_thread=ciptn)\n            assert torch.all(out==output_test_tt.cuda())\n            if best_result is None or timeit_results.average &lt; best_result.average:\n                best_result = timeit_results\n                print(tpbn*16,ciptn,timeit_results)\n            results2.append({'timeit_results':timeit_results, 'threads_per_block':tpbn*16, 'cache_items_per_thread':ciptn})\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [38:27&lt;00:00,  7.17s/it]\n100% 32/32 [07:05&lt;00:00, 23.28s/it]\n64 1 361 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n64 2 353 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 25/25 [04:32&lt;00:00, 18.24s/it]\n100% 21/21 [03:18&lt;00:00, 15.38s/it]\n100% 18/18 [02:31&lt;00:00, 13.07s/it]\n100% 16/16 [02:04&lt;00:00, 11.74s/it]\n128 1 343 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 14/14 [01:41&lt;00:00, 10.31s/it]\n100% 12/12 [01:17&lt;00:00,  9.04s/it]\n100% 11/11 [01:09&lt;00:00,  8.48s/it]\n100% 10/10 [00:57&lt;00:00,  7.53s/it]\n100% 9/9 [00:50&lt;00:00,  7.15s/it]\n100% 9/9 [00:47&lt;00:00,  6.76s/it]\n100% 8/8 [00:42&lt;00:00,  6.49s/it]\n100% 8/8 [00:40&lt;00:00,  6.18s/it]\n256 1 339 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 7/7 [00:34&lt;00:00,  5.68s/it]\n100% 7/7 [00:35&lt;00:00,  5.81s/it]\n100% 6/6 [00:28&lt;00:00,  5.36s/it]\n100% 6/6 [00:27&lt;00:00,  5.11s/it]\n100% 6/6 [00:27&lt;00:00,  5.07s/it]\n100% 5/5 [00:23&lt;00:00,  5.07s/it]\n100% 5/5 [00:22&lt;00:00,  4.87s/it]\n100% 5/5 [00:21&lt;00:00,  4.76s/it]\n100% 5/5 [00:21&lt;00:00,  4.71s/it]\n100% 4/4 [00:16&lt;00:00,  4.24s/it]\n100% 4/4 [00:16&lt;00:00,  4.23s/it]\n100% 4/4 [00:15&lt;00:00,  4.15s/it]\n100% 4/4 [00:16&lt;00:00,  4.37s/it]\n100% 4/4 [00:16&lt;00:00,  4.34s/it]\n100% 4/4 [00:16&lt;00:00,  4.37s/it]\n100% 4/4 [00:15&lt;00:00,  4.10s/it]\n512 1 335 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 3/3 [00:13&lt;00:00,  4.50s/it]\n100% 3/3 [00:13&lt;00:00,  4.47s/it]\n100% 3/3 [00:12&lt;00:00,  4.24s/it]\n100% 3/3 [00:12&lt;00:00,  4.23s/it]\n100% 3/3 [00:12&lt;00:00,  4.25s/it]\n100% 3/3 [00:12&lt;00:00,  4.24s/it]\n100% 3/3 [00:12&lt;00:00,  4.12s/it]\n100% 3/3 [00:11&lt;00:00,  3.94s/it]\n100% 3/3 [00:12&lt;00:00,  4.06s/it]\n100% 3/3 [00:12&lt;00:00,  4.04s/it]\n100% 2/2 [00:08&lt;00:00,  4.08s/it]\n100% 2/2 [00:08&lt;00:00,  4.05s/it]\n100% 2/2 [00:07&lt;00:00,  3.76s/it]\n100% 2/2 [00:07&lt;00:00,  3.75s/it]\n100% 2/2 [00:07&lt;00:00,  3.80s/it]\n100% 2/2 [00:07&lt;00:00,  3.82s/it]\n100% 2/2 [00:07&lt;00:00,  3.90s/it]\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nbest result 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n \nAs you can see there are a few results that are very close but the best one was where there was 1 cache item per thread which is effectively the same as our v1 algorithm.\n\n\n9.2.5 CUDA Kernel Using Shared Memory v3 fp16\nNow we’ll try the same algorithm but with fp16 instead of fp32, which should cut our memory usage and bandwidth in half. Given the lower precision, we need to carefully check that the results are the same, as we’re losing a significant portion (half) of our floating-point resolution.\n\n@cuda.jit\ndef find_overlap_gpu_numba_kernel_smem(s1, s2, out, cache_items_per_thread):\n    cbidx, cbdim, tidx = cuda.blockIdx.x,cuda.blockDim.x,cuda.threadIdx.x\n    s1idx = cbidx * cbdim + tidx\n\n    s1len = len(s1)\n    s2len = len(s2)\n\n    zf = np.float16(0.0)\n\n    if s1idx &lt; len(s1):\n        s10, s11, s12, s13, s14, s15 = s1[s1idx]\n    else:\n        s10, s11, s12, s13, s14, s15 = zf, zf, zf, zf, zf, zf\n    outidx = 0\n\n    s2_chunk_size = min(cbdim*cache_items_per_thread,4096)\n\n    smem = cuda.shared.array(0, dtype=np.float16)\n    for s2idx_beg in range(0,math.ceil(len(s2)/s2_chunk_size)*s2_chunk_size+1,\n                           s2_chunk_size):\n\n        #Load Shared Memory\n        if s2idx_beg+tidx &lt; s2len and tidx*cache_items_per_thread &lt; s2_chunk_size:\n            for i in range(cache_items_per_thread):\n                if s2idx_beg+tidx*cache_items_per_thread+i &lt; s2len and tidx*cache_items_per_thread+i &lt; s2_chunk_size:\n                    for j in range(6):\n                        # smem_idx = (tidx*cache_items_per_thread+i)*6+j\n                        # s2idx_x = s2idx_beg+tidx*cache_items_per_thread+i\n                        # smem[smem_idx] = s2[s2idx_x,j]\n                        smem[(tidx*cache_items_per_thread+i)*6+j] = s2[s2idx_beg+tidx*cache_items_per_thread+i,j]\n        cuda.syncthreads()\n        # print('start read')\n        if s1idx &lt; len(s1):\n\n            for s2idx_global in range(s2idx_beg, min(s2idx_beg+s2_chunk_size,s2len)):\n                s2idx_local = (s2idx_global - s2idx_beg)*6\n                s2l = smem[s2idx_local:s2idx_local+6]\n                if s10 &lt;= s2l[3] and s11 &lt;= s2l[4] and s12 &lt;= s2l[5] and \\\n                    s13 &gt;= s2l[0] and s14 &gt;= s2l[1] and s15 &gt;= s2l[2]:\n\n                    if outidx &lt; 6:\n                        out[s1idx,outidx] = s2idx_global\n                        outidx += 1\n            # if outidx == 6:\n            #     out[s1idx,5] = -2\n        cuda.syncthreads()\n\nWhen declaring the size of the shared memory in this implementation, I initially forgot that it is specified in bytes when the kernel is called. However, when accessed from within the kernel, the bytes per object are taken into account. I was declaring the shared memory size without considering this, which led to continual invalid memory access errors that took quite a while to debug.\n\ndef find_overlap_gpu_numba_smem(s1, s2, out, tpb=256, cache_items_per_thread=1):\n    # Note that we change the calculation from .... * 6 * 4 .... to .... * 6 * 2 .... because fp16\n    # takes 2 bytes instead of 4 like fp32\n    dyn_shared_mem_size = min((tpb*cache_items_per_thread) * 6 * 2, 49152) #48kb of SRAM\n    blocks = math.ceil(len(s1)/(tpb))\n    find_overlap_gpu_numba_kernel_smem[blocks, tpb, 0, dyn_shared_mem_size](ca(s1), ca(s2), ca(out), cache_items_per_thread)\n    return out.cpu()\n\nNow we need to create fp16 torch tensors from our fp32 versions that we were using previously.\n\nset1_tt_16, set2_tt_16 = set1_tt.to(torch.float16), set2_tt.to(torch.float16)\n\nNow we’ll do a quick sanity check to ensure the results are correct.\n\nprint(hardware_runtime_info)\nout = init_out()\nfind_overlap_gpu_numba_smem(set1_tt_16, set2_tt_16, out, tpb=16*16, cache_items_per_thread=1)\nprint('Output is correct?',torch.all(out==output_test_tt.cuda()))\nout\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nOutput is correct? tensor(False, device='cuda:0')\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [ 41295,  41296, 199998, 199999,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\nAs you can see the output is not correct. The question is did we make a mistake in our implementation or is the loss of precision when switching to float16 causing the problem.  Let’s compare the results with our test array output_test_tt and see if we initially spot any differences.\n\noutput_test_tt\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]])\nAs you can see the in last index of set1 the results are different. There are additional matches 41295,  41296. Let’s look at those values.\n\nset1_tt_16[-1],set2_tt_16[41295],set1_tt[-1],set2_tt[41295]\n\nAs you can see they are different between the float16 and float32 versions:\n(tensor([348.5000, 306.7500, 239.3750, 348.5000, 307.0000, 239.5000],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.2500, 298.7500, 239.2500, 348.5000, 307.0000, 239.3750],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.4000, 306.8360, 239.3870, 348.5600, 306.9960, 239.5470],\n        device='cuda:0'),\n tensor([348.2370, 298.6260, 239.2440, 348.3870, 307.0380, 239.3940],\n        device='cuda:0'))\nNow let’s use the check overlap function we built for the PyTorch GPU vectorize implementation, passing in values from both the float16 and float32 inputs to see if it produces different results. The check_overlap_torch function takes two 2-dimensional arrays and returns a list of tuples containing the indices of a match. Since we only want to compare the numbers in question, we need to expand the first dimension of our test elements, which we can do by indexing into them with None. This converts the 1D vector into a 1x6 array, which is required for this function.\n\ncheck_overlap_torch(set1_tt_16[-1][None,],set2_tt_16[41295][None],0)\n\n[(0, 0)]\n\ncheck_overlap_torch(set1_tt[-1][None,],set2_tt[41295][None],0)\n\n[]\nAs you can see, there is a match when using the float16 values but not with the float32 values. This indicates that float16 is not suitable for our implementation and this use case. Half precision is very common in deep learning workloads and works fine in most cases, so it’s something we should check. Although it is not suitable for our use case because it returns incorrect results, I am curious how it performs compared to the float32 implementations, so we’ll run it anyway to see how it works. This will allow you to compare from a benchmarking perspective, but it’s a good lesson that it might not work in all cases and it’s important to validate our outputs as we go.  This will take longer to run, as there are significantly more possible cache_items_per_thread to check because each element only takes up half the amount of shared memory, allowing us to effectively have twice as many elements in shared memory.\n\nprint(hardware_runtime_info)\nresults3 = []\nbest_result = None\nfor tpbn in tqdm(range(4,64+1)): #Lower than 4 takes forever and is not fastest\n    for ciptn in tqdm(range(1,4096//(tpbn*16)+1)): #cache items per thread\n        out = init_out()\n        if tpbn*16 * ciptn &gt; 4096:\n            pass\n        else:\n            out = init_out()\n            #We are going to run less test per iteration because there are a lot more tests to run\n            timeit_results = %timeit -o -q -r 1 -n 5 find_overlap_gpu_numba_smem(set1_tt_16, set2_tt_16, out, tpb=tpbn*16, cache_items_per_thread=ciptn)\n            # We need to remove our output check as we know it's not going to be correct, but we want to see how\n            # fast this runs compared to our float32 implementation.\n            # assert torch.all(out==output_test_tt.cuda())\n            if best_result is None or timeit_results.average &lt; best_result.average:\n                best_result = timeit_results\n                print(tpbn*16,ciptn,timeit_results)\n            results3.append({'timeit_results':timeit_results, 'threads_per_block':tpbn*16, 'cache_items_per_thread':ciptn})\n\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [39:45&lt;00:00,  7.12s/it]\n100% 64/64 [07:12&lt;00:00, 12.09s/it]\n64 1 377 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 2 355 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 3 354 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 51/51 [04:44&lt;00:00,  9.63s/it]\n100% 42/42 [03:20&lt;00:00,  8.20s/it]\n96 1 349 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n96 3 347 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 36/36 [02:33&lt;00:00,  7.05s/it]\n100% 32/32 [02:03&lt;00:00,  6.32s/it]\n128 1 336 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n128 3 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 28/28 [01:42&lt;00:00,  5.65s/it]\n100% 25/25 [01:20&lt;00:00,  5.05s/it]\n100% 23/23 [01:12&lt;00:00,  4.70s/it]\n100% 21/21 [01:01&lt;00:00,  4.32s/it]\n100% 19/19 [00:53&lt;00:00,  4.01s/it]\n100% 18/18 [00:48&lt;00:00,  3.79s/it]\n100% 17/17 [00:44&lt;00:00,  3.61s/it]\n100% 16/16 [00:40&lt;00:00,  3.43s/it]\n100% 15/15 [00:37&lt;00:00,  3.28s/it]\n100% 14/14 [00:33&lt;00:00,  3.12s/it]\n100% 13/13 [00:31&lt;00:00,  3.03s/it]\n100% 12/12 [00:26&lt;00:00,  2.80s/it]\n100% 12/12 [00:27&lt;00:00,  2.74s/it]\n100% 11/11 [00:25&lt;00:00,  2.70s/it]\n100% 11/11 [00:24&lt;00:00,  2.61s/it]\n100% 10/10 [00:21&lt;00:00,  2.45s/it]\n100% 10/10 [00:22&lt;00:00,  2.51s/it]\n100% 9/9 [00:18&lt;00:00,  2.35s/it]\n100% 9/9 [00:18&lt;00:00,  2.33s/it]\n100% 9/9 [00:18&lt;00:00,  2.31s/it]\n100% 8/8 [00:16&lt;00:00,  2.22s/it]\n100% 8/8 [00:16&lt;00:00,  2.21s/it]\n100% 8/8 [00:16&lt;00:00,  2.21s/it]\n100% 8/8 [00:15&lt;00:00,  2.08s/it]\n512 1 331 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 7/7 [00:15&lt;00:00,  2.28s/it]\n100% 7/7 [00:15&lt;00:00,  2.27s/it]\n100% 7/7 [00:14&lt;00:00,  2.14s/it]\n100% 7/7 [00:15&lt;00:00,  2.14s/it]\n100% 6/6 [00:12&lt;00:00,  2.14s/it]\n100% 6/6 [00:12&lt;00:00,  2.13s/it]\n100% 6/6 [00:12&lt;00:00,  2.08s/it]\n100% 6/6 [00:11&lt;00:00,  1.99s/it]\n100% 6/6 [00:12&lt;00:00,  2.04s/it]\n100% 6/6 [00:12&lt;00:00,  2.03s/it]\n100% 5/5 [00:10&lt;00:00,  2.04s/it]\n100% 5/5 [00:10&lt;00:00,  2.03s/it]\n100% 5/5 [00:09&lt;00:00,  1.88s/it]\n100% 5/5 [00:09&lt;00:00,  1.87s/it]\n100% 5/5 [00:09&lt;00:00,  1.88s/it]\n100% 5/5 [00:09&lt;00:00,  1.88s/it]\n100% 5/5 [00:09&lt;00:00,  1.95s/it]\n100% 5/5 [00:09&lt;00:00,  1.93s/it]\n100% 5/5 [00:09&lt;00:00,  1.97s/it]\n100% 4/4 [00:07&lt;00:00,  1.88s/it]\n100% 4/4 [00:07&lt;00:00,  1.79s/it]\n100% 4/4 [00:07&lt;00:00,  1.78s/it]\n100% 4/4 [00:07&lt;00:00,  1.79s/it]\n100% 4/4 [00:07&lt;00:00,  1.79s/it]\n100% 4/4 [00:07&lt;00:00,  1.85s/it]\n100% 4/4 [00:07&lt;00:00,  1.85s/it]\n100% 4/4 [00:07&lt;00:00,  1.87s/it]\n100% 4/4 [00:07&lt;00:00,  1.87s/it]\n100% 4/4 [00:07&lt;00:00,  1.90s/it]\n100% 4/4 [00:07&lt;00:00,  1.89s/it]\n100% 4/4 [00:06&lt;00:00,  1.68s/it]\n100% 4/4 [00:06&lt;00:00,  1.68s/it]\n \nSurpringly this kernel is not faster than the float32 kernel. I would have expected this to be a memory bandwidth bound kernel but it does not seem to be."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n#\nHello world!"
  },
  {
    "objectID": "posts/jupyter-notebook-demo-post/notebook.html",
    "href": "posts/jupyter-notebook-demo-post/notebook.html",
    "title": "Sharing Jupyter Notebooks",
    "section": "",
    "text": "This is a demonstration of some of the cool functionality available when sharing Jupyter Notebooks using Quarto. A big thanks to Isaac Flath for creating this notebook!\n\n1 Prep\n\nCollapseSetup & ImportsGet DataUser Defined Functions\n\n\n\n\n\n\n\nCode\n# Load the package\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n1.0.1 Dataset 1\n\n\nCode\n1+2\n\n\n3\n\n\n\n\n1.0.2 Dataset 2\nOther stuff\n\n\n\n\n\n\n\n\n\n2 Another Section\n\nCommon FunctionalityCollapse\n\n\nabc some text with various markdown style:\n\nA list\nin markdown\n\nwith a particular function highlighted in text for clarity\n\n\n\n\n\n\nTip With Caption\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\nI also sometimes use other types of blocks for example for highlighting quotes I may do.\n\nQuote: Some very interesting Quote can be put here\n\n\n\n\n\n\n\n\n\n3 Yet Another Section"
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "",
    "text": "In this post, we’ll explore how to host applications for free on your local network while making them securely accessible from anywhere over the internet using Cloudflare Tunnels. I’ve been running various applications this way for over a year, and it’s been a major upgrade in how I run services on my network. The ability to host applications locally while accessing them through any web browser has opened up exciting possibilities for running services without ongoing cloud costs."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#what-you-can-build",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#what-you-can-build",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "1 What You Can Build",
    "text": "1 What You Can Build\nBefore diving into the technical details, let’s look at some real examples of what you can run. I’m currently hosting:\n\nJupyter notebooks for AI development, running on my GPU AI workstation - it’s like a private version of Google Colab\nOpen WebUI - a free, open-source AI chat interface that can use both locally-running models via Ollama and OpenAI’s models through their API plus any other models that support the OpenAI API format - it’s like a private version of ChatGPT\nAccess to my router’s web configuration and network management\nApplication and network monitoring through Uptime Kuma\nDirect access to my Synology NAS and its applications such as Surveillance Station and File Hosting\nPiKVM for remote hardware management of my AI workstation\nA local TV streaming service using a HD HomeRun tuner that I built using Fast HTML and FFmpeg\nVarious development and testing environments\n\n\n\n\nExample: Open WebUI Chat Interface Running Locally\n\n\nAll of these applications run on my local hardware but are accessible through any web browser, needing only an internet connection, without the need to install a VPN."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#key-benefits",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#key-benefits",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "2 Key Benefits",
    "text": "2 Key Benefits\nHere’s why this approach is valuable:\n\nCost Efficiency: Running applications locally can be dramatically cheaper than cloud hosting. My NAS has over 30TB of storage - hosting this in the cloud would cost hundreds monthly.\nHardware Utilization: Use your existing hardware, including GPUs for AI workloads, without paying cloud compute costs.\nPrivacy Control: All data remains on your local network, with Cloudflare only acting as a secure gateway.\nSimplified Access: No VPN needed - just open a browser and log in. This is especially useful when using computers where you can’t install a VPN client.\nCertificate Management: Cloudflare handles all SSL certificates automatically - no more managing or renewing certificates manually.\nZero Port Forwarding: No need to expose ports on your router or manage dynamic DNS."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#how-cloudflare-tunnels-work",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#how-cloudflare-tunnels-work",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "3 How Cloudflare Tunnels Work",
    "text": "3 How Cloudflare Tunnels Work\nA Cloudflare Tunnel creates a secure connection between your local network and Cloudflare’s edge network through these components:\n\nA lightweight daemon runs locally, establishing an outbound connection to Cloudflare\nTraffic is routed through Cloudflare’s global network to your local services\nAll traffic is encrypted end-to-end\nAuthentication and access controls are managed at the edge\n\nThe architecture looks like this:\nCloudflare Tunnel Architecture:\nBrowser -&gt; Cloudflare Edge -&gt; Encrypted Tunnel -&gt; Local Daemon -&gt; Your Services\n\n\n\nCloudflare Tunnels Architecture Diagram\n\n\nThis design eliminates the need for inbound firewall rules while providing enterprise-grade security features."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#getting-started",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#getting-started",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "4 Getting Started",
    "text": "4 Getting Started\nLet’s walk through setting up your first Cloudflare Tunnel. You’ll need:\n\nA domain name (can be registered through Cloudflare or elsewhere)\nA Cloudflare account\nDocker installed on your local machine\nAn application you want to make accessible\n\n\n4.1 Initial Setup\nFirst, register your domain with Cloudflare or point your existing domain’s nameservers to Cloudflare. While propagation can technically take up to 24 hours, it usually completes within minutes.\nFor the smoothest experience, I recommend: - Setting up Docker before starting (Synology NAS comes with it pre-installed) - Installing Portainer for easy Docker management - Having your applications ready to expose - Follow along with the Cloudflare Zero Trust setup guide\n\n\n4.2 Running the Tunnel\nThe easiest way to run the tunnel is through Docker. Here’s a basic command:\ndocker run -d \\\n  --name cloudflared \\\n  --restart unless-stopped \\\n  cloudflare/cloudflared:latest \\\n  tunnel --no-autoupdate run --token YOUR_TUNNEL_TOKEN\n\n\n\nAdding a tunnel in Cloudflare Zero Trust\n\n\nThe --restart unless-stopped flag ensures the tunnel restarts automatically after system reboots. You should copy and paste the docker command from your Cloudflare Zero Trust dashboard which will have your token already included. Before running the command, you should edit it to include the extra arguments specified above.\n\n\n4.3 Adding Access Rules to Secure Your Applications\nThe first thing you’ll want to do after creating the tunnel is to set up Access Rules. This is where you’ll define who can access your applications. This is an important step to keeping your applications and local network secure.\nSet up a default access group and wildcard subdomain application definition to ensure all new applications are secure by default.\n\nNavigate to Access -&gt; Access Groups \nClick on the “Add a Group” button\nEnter a name for your access group (ex: “Default Access Group”)\nCheck the “Set as default group” checkbox\nUnder “Define group criteria” -&gt; “Include” -&gt; “Selector” select “Emails” and add the list of email addresses you want to have access to your applications separated by commas. Only include email addresses you trust and ones you’re sure you want to have access to your applications.\nClick “Add require” at the bottom of the section.\nUnder “Require” -&gt; “Selector” select “Country” and add the country you want to allow access from. This is a great extra security measure to further lock down access to your applications. You can always change this later.\nClick “Add exclude” at the bottom of the section.\nUnder “Exclude” -&gt; “Selector” select “Country” and add the country you want to block access from. I have added Russia, China, and North Korea to my exclude list because state sponsored hacking from these countries is more common.\nAdd any additional rules you want to further secure your applications.\nFinally, click “Save” at the bottom of the page.\n\n\n\n\nDefault access group configuration in Cloudflare Zero Trust\n\n\n\n\n4.4 Adding A Master Wildcard Subdomain to apply security by default to any new applications\n\nNavigate to Access -&gt; Applications\nClick on the “Add an application” button \nSelect “Self-hosted”\nFor “Application name” enter a name for your application (ex: “Master Wildcard”)\nUnder “Session Duration” select an appropriate duration for your session which is how long until the user will need to re-authenticate. If you plan on mostly using trusted devices you can set this to a longer duration but if you plan on using it on untrusted devices regularly you should set this to a shorter duration.\nFor “Subdomain” enter a wildcard *\nFor “Domain” select your domain name (ex: mydomain.com)\nUnder “Identity providers” select all of the identity providers you want to use to authenticate users. By default “One-time PIN” is selected which is Cloudflare’s magic link authentication. I recommend adding Google as a provider as well if you typically use Google accounts to access your applications. \nFill out any additional settings you want to apply to your application and the click “Next” at the bottom of the page.\nGive your policy a name in the “Policy name” field.\nEnsure that the group you created earlier is selected in the “Access group” field.\nAdd any additional rules you want to apply to your application and then click “Next” at the bottom of the page. \nSelect “HTTP only” under “Cookie settings” and add any additional settings you want to apply to your application and then click “Add application” at the bottom of the page to finish setting up your master wildcard subdomain.\n\n\n\n4.5 Adding Your First Application\nLet’s say you want to make a web application running on port 8080 accessible. In the Cloudflare Zero Trust dashboard:\n\nNavigate to Networks -&gt; Tunnels\nClick on the tunnel you want to edit and then click on the “Edit” button\nClick on the “Public Hostname” tab\nClick on the “Add a public hostname” button \nEnter your subdomain and select your domain (e.g., app.yourdomain.com)\nSet the type to HTTP(S) and the URL to {your service ip address - ex: 192.168.1.100}:{your service port - ex: 8080} 6b. If your service is running HTTPS, click “Additional application settings” -&gt; “TLS” -&gt; and turn on “No TLS verify” if you’re running a self-signed certificate\nClick on the “Save hostname” button \nIf you want to set up custom security settings for this application you can follow the steps above, but substitute the master wildcard subdomain for this application’s subdomain and whatever other settings you want to apply to this application."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#recommended-security-measures",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#recommended-security-measures",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "5 Recommended Security Measures",
    "text": "5 Recommended Security Measures\nWhile I’m not a security expert, here are some practices that have worked well for me:\n\nEmail Authentication: Create access groups and specify exactly which email addresses can access each application. This is a great additional layer of security to ensure that only trusted users have access to your applications.\nGeographic Restrictions: Limit access to only the countries you need. Unless you have a specific reason, exclude countries known for hosting malicious traffic.\nNetwork Segmentation: If possible, run your Cloudflare tunnel and exposed services on a separate VLAN. This provides an additional layer of protection for your main network.\nDefault Security: Set up a wildcard rule (*.yourdomain.com) with strict access controls as your default. This ensures new services are secure by default.\nTwo-Factor Authentication: Enable 2FA on your Cloudflare account - this is crucial since it controls access to all your services and your internal network.\n\nThese recommendations help create defense in depth without making the system overly complicated to manage. Each person should evaluate their own security needs and implement controls accordingly."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#performance-considerations",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#performance-considerations",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "6 Performance Considerations",
    "text": "6 Performance Considerations\nIn my testing with a 5Gbps Google Fiber connection, I’ve seen:\n\nPort Forwarding: ~2.5-3 Gbps\nCloudflare Tunnel: 500 Mbps download, 600-800 Mbps upload\n\nWhile this is significantly slower than port forwarding, it’s more than adequate for most web applications and development work. The free tier has been more than sufficient for my personal use, though you should be mindful of bandwidth consumption, especially with media streaming. Don’t abuse the free bandwidth by hosting large media files or streaming services using free Cloudflare Tunnels.\nFor context, most home internet connections are well below these speeds, so the tunnel performance won’t be your bottleneck in most cases."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#real-world-example-ai-development-environment",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#real-world-example-ai-development-environment",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "7 Real-World Example: AI Development Environment",
    "text": "7 Real-World Example: AI Development Environment\nLet’s look at how I’ve set up my AI development environment:\n\nJupyter notebook server running on my GPU workstation\nCloudflare Tunnel exposing it at jupyter.mydomain.com\nEmail authentication only allowing access from my email address\nGeographic restriction to my home country\nHTTPS automatically handled by Cloudflare\n\nThis gives me secure access to GPU computing resources from anywhere, without the cost or complexity of cloud services. It’s kind of like having my own private Google Colab instance without any of the limitations."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#docker-and-data-management",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#docker-and-data-management",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "8 Docker and Data Management",
    "text": "8 Docker and Data Management\nMost of my applications run in Docker containers, with volumes mapped to specific paths on my NAS. This approach: - Keeps application data separate from containers - Makes updates and container recreation simple - Ensures data persistence across container updates - Simplifies backup procedures\nWhen setting up new applications, always plan your data storage strategy first. A common mistake is storing important data inside containers, which can be lost when containers are removed or updated."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#looking-forward",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#looking-forward",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "9 Looking Forward",
    "text": "9 Looking Forward\nThe ability to easily host applications on your own hardware creates powerful opportunities for running new AI-based applications locally, developing custom services, and experimenting with new technologies without worrying about cloud costs. In addition, this skillset you build by setting up your own Cloudflare Tunnels is increasingly valuable as businesses look to optimize their infrastructure costs by hosting their own applications locally using Cloudflare Tunnels."
  },
  {
    "objectID": "posts/2024-12-29_host_apps_locally_free/index.html#conclusion",
    "href": "posts/2024-12-29_host_apps_locally_free/index.html#conclusion",
    "title": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels",
    "section": "10 Conclusion",
    "text": "10 Conclusion\nCloudflare Tunnels provides an excellent way to host personal projects and applications without the complexity of traditional networking setups or the ongoing costs of cloud services while providing enterprise-grade security features. While this guide focuses on personal projects, the same principles and technologies scale directly to business environments, from small startups to large enterprises. The skills you develop hosting your own applications this way are directly applicable to professional environments.\nI particularly appreciate how this setup lets me focus on building and running applications rather than managing infrastructure. While there’s always more to learn and improve, this approach has enabled me to build and experiment without worrying about complex networking or excessive cloud costs.\nRemember to evaluate your own security requirements and risk tolerance when implementing any solution. The configurations I’ve described work well for my needs, but your requirements may differ.\nIf you’re interested in learning more, I highly recommend checking out the Lawrence Systems video on Cloudflare Tunnels setup, which provides additional perspectives and implementation details. You can also check out the Cloudflare Tunnels documentation for more information."
  },
  {
    "objectID": "posts/2022-12-13_context_managers_1/notebook.html",
    "href": "posts/2022-12-13_context_managers_1/notebook.html",
    "title": "Context Managers",
    "section": "",
    "text": "This is a quick ‘today I learned’ (TIL) note on Python Context managers. Python context managers are used to wrap arbitrary code with entry (setup) and exit (cleanup) functions. One common places you’ll see them used is when reading data from a file.\n\n# Open file and read contents.\nwith open('test.txt','r') as f:\n    output = f.readlines()\nprint(output)\n\n['This file is called test.txt.\\n', \"This is what's on the second line.\"]\n\n\nIf we try and read from the file f, defined above, we will get an I/O exception because the file as already been closed.\n\ntry:\n    f.readlines()\nexcept Exception as e:\n    print(e)\n\nI/O operation on closed file.\n\n\nHere is the equivalent long hand way to read the data from the file:\n\nf = open('test.txt')\noutput = f.readlines()\nf.close()\nprint(output)\n\n['This file is called test.txt.\\n', \"This is what's on the second line.\"]\n\n\nAs you can see the syntax is more verbose, it would be easier to forget to close the file, and it’s much less clear to see at a glance when we’re operating on the file. This example is relatively trivial as we’re just reading all the lines of the text file into a list but you can probably imagine this could be a lot more complex if you were doing something more complicated like training a neural net.\nNow let’s write our own class that uses a conext manager to cement how they can be implemented.\n\nclass MyContextManagerClass:\n    def __enter__(self):\n        print(\"Entering the context...\")\n        return \"My enter message.\"\n    def __exit__(self, exception_type, exception_value, exception_traceback):\n        print(\"Leaving the context...\")\n        print(exception_type, exception_value, exception_traceback, sep=\"\\n\")\n\n\nwith MyContextManagerClass() as h:\n    print('hi', h)\n\nEntering the context...\nhi My enter message.\nLeaving the context...\nNone\nNone\nNone\n\n\nAs you can see the enter message was printed, the __enter__ return value was passed and then the exit message was printed. Now let’s see what happens if there is an error while within our context.\n\nwith MyContextManagerClass() as h:\n    print(h)\n    print(1/0)\n\nEntering the context...\nMy enter message.\nLeaving the context...\n&lt;class 'ZeroDivisionError'&gt;\ndivision by zero\n&lt;traceback object&gt;\n\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n/tmp/ipykernel_890/1730694347.py in &lt;module&gt;\n      3 with MyContextManagerClass() as h:\n      4     print(h)\n----&gt; 5     print(1/0)\n\nZeroDivisionError: division by zero\n\n\n\nAs you can see an error was thrown but the __exit__ function was run anyways.\nThere are many other ways you can implement and use context managers which you can read about here: Python Conext Managers. Hopefully I’ve given you a taste of what’s possible and given you a basic understanding of they they’re useful.\nHere are a few more examples for your reference:\nExample 1: Using the contextmanager decorator\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef closing(thing):\n    try:\n        print('Starting')\n        yield thing\n    finally:\n        print('Finishing:',thing)\n\n\nwith closing('a'):\n    print('hi')\n\nStarting\nhi\nFinishing: a\n\n\nExample 2: Using ContextDecorator\n\nfrom contextlib import ContextDecorator\n\nclass mycontext(ContextDecorator):\n    def __enter__(self):\n        print('Starting')\n        return self\n\n    def __exit__(self, *exc):\n        print('Finishing')\n        return False\n\n\n@mycontext()\ndef my_function():\n    print('The bit in the middle')\n\n\nmy_function()\n\nStarting\nThe bit in the middle\nFinishing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Self-Hosting Your Applications for Free: A Guide to Cloudflare Tunnels\n\n\n\n\n\n\nself-hosting\n\n\nhosting\n\n\ncloudflare\n\n\napps\n\n\nhome lab\n\n\nfree hosting\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA MODE - Accelerate your code with massively parallel programming plus some other tricks\n\n\n\n\n\n\npython\n\n\ncuda\n\n\nnumba\n\n\nmassively parallel programming\n\n\naccelerated computing\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers From Scratch\n\n\n\n\n\n\npython\n\n\ntransformers\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nContext Managers\n\n\n\n\n\n\npython\n\n\ncode\n\n\ntoday I learned\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nSharing Jupyter Notebooks\n\n\n\n\n\n\nquarto\n\n\ncode\n\n\njupyter\n\n\ndemo\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\nwelcome\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nMat Miller\n\n\n\n\n\n\nNo matching items"
  }
]