[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n#\nHello world!"
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html",
    "href": "posts/2023-06-10_transformers/notebook.html",
    "title": "Transformers From Scratch",
    "section": "",
    "text": "In this blog we’re going to walk through creating and training a transformer from scratch. We’ll go through each foundational element step by step and explain what is happening along the way. This blog is written in a Jupyter notebook which you can download and use to run the code yourself as you follow along. Running the code as you follow along and changing it to see how the output changes will help you learn the concepts better than reading alone. While this is a lengthy topic, please don’t be too alarmed with the length of the notebook or the amount of code. Most of it is copied from previous cells as we build up the transformer. Rather than just showing the code that was changed which would have shortened things up considerably, I chose to copy all required code down to the next cell to allow this entire notebook to be run from top to bottom. This should make it easier to run as well as allow you to experiment with each new concept as we go.\nI’ll be closely following Andrej Karpathy’s YouTube video ‘Let’s build GPT: from scratch, in code, spelled out.’. We’ll be heavily referencing the Attention Is All You Need paper as we go. If you would rather you can download the final version of the code from Andrej’s repo. The dataset we’ll be using for this can downloaded here. You may also find referencing Andrej’s nanoGPT repo helpful as well."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#initial-code-setup",
    "href": "posts/2023-06-10_transformers/notebook.html#initial-code-setup",
    "title": "Transformers From Scratch",
    "section": "7.1 Initial Code Setup",
    "text": "7.1 Initial Code Setup\nTo start with we’re going to modify our BigramLanguageModel to be a TransformerLanguageModel class.\n\nembedding_dim = 32 #The vector size of the token embeddings. Andrej used n_embed as the variable name.\n\nWe’re going to add an embedding dimension, change up the token embedding table and modify the token embedding lookup and logits calculation as we work towards modifying this class into a true transformer. We’ll iteratively test as we go to make sure it is still able to train correctly. Please read through the below code taking note of the comments explaining the changes being made.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size=vocab_size, embedding_dim=embedding_dim):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        #A basic linear layer to pass our token embeddings through. This is a preliminary step, not the final network.\n        #Note the input size in the embedding_dim and the output size is the number of tokens or vocab size.\n        #This is because we are going to be predicting the probability for every token in the vocab that it is the next token.\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n\n        #This will be our lookup table for all of the token embeddings. We'll have an entry for each token (aka vocab size)... \n        #...and each embedding will be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel - Lookup token embeddings\n        logits = self.language_model_head_linear_layer(token_embeddings) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Get predictions\n            logits, loss = self(idx) #This is calling `forward`\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are the token indicies (int).\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx #TODO: Stopped Here        \n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.3278) Validation Loss: tensor(4.3231)\nStep: 200 Training Loss: tensor(2.5421) Validation Loss: tensor(2.5626)\nStep: 400 Training Loss: tensor(2.4982) Validation Loss: tensor(2.5163)\nStep: 600 Training Loss: tensor(2.4936) Validation Loss: tensor(2.5354)\nStep: 800 Training Loss: tensor(2.4983) Validation Loss: tensor(2.5067)\nStep: 1000 Training Loss: tensor(2.5025) Validation Loss: tensor(2.5045)\nStep: 1200 Training Loss: tensor(2.4831) Validation Loss: tensor(2.5028)\nStep: 1400 Training Loss: tensor(2.4866) Validation Loss: tensor(2.5157)\nStep: 1600 Training Loss: tensor(2.4927) Validation Loss: tensor(2.5120)\nStep: 1800 Training Loss: tensor(2.4899) Validation Loss: tensor(2.5120)\nStep: 2000 Training Loss: tensor(2.4804) Validation Loss: tensor(2.5071)\nStep: 2200 Training Loss: tensor(2.4841) Validation Loss: tensor(2.5178)\nStep: 2400 Training Loss: tensor(2.4940) Validation Loss: tensor(2.4883)\nStep: 2600 Training Loss: tensor(2.4956) Validation Loss: tensor(2.5065)\nStep: 2800 Training Loss: tensor(2.4799) Validation Loss: tensor(2.5138)\nStep: 2999 Training Loss: tensor(2.4870) Validation Loss: tensor(2.5165)\n\n\n\nCExthy bridcowindakis s, bth\n\nHAPORDurayoule.\nS:\nO:\nIS:\nThachangs ar bthar usqur, vethar dilasoate arche my.\n\nHD:\n\nYom o mur\nYowhthetof isth bot mil ndill, bes ireeesenghin lat Heridrovets, and Win nghir.\nThabousel lind me l.\nHAser ce wiry ptupr aisspllwhy.\nHAntoul noroopetelaves\nMPOLI swod mothakleo Windo whth eiiby we ath dourive wee, ired t so mo she te\n\nAN ad nterurt f sor; irist m:\n\nThiny aleronth, af Pre?\n\nWISo myay INouk!\nKENoby sarardsal thes ghesthinin cour ay aney RDUES:\nI fr t ce.\nJ\nCPU times: user 17 s, sys: 438 ms, total: 17.4 s\nWall time: 17.4 s\n\n\nWe need to also encode the token position so we’ll need to add another embedding table for that which will be learned as well."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#building-up-to-self-attention",
    "href": "posts/2023-06-10_transformers/notebook.html#building-up-to-self-attention",
    "title": "Transformers From Scratch",
    "section": "7.2 Building Up To Self Attention",
    "text": "7.2 Building Up To Self Attention\nWe’ll go through the simple cumulative token average again using matrix multiplication and modify it over time to be self attention.\n\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\ntril\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\nweights = torch.zeros(T,T,dtype=torch.float)\nweights = weights.masked_fill(tril == 0, float('-inf'))\nweights = torch.softmax(weights, dim=-1)\nweights\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\n\nout = weights @ x\nout.shape\n\ntorch.Size([32, 8, 32])\n\n\nWhen using the cumulative mean the weights are fixed, but ideally we want the weights to variable and learned so each token can interact with other tokens a varying amount based on learned paramters of what is most important. Some tokens will find other tokens more interesting than others and we want that to be data dependent and learned through training.\nThe example Andrej gives is “maybe if I’m a vowel token I am more interested in past consonant tokens and I want the [consonant] data to flow to me, this is the problem that self attention solves”. The way that self attention solves this is that every single node or token will emit 2 vectors, a query and key vector. The query vector roughly represents “what am I looking for” and the key vector roughly represents “what do I contain”. The way we then get the affinities between each token is by performing a dot product (matrix multiplication) of all the query vectors against all of the key vectors. So for a given query vector, the dot product is calculated against all of the key vectors and the results of that become the weights that are used for each token. This is the weight variable we used above except now instead of being a fixed average, it varies per token and is learned. If the key and query are aligned they will produce a larger value when the dot product is taken between them which leads to a larger value in the weight matrix.\nLet’s take the above example and modify it to implement self attention.\nFirst we need to define our head size. We will use 16. This will be the side dimension of a matrix where each query and key vector are matrix multiplied together. To get the query and key vectors from the token embeddings we first need to run the token embedding through a linear layer for the query and key which will generate a vector of size head_size.\n\n#version 4: self attention\n#setup\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\n\nhead_size = 16 #Self attention head size\n\n#Learned vector to extract key vector from token embedding vector\nkey_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract query vector from token embedding vector\nquery_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Extract query and key values for every token in the batch in parallel\nkey = key_layer(x) # (B,T,head_size)\nquery = query_layer(x) # (B,T,head_size) #TODO:\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\nNow we will calculate the affinities (weights) between each token in each sequence by matrix multiplying all of the queries and keys. If we simply try to calculate query @ key it will fail because the shapes are not correct to be able to do matrix multiplication. In our case both key and query are of shape (B,T,head_size) which are incompatible shapes to be matrix multiplied together. We need to transpose, or rotate, the key in the T and head_size dimension so they can be matrix multiplied. We cannot simply use the .T transpose because it would transpose in the batch dimension as well which we do not want so instead we’ll specify which dimensions to transpose which we can do by calling key.transpose(-2, -1) which will transpose the last 2 dimensions.\n\n#Calculate affinity (weights)\n#(B,T,head_size) @ (B, head_size, T) which is (32,8,16) @ (32,16,8) -&gt; (B, T, T) which is (32,8,8)\nweights = query @ key.transpose(-2, -1) \nweights.shape\n\ntorch.Size([32, 8, 8])\n\n\n\nweights[0]\n\ntensor([[ 0.2746, -1.2489,  0.5325,  0.2690,  0.1749, -0.7806,  0.1727,  1.3460],\n        [ 1.1833,  1.3634,  0.8982, -0.2749, -0.6459, -0.9106, -0.1111, -1.6019],\n        [-0.5018, -0.0807, -1.0500, -0.7615,  0.3193, -0.1342,  0.3969, -1.8405],\n        [ 0.5840,  0.6687, -0.5924,  0.4017,  0.3058, -0.6051, -0.0642, -0.6784],\n        [-0.2627, -2.5447,  1.5607, -1.5866, -0.6412, -1.3504, -0.0761, -2.3587],\n        [-0.8941, -2.0453, -1.5232,  1.5689, -0.5483,  1.9307, -0.6665, -1.7059],\n        [-0.2565,  0.5645, -0.7255, -0.2768, -0.3469,  0.7811, -0.5242, -0.9621],\n        [ 0.7061,  1.1336,  0.4749,  0.7493,  0.5897,  1.3509,  0.7673, -0.4065]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNow we have weights that are calculated based on each token’s affinity to every other token. We then apply the same filtering that we did previously with our cumulative mean so we simply remove the line where the weights were set to zero. This will allow us to finally apply a learned weighting to each previous token embedding.\n\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\n\n# weights = torch.zeros(T,T,dtype=torch.float) #Removed now that weights are calculated\nweights = weights.masked_fill(tril == 0, float('-inf')) #Masks future tokens\nweights = torch.softmax(weights, dim=-1) #Provides even distribution (weights that add up to 1)\n\nout = weights @ x\nout.shape\n\ntorch.Size([32, 8, 32])\n\n\nYou can see the weights below. Notice they are no longer uniform. They can now be indivual and learned from the data.\n\nweights[0] #TODO\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4551, 0.5449, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3224, 0.4912, 0.1864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3096, 0.3370, 0.0955, 0.2580, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1213, 0.0124, 0.7510, 0.0323, 0.0831, 0.0000, 0.0000, 0.0000],\n        [0.0314, 0.0099, 0.0167, 0.3685, 0.0444, 0.5291, 0.0000, 0.0000],\n        [0.1066, 0.2423, 0.0667, 0.1045, 0.0974, 0.3009, 0.0816, 0.0000],\n        [0.1168, 0.1792, 0.0927, 0.1220, 0.1040, 0.2227, 0.1242, 0.0384]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNow each token will be able to calculate its affinity to all other tokens. You can see in the example by looking at the bottom row, that the 8th token has a high affinity for the 6th token because it has the highest value: \n[0.1168, 0.1792, 0.0927, 0.1220, 0.1040,0.2227, 0.1242, 0.0384]\nThere is one more part of self attention we need to look at. That is that when we aggregate the tokens out = weights @ x we don’t aggregate the tokens exactly, we aggregate the value, so in the same way that we calculate key and query via passing the token embedding through a linear layer, we will do the same to get the value.\n\n#version 4: self attention\n#setup\ntorch.manual_seed(TORCH_SEED)\nB,T,C = batch_size, context_length, embedding_dim\nx = torch.randn((B,T,C))\nprint('Batch Size (B):',B,'Context Length (T):',T,'Embedding Dimension (C):',C)\n\nhead_size = 16 #Self attention head size\n\n#Learned vector to extract key vector from token embedding vector\nkey_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract query vector from token embedding vector\nquery_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) \n\n#Learned vector to extract value vector from token embedding vector\nvalue_layer = nn.Linear(in_features=C, out_features=head_size, bias=False) #NEW\n\n#Extract query, key and value for every token in the batch in parallel\nkey = key_layer(x) # (B,T,head_size)\nquery = query_layer(x) # (B,T,head_size)\nvalue = value_layer(x) # (B,T,head_size) #NEW\n\n#Calculate affinity (weights)\n#(B,T,head_size) @ (B, head_size, T) which is (32,8,16) @ (32,16,8) -&gt; (B, T, T) which is (32,8,8)\nweights = query @ key.transpose(-2, -1) \nweights.shape\n\nBatch Size (B): 32 Context Length (T): 8 Embedding Dimension (C): 32\n\n\ntorch.Size([32, 8, 8])\n\n\nAnd now instead of calculating the output by matrix multiplying the weights by x we multiply the weights by value.\n\ntril = torch.tril(torch.ones(T,T,dtype=torch.long))\n\nweights = weights.masked_fill(tril == 0, float('-inf')) #Masks future tokens\nweights = torch.softmax(weights, dim=-1) #Provides even distribution (weights that add up to 1)\n\n# out = weights @ x\nout = weights @ value #NEW (B, T, T) @ (B, T, head_size) = (B, T, head_size)\nout.shape\n\ntorch.Size([32, 8, 16])\n\n\nNotice how the shape of out has changed from torch.Size([32, 8, 32]) to torch.Size([32, 8, 16]) now that the we are using value which is of length 16 instead of the token embedding x which was of length 32.\nYou can think of the token embedding x as private information of the token and it must be passed through the linear layers to get the query, key and value. You can think of it as the token embedding x has all the information about the token and:\nquery: represents the things that the token is interested in or wants.\nkey: represents the things the token has.\nvalue: represents, if you find the token interesting, the information the token wants to communicate.\nAdditional Notes on Attention: link 1) Attention is a communication mechanism. You can think of it as if you had nodes in a directed graph:\n\nEach node has a vector of information (token embedding) and it gets a weighted sum of all of the nodes that point to it. This is done in a data dependent manner, so it depends on what data is stored in each node at any point in time. Our graph does not look quite like the example. Instead, our graph has 8 nodes. The first node is pointed to by only itself, our second node is pointed to by the first node and itself, our third node is pointed to by our first and second nodes as well as itself and so on. This structure is common in auto-regressive scenarious. \nAuto-regressive in this context refers to a type of model that generates sequences by modeling the probability of each item in the sequence given its preceding items. In other words, autoregressive language models generate predictions step by step, where each prediction is dependent on the ones that came before it.\nIn principal attention can be applied to any arbitrary directed graph as it is just a communication mechanism between nodes.\n\nThere is no notion of space or position. Attention simply acts over a set of vectors in this graph. The nodes have no idea of where they are positioned within the space which is why we need to encode them positionally which gives them information that anchors them to a specific position. i.e. inherently the nodes, representing characters in our example, don’t know what position they occur in relative to the other nodes which is why we need to positionally encode them. You can contrast this with convolutional neural networks where the data and network inherently are modeled spatially. For example CNN’s are regularly used for computer vision applications. In these applications adjacent pixels are fed into the CNN where convolutional filters act in space preserving the spatial information about the data.\n\nAttention, in contrast with CNN’s, has no notion of space, so space or position or location need to be encoded into the nodes through some other mechanism, which in our case is a positional encoding vector. This position vector will be added to the token prior to it being processed through the linear layers.\nAdditional Notes: link * Each example across batch dimensions is processed completely independently. Information from an item in a batch does not affect information in another item within the batch. Different items within a batch never talk to eachother. * In an encoder network (block), you do not filter out future tokens, only in a decoder network. This means that in an encoder network, these lines from our previous example would be removed:\ntril = torch.tril(torch.ones(T,T,dtype=torch.long)) \nweights = weights.masked_fill(tril == 0, float('-inf'))\n\nThere are many instances where you want all of the nodes to talk to each other, such as in sentiment analysis for example, because later on in the network you are making a simple prediction on whether the text is positive or negative. Another example would be vision transformers where you want all image patches to talk to each other. In these instances you use an encoder block which does not have masking in contrast to the decoder block which is what we have been focusing on here. * There are different types of attention. What we’re looking at now is self attention. The reason this is self-attention is because the data comes from the same source (x). Attention can be much more general than self attention, in that the source of the data can be from a different source. For example in encoder decoder networks, the queries could be produced from x but the the keys and values could come from a completely different source, for example from different encoder blocks that we would want to condition on. A real world example of this could be translating from one language to another, where the original or input language comes from an separate encoder network. The encoder network provides the keys and values and the decoder network provides the queries. This is called cross attention and is where there is a separate set of nodes we would like to pull information from into our node. Self attention, again, is where we are pulling keys, queries and values from one set of nodes.\n\nSo far we’ve implemented most of the attenion equation from the original Attention is all you need paper. \nAttention(Q,K,V) = softmax((Q*K^T)/(sqrt(dk))*V Where: Q = Query, K = Key, V = Value, dk = dimension of the Q and K or ‘head’.\nThe piece we are missing is dividing by sqrt(dk) which makes this ‘scaled self attention’. To do this we need to divide weights by sqrt(dk) or the dimension of the Q,K head. This makes it so when Q,K are unit variance, weights will be unit variance too which is important so softmax will remain diffused and not be saturated too much, i.e. the dot products betweek Q and K can become very large which pushes the gradients through the softmax to become very small which negatively impact training. This is why we want to scale them first before taking the softmax.\nLet’s look at a real example of this:\nWhere q and k are a gausian or normal distributions so the mean of the values is 0 and the standard deviation is 1. When you compute the matrix multiplication between them you will notice that the variance of weights is quite high.\n\ntorch.manual_seed(TORCH_SEED)\nq = torch.randn(B,T,head_size)\nk = torch.randn(B,T,head_size)\nprint('Mean of q:',q.mean(),'Variance of q:',q.var(),'Mean of k:',k.mean(),'Variance of k:',k.var())\nweights = q @ k.transpose(-2,-1)\nprint('Shape of weights:',weights.shape,'Mean of weights:',weights.mean(),'Variance of weights:',weights.var(),\n      '\\nMin of weights:',weights.min(),'Max of weights:',weights.max())\n\nMean of q: tensor(0.0021) Variance of q: tensor(0.9985) Mean of k: tensor(-0.0129) Variance of k: tensor(1.0255)\nShape of weights: torch.Size([32, 8, 8]) Mean of weights: tensor(-0.0302) Variance of weights: tensor(17.3386) \nMin of weights: tensor(-16.3490) Max of weights: tensor(13.1295)\n\n\n\nweights[0,0]\n\ntensor([-3.9763,  1.5713, -1.4034, -2.8632,  4.2487,  1.1146, -6.5130, -2.7662])\n\n\nNow if you divide the dot product of q and k by the square root of the head_size you can see that it returns the variance of weights back to 1 instead of approximately 17 prior to scaling.\n\nimport math\nweights = (q @ k.transpose(-2,-1)) / math.sqrt(head_size) #TODO Output size is (B,T,T) (32,8,8)\nprint('Shape of weights:',weights.shape,'Mean of weights:',weights.mean(),'Variance of weights:',weights.var(),\n      '\\nMin of weights:',weights.min(),'Max of weights:',weights.max())\n\nShape of weights: torch.Size([32, 8, 8]) Mean of weights: tensor(-0.0075) Variance of weights: tensor(1.0837) \nMin of weights: tensor(-4.0872) Max of weights: tensor(3.2824)\n\n\n\nweights[0,0]\n\ntensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])\n\n\nWe’ll create a very basic function to plot the tensors to help visualize the results.\n\nimport matplotlib.pyplot as plt\ndef plot_1d_tensor(x):\n    print(x)\n    plt.bar(range(len(x)), x)\n\nAgain, the reason that scaling weights is important is because of the subsequent softmax that is applied. When large values are input into softmax it causes the gradients to be small and the output of the softmax to converge toward one-hot vectors. First we’ll start out with one of the example weights that has already been divided by math.sqrt(head_size).\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915]), dim=-1))\n\ntensor([0.0465, 0.1862, 0.0885, 0.0614, 0.3636, 0.1661, 0.0247, 0.0630])\n\n\n\n\n\n\n\n\n\nYou can see the the output of softmax here is diffuse. None of the output values are overly large or small. If you multiply these same values by math.sqrt(head_size), effectively undoing scaling we applied, you will see that the results after softmax are less evenly distributed or diffuse.\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])*math.sqrt(head_size), dim=-1))\n\ntensor([2.3960e-04, 6.1487e-02, 3.1406e-03, 7.2937e-04, 8.9464e-01, 3.8940e-02,\n        1.8965e-05, 8.0382e-04])\n\n\n\n\n\n\n\n\n\nIf you push it even further you can see that the second item in the vector continues to grow even though the value of each element, relative to eachother has not changed.\n\nplot_1d_tensor(F.softmax(torch.tensor([-0.9941,  0.3928, -0.3508, -0.7158,  1.0622,  0.2786, -1.6282, -0.6915])*head_size, dim=-1))\n\ntensor([5.1446e-15, 2.2311e-05, 1.5187e-10, 4.4175e-13, 9.9997e-01, 3.5890e-06,\n        2.0192e-19, 6.5168e-13])\n\n\n\n\n\n\n\n\n\nAs the input values to the softmax continue to grow the result of the softmax continues to converge to a one-hot encoded vector, which is where one of the values in the vector is 1 and all the rest are 0’s. In effect this would make it so 1 node will only draw information from one other node, which is generally not what we want. This is especially a problem during initialization of the network before training, as it can be difficult for the network to recover from this during training."
  },
  {
    "objectID": "posts/2023-06-10_transformers/notebook.html#continuing-model-definition",
    "href": "posts/2023-06-10_transformers/notebook.html#continuing-model-definition",
    "title": "Transformers From Scratch",
    "section": "7.3 Continuing model definition",
    "text": "7.3 Continuing model definition\nNow we’re going to create a Head module where we’ll implement a single self attention head which we’ll use in our transformer, replacing the bigram model. You can reference the video link here to follow along if you would like.\n\nclass Head(nn.Module):\n    \"\"\" one self attention head \"\"\"\n    def __init__(self, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.head_size = head_size\n        self.context_length = context_length\n        self.key_layer   = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.query_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.value_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones((self.context_length, self.context_length))))\n    \n    def forward(self, x):\n        B,T,C = x.shape\n        assert T &lt;= self.context_length #check that x.shape matches pre-defined dims\n        assert C == self.embedding_dim\n        q = self.query_layer(x) #(B,T,C) (batch size, context length, head_size\n        k = self.key_layer(x) #(B,T,C)\n        v = self.value_layer(x) #(B,T,C)\n        \n        #compute scores based on affinities\n        weights = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 # (B,T,C) @ (B,C,T) -&gt; (B,T,T)\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) #(B,T,T)\n        weights = F.softmax(input=weights, dim=-1) #(B,T,T)\n        \n        #perform weighted aggragation of the values\n        out = weights @ v # (B,T,T) @ (B,T,C) -&gt; (B,T,C)\n        return out\n\n# Head()(x)\n\nThe register_buffer method is utilized to incorporate the tril matrix as a part of the model’s state. This integration ensures that tril is consistently saved and loaded with the model, maintaining uniform behavior across various runs and settings. Crucially, being a buffer, tril is excluded from gradient calculations and is not included as a parameter during model optimization, thereby rendering it a non-trainable component of the model.\nTo make visualizing the training loss easier we’ll create a simple function to plot them.\n\ndef plot_losses(losses):\n    train_losses = [o['train'] for o in losses if o.get('train') is not None]\n    valid_losses = [o['valid'] for o in losses if o.get('valid') is not None]\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(valid_losses, label='Validation Loss')\n    plt.ylabel('Loss')\n    plt.title('Losses')\n    plt.legend()\n    plt.show()\n\nNow we’ll add our new Head implementation to the TransformerLanguageModel class and train a model to ensure everything is working as well as to get a baseline of the results. Note we are also adding a token_position_embedding_table to encode the token positions. This learned looked up value will be added to the token_embeddings.\n\nlearning_rate = 1e-3 # decreate the learning rate because self attention cannot tolerate very high learning rates.\nmax_iters = 5000\n\n\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.self_attention_head_linear_layer = Head(head_size=head_size, embedding_dim=embedding_dim, context_length=context_length)\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.head_size, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.self_attention_head_linear_layer(x) #apply one head of self attention\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n        \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.1743) Validation Loss: tensor(4.1712)\nStep: 200 Training Loss: tensor(3.1199) Validation Loss: tensor(3.1343)\nStep: 400 Training Loss: tensor(2.8712) Validation Loss: tensor(2.8892)\nStep: 600 Training Loss: tensor(2.7071) Validation Loss: tensor(2.7260)\nStep: 800 Training Loss: tensor(2.6324) Validation Loss: tensor(2.6392)\nStep: 1000 Training Loss: tensor(2.5896) Validation Loss: tensor(2.5849)\nStep: 1200 Training Loss: tensor(2.5460) Validation Loss: tensor(2.5497)\nStep: 1400 Training Loss: tensor(2.5158) Validation Loss: tensor(2.5259)\nStep: 1600 Training Loss: tensor(2.5000) Validation Loss: tensor(2.5051)\nStep: 1800 Training Loss: tensor(2.4885) Validation Loss: tensor(2.4980)\nStep: 2000 Training Loss: tensor(2.4632) Validation Loss: tensor(2.4858)\nStep: 2200 Training Loss: tensor(2.4572) Validation Loss: tensor(2.4797)\nStep: 2400 Training Loss: tensor(2.4632) Validation Loss: tensor(2.4467)\nStep: 2600 Training Loss: tensor(2.4587) Validation Loss: tensor(2.4553)\nStep: 2800 Training Loss: tensor(2.4338) Validation Loss: tensor(2.4533)\nStep: 3000 Training Loss: tensor(2.4402) Validation Loss: tensor(2.4562)\nStep: 3200 Training Loss: tensor(2.4409) Validation Loss: tensor(2.4492)\nStep: 3400 Training Loss: tensor(2.4249) Validation Loss: tensor(2.4487)\nStep: 3600 Training Loss: tensor(2.4376) Validation Loss: tensor(2.4395)\nStep: 3800 Training Loss: tensor(2.4166) Validation Loss: tensor(2.4278)\nStep: 4000 Training Loss: tensor(2.4102) Validation Loss: tensor(2.4275)\nStep: 4200 Training Loss: tensor(2.4191) Validation Loss: tensor(2.4384)\nStep: 4400 Training Loss: tensor(2.4178) Validation Loss: tensor(2.4217)\nStep: 4600 Training Loss: tensor(2.4077) Validation Loss: tensor(2.4109)\nStep: 4800 Training Loss: tensor(2.4062) Validation Loss: tensor(2.4189)\nStep: 4999 Training Loss: tensor(2.4043) Validation Loss: tensor(2.4176)\n\nAnd thef tridcowind tis n, ber\n\nHiset bobe toe.\nS:\nO-' my dalatanss:\nWant he uw hathe.\nWar dthas ate awice my.\n\nHaldaru zorou wabuts, tof is hy me mil ndill, aes iree sen cin lat Het drovets, and Win ng:\nWilerabous lplind peallllishe onchiry:\nAugr aiss hawty.\n\n'Thake norodpeeelaves\nMomy.\nWhod mothake onWindo whe Ceiiby, wout, fourive wees ired thoous\nAr-x's uhe kad nterthirf so;\nAngis! m:\nE nge male ont ffaf Pre?\n\nWISo myat houre!\n\nWidby ak\nSadsal thes ghe thidin cour ay aney Iry ts chan th voul\n\n\n\n\n\n\n\n\n\nNext we’ll add multi-head attention which is just computing multiple attention heads together in parallel and then concatenating the results. \n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads:int, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        \n        self.heads = nn.ModuleList([\n            Head(head_size=self.head_size, embedding_dim=self.embedding_dim, context_length=self.context_length) \n            for _ in range(self.num_heads)\n        ])\n        \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1) #Note the concat is in the last 'C' dimension =&gt; (B,T,C*num_heads)\n            \n\nNow let’s add our newly created multi-head attention back into our Model.\n\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        # self.self_attention_head_linear_layer = Head(head_size=head_size, embedding_dim=embedding_dim, context_length=context_length)\n        #4 heads of 8 dimensional self attention.\n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        # x = self.self_attention_head_linear_layer(x) #apply one head of self attention\n        x = self.multi_self_attention_heads_layer(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2248) Validation Loss: tensor(4.2250)\nStep: 200 Training Loss: tensor(3.0112) Validation Loss: tensor(3.0132)\nStep: 400 Training Loss: tensor(2.7330) Validation Loss: tensor(2.7487)\nStep: 600 Training Loss: tensor(2.6190) Validation Loss: tensor(2.6244)\nStep: 800 Training Loss: tensor(2.5537) Validation Loss: tensor(2.5700)\nStep: 1000 Training Loss: tensor(2.5222) Validation Loss: tensor(2.5220)\nStep: 1200 Training Loss: tensor(2.4785) Validation Loss: tensor(2.4870)\nStep: 1400 Training Loss: tensor(2.4509) Validation Loss: tensor(2.4563)\nStep: 1600 Training Loss: tensor(2.4205) Validation Loss: tensor(2.4278)\nStep: 1800 Training Loss: tensor(2.3966) Validation Loss: tensor(2.4144)\nStep: 2000 Training Loss: tensor(2.3658) Validation Loss: tensor(2.3828)\nStep: 2200 Training Loss: tensor(2.3729) Validation Loss: tensor(2.3910)\nStep: 2400 Training Loss: tensor(2.3579) Validation Loss: tensor(2.3466)\nStep: 2600 Training Loss: tensor(2.3544) Validation Loss: tensor(2.3499)\nStep: 2800 Training Loss: tensor(2.3267) Validation Loss: tensor(2.3427)\nStep: 3000 Training Loss: tensor(2.3259) Validation Loss: tensor(2.3410)\nStep: 3200 Training Loss: tensor(2.3180) Validation Loss: tensor(2.3313)\nStep: 3400 Training Loss: tensor(2.3070) Validation Loss: tensor(2.3142)\nStep: 3600 Training Loss: tensor(2.3024) Validation Loss: tensor(2.3078)\nStep: 3800 Training Loss: tensor(2.2728) Validation Loss: tensor(2.3038)\nStep: 4000 Training Loss: tensor(2.2630) Validation Loss: tensor(2.2855)\nStep: 4200 Training Loss: tensor(2.2825) Validation Loss: tensor(2.2850)\nStep: 4400 Training Loss: tensor(2.2734) Validation Loss: tensor(2.2868)\nStep: 4600 Training Loss: tensor(2.2629) Validation Loss: tensor(2.2753)\nStep: 4800 Training Loss: tensor(2.2425) Validation Loss: tensor(2.2706)\nStep: 4999 Training Loss: tensor(2.2440) Validation Loss: tensor(2.2609)\n\nAnd they tridcowd,\nThis so be madises bube to tavegr-'t theall ands:\nWant he us hat tot?\nWedtlas anes wice my.\n\nHDER:\nAt onoth\nYouts, tof is hy me mil nowlit,\nWheirwe sen cin lat Het drov the and the nown iserans!\n lolind teall thus, cocrivy prugh aiss hewty.\nHllings kne\nTo thig I whom.\n\nWhoul to ake onWinso whre piiby we atit,\nCrive winghience poo mo the thu the danterupt fis are;\nDe! muf thre male of,\nTo fis.\nFe I So myakny, be!\nWhied is:\nSadsal the E'd st huin couk ay andy Iry to cof my carey\n\n\n\n\n\n\n\n\n\nAs you can see there is quite an improvement in the loss, going from Validation Loss: tensor(2.4176) with a single attention head to Validation Loss: tensor(2.2609) with our multi-attention head that has 4 heads. Note, these losses may vary somewhat between training runs. The results are still nonsense, but are looking closer to the training text than previous attempts. The reason that multi-headed attention works better than a single self attention block is that it is helpful to have multiple communication channels between tokens so they can each be looking for different things over different communication channels. As an example one communication channel make be looking back at vowels or consonants while another might be looking for the previous space.\nIf you look at this transformer block diagram, you can see that we’ve implemented quite a bit of it so far. \nWe’ve implemented the output embeddings, positional embeddings, (the lower) masked multi-head attention, and the final linear and softmax layers. We are going to skip the multi-head attention block in the middle as that is only needed if your model has an encoder block, which ours does not. This leaves the feed forward network to implement which is just a simple multi layer perceptron. In addition the entire block between the positional encodings and final linear layer can be stacked on top of itself multiple times signified by Nx.\nHere is the equation for the feed forward network, which is a simple multi layer perceptron:  \n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"A simple linear network followed by a non-linearity\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        return self.ffn(x)\n\nNote: In the equation it defines a (linear layer), (relu), and (linear layer). We’ll add the final linear layer later. Now let’s add our FFN to our Transformer Model.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        #4 heads of 8 dimensional self attention.\n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.multi_self_attention_heads_layer(x) # (B,T,C)\n        x = self.feed_forward_network(x) # (B,T,C) NEW\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2022) Validation Loss: tensor(4.2019)\nStep: 200 Training Loss: tensor(2.9494) Validation Loss: tensor(2.9685)\nStep: 400 Training Loss: tensor(2.6759) Validation Loss: tensor(2.6864)\nStep: 600 Training Loss: tensor(2.5779) Validation Loss: tensor(2.5799)\nStep: 800 Training Loss: tensor(2.5171) Validation Loss: tensor(2.5197)\nStep: 1000 Training Loss: tensor(2.4739) Validation Loss: tensor(2.4704)\nStep: 1200 Training Loss: tensor(2.4210) Validation Loss: tensor(2.4257)\nStep: 1400 Training Loss: tensor(2.4079) Validation Loss: tensor(2.4105)\nStep: 1600 Training Loss: tensor(2.3843) Validation Loss: tensor(2.3845)\nStep: 1800 Training Loss: tensor(2.3682) Validation Loss: tensor(2.3731)\nStep: 2000 Training Loss: tensor(2.3387) Validation Loss: tensor(2.3475)\nStep: 2200 Training Loss: tensor(2.3342) Validation Loss: tensor(2.3500)\nStep: 2400 Training Loss: tensor(2.3180) Validation Loss: tensor(2.3127)\nStep: 2600 Training Loss: tensor(2.3176) Validation Loss: tensor(2.3160)\nStep: 2800 Training Loss: tensor(2.2881) Validation Loss: tensor(2.3087)\nStep: 3000 Training Loss: tensor(2.2834) Validation Loss: tensor(2.3059)\nStep: 3200 Training Loss: tensor(2.2796) Validation Loss: tensor(2.2901)\nStep: 3400 Training Loss: tensor(2.2719) Validation Loss: tensor(2.2743)\nStep: 3600 Training Loss: tensor(2.2675) Validation Loss: tensor(2.2681)\nStep: 3800 Training Loss: tensor(2.2428) Validation Loss: tensor(2.2751)\nStep: 4000 Training Loss: tensor(2.2294) Validation Loss: tensor(2.2524)\nStep: 4200 Training Loss: tensor(2.2468) Validation Loss: tensor(2.2545)\nStep: 4400 Training Loss: tensor(2.2373) Validation Loss: tensor(2.2437)\nStep: 4600 Training Loss: tensor(2.2310) Validation Loss: tensor(2.2448)\nStep: 4800 Training Loss: tensor(2.2182) Validation Loss: tensor(2.2522)\nStep: 4999 Training Loss: tensor(2.2135) Validation Loss: tensor(2.2291)\n\nWher bef bridcowf,\nThe lay ble\nbairet bube to tave O-' my dalllauss:\nWant he us he hertbar dilth anes with my thand a wizorm he offs, to fit her! Varl nowlit,\nWheiree sen cin lat Heacliov the and the nown!\nFerablesel lind teall thull cechir speave aiss hewty.\nHETBHUSIRCBETI:\nAlave whom\nIll, demet aklecal-'so wher piichs withe dour warce hidend thoouse the the the danderthirf son; igis! muf thre ifled at tise Pried my of.\n\nHKINGLER:\nWidby and adsal ther grest hoin cour ay aney Iry thel fronf veay\nCPU times: user 1min 15s, sys: 445 ms, total: 1min 15s\nWall time: 1min 15s\n\n\n\n\n\n\n\n\n\nOur loss has improved again from Validation Loss: tensor(2.2854) now to Validation Loss: tensor(2.2720) now that we’ve added the feed forward network.\nNext we need to create a Block module that incorporates everthing within the block on the transformer architecture diagram (grey box) which will then allow us to stack them.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        \n    def forward(self, x):\n        return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        \n\nNow we can add our new Transformer Block to our model and start stacking it.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        # self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=4, head_size=self.embedding_dim//4)\n        # self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n        ) #NEW\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        # x = self.multi_self_attention_heads_layer(x) # (B,T,C)\n        # x = self.feed_forward_network(x) # (B,T,C)\n        x = self.transformer_blocks(x) #NEW\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.2116) Validation Loss: tensor(4.2078)\nStep: 200 Training Loss: tensor(3.2643) Validation Loss: tensor(3.2907)\nStep: 400 Training Loss: tensor(3.1541) Validation Loss: tensor(3.1676)\nStep: 600 Training Loss: tensor(3.0360) Validation Loss: tensor(3.0239)\nStep: 800 Training Loss: tensor(2.8569) Validation Loss: tensor(2.8526)\nStep: 1000 Training Loss: tensor(2.7738) Validation Loss: tensor(2.7607)\nStep: 1200 Training Loss: tensor(2.6645) Validation Loss: tensor(2.6827)\nStep: 1400 Training Loss: tensor(2.6202) Validation Loss: tensor(2.6159)\nStep: 1600 Training Loss: tensor(2.5581) Validation Loss: tensor(2.5613)\nStep: 1800 Training Loss: tensor(2.5231) Validation Loss: tensor(2.5388)\nStep: 2000 Training Loss: tensor(2.5020) Validation Loss: tensor(2.5028)\nStep: 2200 Training Loss: tensor(2.4899) Validation Loss: tensor(2.4974)\nStep: 2400 Training Loss: tensor(2.4812) Validation Loss: tensor(2.4668)\nStep: 2600 Training Loss: tensor(2.4656) Validation Loss: tensor(2.4641)\nStep: 2800 Training Loss: tensor(2.4574) Validation Loss: tensor(2.4511)\nStep: 3000 Training Loss: tensor(2.4306) Validation Loss: tensor(2.4413)\nStep: 3200 Training Loss: tensor(2.4016) Validation Loss: tensor(2.4273)\nStep: 3400 Training Loss: tensor(2.3946) Validation Loss: tensor(2.3999)\nStep: 3600 Training Loss: tensor(2.3738) Validation Loss: tensor(2.3823)\nStep: 3800 Training Loss: tensor(2.3844) Validation Loss: tensor(2.3700)\nStep: 4000 Training Loss: tensor(2.3571) Validation Loss: tensor(2.3570)\nStep: 4200 Training Loss: tensor(2.3426) Validation Loss: tensor(2.3729)\nStep: 4400 Training Loss: tensor(2.3394) Validation Loss: tensor(2.3696)\nStep: 4600 Training Loss: tensor(2.3300) Validation Loss: tensor(2.3343)\nStep: 4800 Training Loss: tensor(2.3263) Validation Loss: tensor(2.3400)\nStep: 4999 Training Loss: tensor(2.3301) Validation Loss: tensor(2.3403)\n\nAnd thik bry cowd,\nThis bor thibe sou bobe to:\nave rud my thichanss:\nWarth fou qor, ve bar dilth afe aw cramy.\n\nHhy ar mereou waow somtof is he ce mil nowlincaes ireees, hein latiser lilv the and the non ond wans!\n\nAplind pealltliser cechiry: tur hais's, why hou to u nor\nTo thigh sond:\nIl wo to thake o Windo wher eiibk we ati dourive we hidend thoo mowr-x'd und kad nonrtf he sor; iris! mef thin inled,\nThe af Pre?\n\nKIS\nINUSH:\nNube!\n\nGiyd is:\nards beace Eghes bidin cou afar tey ir-ltome fronf ve y\nCPU times: user 2min 56s, sys: 421 ms, total: 2min 57s\nWall time: 2min 56s\n\n\n\n\n\n\n\n\n\nAs you can see the accuracy actually got worse. Given our new much more powerful model, this is not something that we want. As the depth of models increase they can become harder to train. Fortunately there are a few things that we can do about that. link\nFirst we can implement skip connections, also known as residual connections, which are depicted on the transformer architecture diagram as black lines that bypass the masked multi-head attention block and feed into the add and norm block. You can also see one bypassing the FFN. The idea for these originally came from deep residual networks paper. In this case we are going to add the input data back to the output of the blocks that are being skipped. When you use addition, the gradients are evenly distributed between both the skip branch and the block branch. An alternative that is sometimes used is a simple concatenation of the input and output of the skipped block. \nWhen we initialize the network before training we typically want to start off with very low weights for the branches that go through the blocks so the blocks contribute very little to the overall loss. This way the gradient signal makes its way through the entire network. Then during training the network will slowly increase the weights and participation of the blocks.\nNow let’s implement the skip connections in our TransfomerBlock module.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        \n    def forward(self, x):\n        # return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        x = x + self.multi_self_attention_heads_layer(x) # adding input back to the output of each block for skip connection. NEW\n        x = x + self.feed_forward_network(x) # adding input back to the output of each block for skip connection. NEW\n        return x\n        \n\nWe also need to add a projection layer to our MultiHeadAttention module as well as the feed forward network. This is a simple linear layer.\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads:int, head_size:int=head_size, embedding_dim:int=embedding_dim, context_length:int=context_length):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        \n        self.heads = nn.ModuleList([\n            Head(head_size=self.head_size, embedding_dim=self.embedding_dim, \n                 context_length=self.context_length) \n            for _ in range(self.num_heads)])\n        \n        self.projection_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=True) #NEW\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.projection_layer(out) #NEW\n        return out\n            \n\nIn the FFN rather than adding the same projection layer parameter we’ll simply just add an additional linear layer to the existing sequential module. Also we are going to fan out and then back in by a factor of 4 between the linear layers in the FFN to add additional computation.\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"A simple linear network followed by a non-linearity\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim*4),#Updated\n            nn.ReLU(),\n            nn.Linear(in_features=self.embedding_dim*4, out_features=self.embedding_dim) #NEW\n        )\n    def forward(self, x):\n        return self.ffn(x)\n\nNow let’s train the network again to see how we end up.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n        )\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.transformer_blocks(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.6328) Validation Loss: tensor(4.6313)\nStep: 200 Training Loss: tensor(2.5782) Validation Loss: tensor(2.5969)\nStep: 400 Training Loss: tensor(2.4491) Validation Loss: tensor(2.4365)\nStep: 600 Training Loss: tensor(2.3560) Validation Loss: tensor(2.3455)\nStep: 800 Training Loss: tensor(2.2816) Validation Loss: tensor(2.2922)\nStep: 1000 Training Loss: tensor(2.2414) Validation Loss: tensor(2.2609)\nStep: 1200 Training Loss: tensor(2.2245) Validation Loss: tensor(2.2473)\nStep: 1400 Training Loss: tensor(2.1878) Validation Loss: tensor(2.2126)\nStep: 1600 Training Loss: tensor(2.1557) Validation Loss: tensor(2.1949)\nStep: 1800 Training Loss: tensor(2.1444) Validation Loss: tensor(2.1952)\nStep: 2000 Training Loss: tensor(2.1448) Validation Loss: tensor(2.1569)\nStep: 2200 Training Loss: tensor(2.1297) Validation Loss: tensor(2.1741)\nStep: 2400 Training Loss: tensor(2.0952) Validation Loss: tensor(2.1558)\nStep: 2600 Training Loss: tensor(2.0832) Validation Loss: tensor(2.1392)\nStep: 2800 Training Loss: tensor(2.0740) Validation Loss: tensor(2.1216)\nStep: 3000 Training Loss: tensor(2.0602) Validation Loss: tensor(2.1131)\nStep: 3200 Training Loss: tensor(2.0669) Validation Loss: tensor(2.1428)\nStep: 3400 Training Loss: tensor(2.0427) Validation Loss: tensor(2.0881)\nStep: 3600 Training Loss: tensor(2.0371) Validation Loss: tensor(2.1069)\nStep: 3800 Training Loss: tensor(2.0253) Validation Loss: tensor(2.1075)\nStep: 4000 Training Loss: tensor(2.0300) Validation Loss: tensor(2.1037)\nStep: 4200 Training Loss: tensor(2.0191) Validation Loss: tensor(2.0958)\nStep: 4400 Training Loss: tensor(2.0207) Validation Loss: tensor(2.0896)\nStep: 4600 Training Loss: tensor(1.9983) Validation Loss: tensor(2.0888)\nStep: 4800 Training Loss: tensor(1.9998) Validation Loss: tensor(2.0826)\nStep: 4999 Training Loss: tensor(1.9828) Validation Loss: tensor(2.0681)\n\n\nKING RIVAR:\nI will to lay ble\n\nHAPOMENBELA:\nAnd thruans that hands:\nWaither us his vet?\n\nMEXENDEL:\nWarch, my feans' to zokn he oursertef it her than welll butes is eesen cin latistlivilv the do kine nown is wace!\n lill dise littius, on him speage aissell, yet lord.\nI mame, this down'st you, thee killo Wicho dhat evings to thed suis Then, it he poorter,-; the day danter firf sorre;\nI therf threy fleront than Pried by of.\n\nHENNG ERLANCE:\nYO:\nArd all his a for huin cour ay and your to-chan the!\n\nJ\nCPU times: user 3min 17s, sys: 490 ms, total: 3min 18s\nWall time: 3min 17s\n\n\n\n\n\n\n\n\n\nThis looks much better than our last run without the residual layers which had a loss of Validation Loss: tensor(2.4430) and it also beats the previous run before that had a los of Validation Loss: tensor(2.2720) with a final loss of Validation Loss: tensor(2.0940). Also, as you can see the text output, while still gibberish, is much better than in all previous runs.\nThe second trick that helps with training deep neural nets, in addition to residual blocks, is the Norm as depicted on the block which in our case is layer norm. Let’s implement and add that. link\n\nclass LayerNorm:\n    def __init__(self, dim:int, eps:float=1e-5):\n        self.dim = dim\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n    \n    def __call__(self, x):\n        x_mean = x.mean(dim=1, keepdim=True) # layer mean\n        x_variance = x.var(dim=1, keepdim=True) # layer variance\n        x_hat = (x - x_mean) / torch.sqrt(x_variance + self.eps) # normalize to the unit variance\n        self.out = self.gamma * x_hat + self.beta\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nSince the original attention is all you need paper came out, it has become more common to apply the norm prior to the blocks instead of after them with the add as is depicted on the transformer architecture diagram. We will follow what common practice is today. Also instead of using the layer norm we developed, we will use the Pytorch version instead.\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block: Communication folled by computation.\"\"\"\n    def __init__(self, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 num_heads:int=4):\n        #embedding_dim: embedding dimension, num_heads: the number of heads that we want\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = embedding_dim // num_heads\n        self.num_heads = num_heads\n        \n        self.multi_self_attention_heads_layer = MultiHeadAttention(num_heads=self.num_heads, head_size=self.head_size, \n                                                                   embedding_dim=embedding_dim, context_length=context_length)\n        self.feed_forward_network = FeedForwardNetwork(embedding_dim=self.embedding_dim)\n        self.layer_norm_1 = nn.LayerNorm(normalized_shape=self.embedding_dim) #NEW\n        self.layer_norm_2 = nn.LayerNorm(normalized_shape=self.embedding_dim) #NEW\n        \n    def forward(self, x):\n        # return self.feed_forward_network(self.multi_self_attention_heads_layer(x))\n        x = x + self.multi_self_attention_heads_layer(self.layer_norm_1(x)) # added layer norm. UPDATED\n        x = x + self.feed_forward_network(self.layer_norm_2(x)) # added layer norm. UPDATED\n        return x\n        \n\nThese layer norms are applied to each token embedding to ensure they start off having a unit gausian at initialization, but because of the trainable parameters, this may change during training. \nWe also need to add a layer norm after the last transformer block and before the last linear layer. Now let’s train the model again and see how it does.\n\n%%time\n#| output: true\ntorch.manual_seed(TORCH_SEED)\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size:int=vocab_size, embedding_dim:int=embedding_dim, context_length:int=context_length, \n                 head_size:int=head_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_length = context_length\n        self.head_size = head_size\n\n        #This will be our lookup table for embeddings. We'll have an entry for each token (aka vocab size) and each embedding will... \n        #...be a vector of dimension embedding_dim.\n        self.token_embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n        self.token_position_embedding_table = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.embedding_dim)\n        \n        self.transformer_blocks = nn.Sequential(\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            TransformerBlock(embedding_dim=embedding_dim, num_heads=4, context_length=context_length),\n            nn.LayerNorm(embedding_dim), #NEW\n        )\n        self.language_model_head_linear_layer = nn.Linear(in_features=self.embedding_dim, out_features=self.vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #Both idx and targets are (B,T) Batch x Time array of integers\n        B,T = idx.shape\n        token_embeddings = self.token_embedding_table(idx) #(B,T,C) Batch, Time, Channel\n        token_position_embeddings = self.token_position_embedding_table(torch.arange(T, device=device)) #(T,C)\n        x = token_embeddings + token_position_embeddings\n        x = self.transformer_blocks(x)\n        logits = self.language_model_head_linear_layer(x) #(B,T,C) Where C is now token logits of size vocab_size\n        \n        if targets is not None:\n            B,T,C = logits.shape\n            logits_reshaped = logits.view(B*T,C)\n            targets_reshaped = targets.view(B*T)\n            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n        else:\n            loss=None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        #idx is (B,T) array of indicies in the current context\n        for _ in range(max_new_tokens):\n            #Crop idx to the max size of our positional embeddings table \n            idx_crop = idx[:,-self.context_length:]\n            #Get predictions\n            logits, loss = self(idx_crop)\n            #Get the last time step from logits where the dimensions of the logits are (B,T,C)\n            logits_last_timestep = logits[:,-1,:] #Becomes (B,C)\n            # print('Shape of logits_last_timestep:',logits_last_timestep.shape) #confirming shape\n            #Apply softmax to get probabilities\n            probs = F.softmax(input=logits_last_timestep, dim=-1) #(B,C)\n            # print('Shape of probs:', probs.shape) #confirming shape\n            #Sample from the probs distribution.\n            idx_next = torch.multinomial(input=probs, num_samples=1) #(B,1) Returns (B,idxs) where idxs are random integer indicies.\n            # print('Shape of idx_next:',idx_next.shape,'and contents:',idx_next) #look at the shape and contents of idx_next\n            #Append the sampled indexes idx_next to idx\n            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n        return idx\n\nmodel = TransformerLanguageModel(vocab_size=vocab_size, embedding_dim=embedding_dim, context_length=context_length)\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\ntracked_losses = list()\nfor step in range(max_iters):\n    \n    if step % eval_iters == 0 or step == max_iters-1:\n        losses = estimate_loss()\n        tracked_losses.append(losses)\n        print('Step:',step,'Training Loss:',losses['train'],'Validation Loss:',losses['valid'])\n    \n    xb,yb = get_batch('train')\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nplot_losses(tracked_losses)\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(model.generate(context,max_new_tokens=500)[0].tolist()))\n\nStep: 0 Training Loss: tensor(4.3103) Validation Loss: tensor(4.3100)\nStep: 200 Training Loss: tensor(2.6644) Validation Loss: tensor(2.6888)\nStep: 400 Training Loss: tensor(2.4590) Validation Loss: tensor(2.4470)\nStep: 600 Training Loss: tensor(2.3602) Validation Loss: tensor(2.3479)\nStep: 800 Training Loss: tensor(2.2801) Validation Loss: tensor(2.2854)\nStep: 1000 Training Loss: tensor(2.2313) Validation Loss: tensor(2.2563)\nStep: 1200 Training Loss: tensor(2.2185) Validation Loss: tensor(2.2377)\nStep: 1400 Training Loss: tensor(2.1741) Validation Loss: tensor(2.2103)\nStep: 1600 Training Loss: tensor(2.1425) Validation Loss: tensor(2.1853)\nStep: 1800 Training Loss: tensor(2.1290) Validation Loss: tensor(2.1792)\nStep: 2000 Training Loss: tensor(2.1295) Validation Loss: tensor(2.1381)\nStep: 2200 Training Loss: tensor(2.1140) Validation Loss: tensor(2.1594)\nStep: 2400 Training Loss: tensor(2.0825) Validation Loss: tensor(2.1407)\nStep: 2600 Training Loss: tensor(2.0727) Validation Loss: tensor(2.1325)\nStep: 2800 Training Loss: tensor(2.0618) Validation Loss: tensor(2.1148)\nStep: 3000 Training Loss: tensor(2.0459) Validation Loss: tensor(2.1033)\nStep: 3200 Training Loss: tensor(2.0515) Validation Loss: tensor(2.1216)\nStep: 3400 Training Loss: tensor(2.0321) Validation Loss: tensor(2.0743)\nStep: 3600 Training Loss: tensor(2.0179) Validation Loss: tensor(2.0913)\nStep: 3800 Training Loss: tensor(2.0171) Validation Loss: tensor(2.0952)\nStep: 4000 Training Loss: tensor(2.0151) Validation Loss: tensor(2.0876)\nStep: 4200 Training Loss: tensor(1.9998) Validation Loss: tensor(2.0803)\nStep: 4400 Training Loss: tensor(2.0134) Validation Loss: tensor(2.0872)\nStep: 4600 Training Loss: tensor(1.9862) Validation Loss: tensor(2.0807)\nStep: 4800 Training Loss: tensor(1.9923) Validation Loss: tensor(2.0776)\nStep: 4999 Training Loss: tensor(1.9644) Validation Loss: tensor(2.0590)\n\nWill be Roridce.\n\nSTAOLOLIO:\nKI a set bube to takegry.\n\nMBROKING\nMy LANGANGENV KINCE:\nthat dight ane away, my feans' to zormuse off Lroof is here vail; dight,\nWhiiree,\nYou, will is therev the do;\nWhe now oir wans!\nAl lind teal.\n-huch courly speap; airse, why.\nHerents norfore elguls;\nProtle, demees kneoul-wou what eiich o' maits, rive ceessience poor gier; thume known,\nrefter so;\nAngatt must wity ale of whith Pried by of.\n\nHKING ESTEL:\nPrisar adaid the Edwart hiin courchard ny ity to chan the whi\nCPU times: user 3min 38s, sys: 526 ms, total: 3min 38s\nWall time: 3min 38s\n\n\n\n\n\n\n\n\n\nThe loss is now down to Validation Loss: tensor(2.0630) from Validation Loss: tensor(2.0940) during the last run.\n\n%reset -f\n#| output: true"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Transformers From Scratch\n\n\n\n\n\n\npython\n\n\ntransformers\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nContext Managers\n\n\n\n\n\n\npython\n\n\ncode\n\n\ntoday I learned\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nSharing Jupyter Notebooks\n\n\n\n\n\n\nquarto\n\n\ncode\n\n\njupyter\n\n\ndemo\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nMat Miller\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\nwelcome\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nMat Miller\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Mat Miller. I work on technology in the engineering and construction sector in the North America. My degree is in engineering and I am a self-taught coder. The purpose of this blog is to document my journey learning AI through the fast.ai course and to discuss other technology projects I find interesting."
  },
  {
    "objectID": "posts/jupyter-notebook-demo-post/notebook.html",
    "href": "posts/jupyter-notebook-demo-post/notebook.html",
    "title": "Sharing Jupyter Notebooks",
    "section": "",
    "text": "This is a demonstration of some of the cool functionality available when sharing Jupyter Notebooks using Quarto. A big thanks to Isaac Flath for creating this notebook!\n\n1 Prep\n\nCollapseSetup & ImportsGet DataUser Defined Functions\n\n\n\n\n\n\n\nCode\n# Load the package\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n1.0.1 Dataset 1\n\n\nCode\n1+2\n\n\n3\n\n\n\n\n1.0.2 Dataset 2\nOther stuff\n\n\n\n\n\n\n\n\n\n2 Another Section\n\nCommon FunctionalityCollapse\n\n\nabc some text with various markdown style:\n\nA list\nin markdown\n\nwith a particular function highlighted in text for clarity\n\n\n\n\n\n\nTip With Caption\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\nI also sometimes use other types of blocks for example for highlighting quotes I may do.\n\nQuote: Some very interesting Quote can be put here\n\n\n\n\n\n\n\n\n\n3 Yet Another Section"
  },
  {
    "objectID": "posts/2022-12-13_context_managers_1/notebook.html",
    "href": "posts/2022-12-13_context_managers_1/notebook.html",
    "title": "Context Managers",
    "section": "",
    "text": "This is a quick ‘today I learned’ (TIL) note on Python Context managers. Python context managers are used to wrap arbitrary code with entry (setup) and exit (cleanup) functions. One common places you’ll see them used is when reading data from a file.\n\n# Open file and read contents.\nwith open('test.txt','r') as f:\n    output = f.readlines()\nprint(output)\n\n['This file is called test.txt.\\n', \"This is what's on the second line.\"]\n\n\nIf we try and read from the file f, defined above, we will get an I/O exception because the file as already been closed.\n\ntry:\n    f.readlines()\nexcept Exception as e:\n    print(e)\n\nI/O operation on closed file.\n\n\nHere is the equivalent long hand way to read the data from the file:\n\nf = open('test.txt')\noutput = f.readlines()\nf.close()\nprint(output)\n\n['This file is called test.txt.\\n', \"This is what's on the second line.\"]\n\n\nAs you can see the syntax is more verbose, it would be easier to forget to close the file, and it’s much less clear to see at a glance when we’re operating on the file. This example is relatively trivial as we’re just reading all the lines of the text file into a list but you can probably imagine this could be a lot more complex if you were doing something more complicated like training a neural net.\nNow let’s write our own class that uses a conext manager to cement how they can be implemented.\n\nclass MyContextManagerClass:\n    def __enter__(self):\n        print(\"Entering the context...\")\n        return \"My enter message.\"\n    def __exit__(self, exception_type, exception_value, exception_traceback):\n        print(\"Leaving the context...\")\n        print(exception_type, exception_value, exception_traceback, sep=\"\\n\")\n\n\nwith MyContextManagerClass() as h:\n    print('hi', h)\n\nEntering the context...\nhi My enter message.\nLeaving the context...\nNone\nNone\nNone\n\n\nAs you can see the enter message was printed, the __enter__ return value was passed and then the exit message was printed. Now let’s see what happens if there is an error while within our context.\n\nwith MyContextManagerClass() as h:\n    print(h)\n    print(1/0)\n\nEntering the context...\nMy enter message.\nLeaving the context...\n&lt;class 'ZeroDivisionError'&gt;\ndivision by zero\n&lt;traceback object&gt;\n\n\nZeroDivisionError: division by zero\n\n\nAs you can see an error was thrown but the __exit__ function was run anyways.\nThere are many other ways you can implement and use context managers which you can read about here: Python Conext Managers. Hopefully I’ve given you a taste of what’s possible and given you a basic understanding of they they’re useful.\nHere are a few more examples for your reference:\nExample 1: Using the contextmanager decorator\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef closing(thing):\n    try:\n        print('Starting')\n        yield thing\n    finally:\n        print('Finishing:',thing)\n\n\nwith closing('a'):\n    print('hi')\n\nStarting\nhi\nFinishing: a\n\n\nExample 2: Using ContextDecorator\n\nfrom contextlib import ContextDecorator\n\nclass mycontext(ContextDecorator):\n    def __enter__(self):\n        print('Starting')\n        return self\n\n    def __exit__(self, *exc):\n        print('Finishing')\n        return False\n\n\n@mycontext()\ndef my_function():\n    print('The bit in the middle')\n\n\nmy_function()\n\nStarting\nThe bit in the middle\nFinishing"
  }
]