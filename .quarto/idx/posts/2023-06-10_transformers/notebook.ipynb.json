{"title":"Transformers From Scratch","markdown":{"yaml":{"title":"Transformers From Scratch","author":"Mat Miller","date":"01/06/2024","categories":["python","transformers"],"image":"transformer_coding_1.png","card-style":"summary","resources":["*.png","*.jpeg"],"format":{"html":{"number-sections":true,"toc":true,"code-fold":false,"self-contained":true}}},"headingText":"Getting started","containsRefs":false,"markdown":"\n\n[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/matdmiller/blog/blob/main/posts/2023-06-10_transformers/notebook.ipynb)<br><br>\nIn this blog we're going to walk through creating and training a transformer from scratch. We'll go through each foundational element step by step and explain what is happening along the way. This blog is written in a Jupyter notebook which you can [download](https://github.com/matdmiller/blog/blob/main/posts/2023-06-10_transformers/notebook.ipynb) and use to run the code yourself as you follow along. Running the code as you follow along and changing it to see how the output changes will help you learn the concepts better than reading alone. While this is a lengthy topic, please don't be too alarmed with the length of the notebook or the amount of code. Most of it is copied from previous cells as we build up the transformer. Rather than just showing the code that was changed which would have shortened things up considerably, I chose to copy all required code down to the next cell to allow this entire notebook to be run from top to bottom. This should make it easier to run as well as allow you to experiment with each new concept as we go.\n\nI'll be closely following [Andrej Karpathy's YouTube video 'Let's build GPT: from scratch, in code, spelled out.'](https://www.youtube.com/watch?v=kCc8FmEb1nY). We'll be heavily referencing the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper as we go. If you would rather you can download the final version of the code from [Andrej's repo](https://github.com/karpathy/ng-video-lecture). The dataset we'll be using for this can downloaded [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). You may also find referencing Andrej's [nanoGPT](https://github.com/karpathy/nanoGPT) repo helpful as well.\n\n\nTo get started we'll first download the mini shakespeare dataset.\n\nLet's open it up and take a peek at what's inside.\n\n# Tokenization\n\nNow that we have our dataset the first thing we need to do is break it down into tokens which will be fed into the model. This is known as tokenization. There are various techniques for tokenization, `sub-word` being used in most of the more modern LLM's. Because our dataset is small for this toy problem, we are instead going to tokenize by each individual character. This makes tokenzation much simpler to implement and will also significantly reduce the number of unique tokens. The first thing we need to do is to go through the dataset and get a list of all unique characters or tokens which is known as the vocab.\n\nThe neural networks are unable to take in characters directly. We must first convert the tokens to integers, which in our case means converting each individual character into integers. Those integers will then be used to index into the set of token embeddings. Token embeddings are learned vectors that represent each token that will be passed into the model. Let's create our character to index (integer) mapping.\n\nNow that we have our tokenizer we can go through and tokenize the dataset. This step may sometimes be done on the fly during training and inference instead of ahead of time with large datasets, but for our small dataset we can easily and quickly do this without significant memory utilization.\n\nWe are going to import pytorch and create a tensor for our encoded dataset. Pytorch is the library we'll be using to create and train the model. \n\nNow that we have the encoded text, we need to split it into a training and validation set. Creating a validation set will allow us to test the performance of our model after each epoch, or at an another interval of our choosing, to ensure that it's training correctly or improving over time. A typical split between training and validation is 90/10 which is what we'll use.\n\nNext we need to choose our context length. The context lenth is the maximum length of the sequence used when training the transformer. This is sometimes also referred to as the block size, which is how Andrej refers to it. When the transformer is trained, it is trained on each combination of tokens up to the maximum context length. For example if the context length was 5 the transformer would be trained on (0,1), (0,1,2), (0,1,2,3), and (0,1,2,3,4). Let's look at a more concrete example in code.\n\n# Data Loader\n\nNow we'll implement a function to get a batch of data from our training and validation datasets. We'll specify which dataset to pull from as a parameter and return the inputs and targets which we're naming x and y respectively. We'll go ahead and print out everything through the process so you can see what's going on.\n\nYou may have noticed that the inputs and targets appear to be the same, just shifted by one, which is correct. To help give you a better understanding on how the inputs and targets are combined, we printed out every combination of input and target. As you can see each input sequence is really multiple sequences starting with the first token in the sequence as the input and the second token in the sequence being the target all the way to the full input sequence being the input and the subsequent character being the target.\n\n# Bigram Model\n\nBefore jumping into using a transformer we'll start with a bigram model. A bigram model predicts the probability of one token following another. For example given the token for the letter 'a' what is the probability of each token in the vocab will be the next token.\n\nNow we'll add a loss function to the forward method. We will use the negative log likelihood, which is known in pytorch as the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html). To be able to use the cross entropy loss we'll need to reshape the output and targets to match the format that it expects. The model output should be a 2D tensor (B\\*T x C) and targets should be a 1D tensor (B\\*T). We need to squash the batch and time dimensions on the model output and the batch and time dimensions of the targets using the `.view` method.\n\nNow we're going to add a generate method to perform character generation from our model. Instead of explaining here what each generation step is doing, detailed code comments have been added before each step. There are also commented out `print` statements that you can uncomment and run to help you understand what's going on as well.<br>\n\nOne pytorch function you may not have seen before is [`torch.multinomial`](https://pytorch.org/docs/stable/generated/torch.multinomial.html). It returns `num_samples`, in our case 1, based on the weighted probability distribution of the predictions for each token.\n\nThe output of this is totally nonsensical noise because the model is not trained.  Next we'll get set up to train the model.\n\nThe first thing we'll do is create an optimizer:\n\nFor more info on the `Adam` optimizer or optimizers in general, check out this great [video](https://youtu.be/nlVOG2Nzc3k?si=Lj7UH14PG7u48F4M&t=995) from fast.ai. <br>\n<br>\nNext, we'll create a basic training loop.\n\nAs you can see the loss is improving, but only a little bit.  Next we'll train for longer.\n\nThe loss seems to be steadily decreasing. Let's train for a lot longer and then see what the results look like.\n\nOk, it looks like the loss has stopped improving. Now we'll see what the output looks like.\n\nThe results are still not great, but they're looking more reasonable than they were at first. There are now things that look like words and sentences. Keep in mind this model is predicting the next token or character solely based on the previous token so it doesn't really have a lot to go by.  Learning about the dataset, training loop and getting a basline with a simplistic model is always a good first step in any AI project. This allows you to see if further improvements are helping as well as gives you a baseline to compare your model with.\n\nBefore moving on, let's do a quick loop through the validation set to see what its loss is so we can use it for comparison in the future.\n\nThe final validation loss appears similar to the training loss which makes sense given how basic this model is. Now we'll move on to creating a transformer and see how it does compared to the our basic bigram model.\n\nClearing out variables from above to start fresh.\n\n# Code re-write in preparation for Transformers\n\nWe're going to re-write some of our code from before to clean things up before jumping into building the transformer.\n\nCreate the token mapping functions\n\nNext we'll tokenize our dataset. \n\nAnd split it into training and validation sets\n\nNext we'll set up a basic data loader to get data in batches\n\nNext we'll create a function to estimate the loss for our model. Typically this is calculated against the training set for each training step and at the end of each epoch for the validation set but to keep things simple we'll just calculate it when called based on the number of steps specified as `eval_iters` and take the mean for the training and validation sets respectively. This also helps smooth out the loss values.\n\nWe're going to keep the model framework from the Bigram model and add in the transfomer parts shortly. For now, we just need to make sure that our updated code still works.\n\n# Previous Token Averages - Building Intuition for Self Attention\n\nAttention was the key discovery that enabled the transformer architecture. The idea is that each token should be able to communicate with or look at each previous token in the sequence but not future tokens. For example given token number 4 in a sequence of 8 tokens, token 4 should be able to access token 1, 2 and 3, but not tokens 5 through 8. We will demonstrate this with a for loop implementation to cement the concept and then show the equivalent calculation using matrix multiplication which is how transformers are implemented in real life because the matrix multiplication is orders of magnitudes faster than basic nested `for` loops.\n\nIn this example we're going to take the average of the previous tokens, just for illustration purposes, not because the average is a good way to represent the data from previous tokens.\n\nEach item in x_bag_of_words should be the cumulative mean of all values in x up to that index. For index 0 you can see the results are the same and for index 1 you can quickly recognize that in fact `x_bag_of_words` is the cumulative mean of x at index 0 and 1.\n\nNext, we'll delve into the basic operations of matrix multiplication. Specifically, we're focusing on the multiplication of two matrices, denoted as `a` and `b`. In Python, matrix multiplication is symbolized using the `@` operator. The process of matrix multiplication entails multiplying the rows of the first matrix (in this case, matrix `a`) by the columns of the second matrix (here, matrix `b`) when dealing with 2-dimensional matrices. After multiplying, the results are summed up to give the final outcome.\n\nLet's consider a practical example to illustrate this: calculating the value of `c[0,0]`. To achieve this, we need to multiply the first row of matrix `a` with the first column of matrix `b`, and then sum the results. \n\nIn mathematical terms, it looks like this: `(a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])`. Using the example values below, the equation becomes `(1 * 1) + (2 * 3) + (3 * 5) = 22`. \n\nTo enhance your understanding and intuition of matrix multiplication, I recommend the website http://matrixmultiplication.xyz/. It provides interactive visualizations of the matrix multiplication process, which can help you to understand the concept.\n\nThe next function we need to learn about is the pytorch [`tril`](https://pytorch.org/docs/stable/generated/torch.tril.html) function. It zeros out the upper right portion of a matrix.  Let's look at a few quick examples to help visualize the concept.<br>\n\nFirst let's apply it to a 3x3 matrix of ones.\n\nNext we'll apply it to a 4x4 matrix of random numbers just to show this can be used with numbers other than ones.\n\nNow let's see what happens when we use the tril function with our matrix multiply from before:\n\nNext we'll switch out our first matrix with a matrix of ones instead.\n\nLet's examine the first row in `c` which contains the values `[1, 2]`. Notice that it is the cumulative sum of the first row of `b` and rows 2 and 3 in `b` are effectively masked out. Row 2 in `c` is the cumulative sum of rows 1 and 2 from `b` and so on. Hopefully you can see that, like in our `for` loop, we are effectively cumulatively summing up `b`.\n\nNow we're going to reproduce our `for` loop results using matrix multiplication.\n\nComparing to `x_bag_of_words`, we can see the values are different.  This is because we have calculated the cumulative sum, but not the mean. This is why at each batch's idx 0 the answer is correct as it's only summing 1 item and the sum divided by 1 is both the mean and sum.\n\nNow we'll divide by the number of items that were summed to get the mean. Just like before we're going to use a matrix operation to keep things fast. First we'll create the matrix:\n\nAnd now we'll divide our cumulative sum by the number of items summed.\n\nAnd check whether it matches our previously calculated bow, which it does.\n\nAs an aside, let's look at how long the x_bag_of_words took to calculate via a for loop vs our matrix version. As the number of calculations is tiny in this case, we're going to increase the complexity so you can more easily see the time differences.<br>\nFirst we'll do the for loop version:\n\nThe for loop version on my PC took 4.3 seconds: `4.33 s ± 24.2 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)`. <br>\nNow we'll test the matrix version:\n\nThe matrix multiplication version on the other hand took 20 ms. `19.8 ms ± 872 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)`<br>\nNow we'll confirm the results are the same\n\nAs you can see the results were the same but the matrix multiplication version finished in ~20ms vs ~4.3s (numbers may vary between runs and machines) for the for loop version which is ~220x faster and that is running the matmul on the cpu instead of gpu which would speed it up even further.\n\nAlternatively we can use Andrej's method which should be even more efficient because we're dividing the mask elements by the number of elements to be summed which is effectively moving the division by `num_toks_summed` out of our final timed calculation.\n\n`20.7 ms ± 1.21 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)` In this case the numbers are very close. This could be influenced by a number of factors, but in general it's best to try and compute things once vs for each iteration if possible.\n\nNow let's look at the 3rd version of calculating this using softmax. It should produce an identical result.\n\n`20.5 ms ± 1.97 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)`\n\nSoftmax is used to create probabilities that previous tokens will interact with the current token. Intuitvely you can think of this as not all previous tokens in a sequence carry equal weight or importance so this allows the model to assign a weigting to all previous tokens. All probabilities output from softmax add up to 1. `-inf` is used to mask out future tokens which should not be accesible to the current token. Running `-inf` through softmax yields a 0 probability. In the transformer instead of the `mask` values being either identical or 0 (ex: `[0.5, 0.5, 0, 0]`) the weighting of past tokens will be learned. i.e. certain tokens can communicate with certain other tokens with a stronger weight based on learned values from the data. Andrej calls this `wei` for 'weights', which is the weighting used to determine which tokens communicate with eachother. This is different that what we're doing now which is forcing all non-0 weights to be the same. What we're doing now is just building up the math and intuition on how attention works.\n\n# Self attention\n\n## Initial Code Setup\n\nTo start with we're going to modify our `BigramLanguageModel` to be a `TransformerLanguageModel` class.\n\nWe're going to add an embedding dimension, change up the token embedding table and modify the token embedding lookup and logits calculation as we work towards modifying this class into a true transformer. We'll iteratively test as we go to make sure it is still able to train correctly. Please read through the below code taking note of the comments explaining the changes being made.\n\nWe need to also encode the token position so we'll need to add another embedding table for that which will be learned as well.\n\n## Building Up To Self Attention\n\nWe'll go through the simple cumulative token average again using matrix multiplication and modify it over time to be self attention. \n\nWhen using the cumulative mean the weights are fixed, but ideally we want the weights to variable and learned so each token can interact with other tokens a varying amount based on learned paramters of what is most important. Some tokens will find other tokens more interesting than others and we want that to be data dependent and learned through training.<br>\n\n[The example Andrej gives](https://youtu.be/kCc8FmEb1nY?t=3820) is \"maybe if I'm a vowel token I am more interested in past consonant tokens and I want the [consonant] data to flow to me, this is the problem that self attention solves\". The way that self attention solves this is that every single node or token will emit 2 vectors, a query and key vector. The query vector roughly represents \"what am I looking for\" and the key vector roughly represents \"what do I contain\". The way we then get the affinities between each token is by performing a dot product (matrix multiplication) of all the query vectors against all of the key vectors. So for a given query vector, the dot product is calculated against all of the key vectors and the results of that become the weights that are used for each token. This is the `weight` variable we used above except now instead of being a fixed average, it varies per token and is learned. If the key and query are aligned they will produce a larger value when the dot product is taken between them which leads to a larger value in the `weight` matrix. \n\n<br>Let's take the above example and modify it to implement self attention.\n\n<br>First we need to define our head size. We will use 16. This will be the side dimension of a matrix where each query and key vector are matrix multiplied together. To get the query and key vectors from the token embeddings we first need to run the token embedding through a linear layer for the query and key which will generate a vector of size `head_size`.\n\nNow we will calculate the affinities (weights) between each token in each sequence by matrix multiplying all of the queries and keys. If we simply try to calculate `query @ key` it will fail because the shapes are not correct to be able to do matrix multiplication. In our case both `key` and `query` are of shape (B,T,head_size) which are incompatible shapes to be matrix multiplied together. We need to transpose, or rotate, the `key` in the T and head_size dimension so they can be matrix multiplied. We cannot simply use the `.T` transpose because it would transpose in the batch dimension as well which we do not want so instead we'll specify which dimensions to transpose which we can do by calling `key.transpose(-2, -1)` which will transpose the last 2 dimensions.\n\nNow we have weights that are calculated based on each token's affinity to every other token. We then apply the same filtering that we did previously with our cumulative mean so we simply remove the line where the `weights` were set to zero. This will allow us to finally apply a learned weighting to each previous token embedding.\n\nYou can see the weights below. Notice they are no longer uniform. They can now be indivual and learned from the data.\n\nNow each token will be able to calculate its affinity to all other tokens. You can see in the example by looking at the bottom row, that the 8th token has a high affinity for the 6th token because it has the highest value: <br>\n\n`[0.1168, 0.1792, 0.0927, 0.1220, 0.1040, `**`0.2227`**`, 0.1242, 0.0384]`<br>\n\nThere is [one more part](https://youtu.be/kCc8FmEb1nY?t=4207) of self attention we need to look at.  That is that when we aggregate the tokens `out = weights @ x` we don't aggregate the tokens exactly, we aggregate the `value`, so in the same way that we calculate `key` and `query` via passing the token embedding through a linear layer, we will do the same to get the `value`.\n\nAnd now instead of calculating the output by matrix multiplying the `weights` by `x` we multiply the weights by `value`.\n\nNotice how the shape of `out` has changed from `torch.Size([32, 8, 32])` to `torch.Size([32, 8, 16])` now that the we are using `value` which is of length 16 instead of the token embedding `x` which was of length 32.<br>\n\nYou can think of the token embedding `x` as private information of the token and it must be passed through the linear layers to get the `query`, `key` and `value`. You can think of it as the token embedding `x` has all the information about the token and:<br>\n\n`query`: represents the things that the token is interested in or wants.<br>\n\n`key`: represents the things the token has.<br>\n\n`value`: represents, if you find the token interesting, the information the token wants to communicate.\n\n**Additional Notes on Attention:** [link](https://youtu.be/kCc8FmEb1nY?t=4298)<br>\n1) Attention is a communication mechanism. You can think of it as if you had nodes in a directed graph:<br>\n\n![Image of a Directed Graph](./directed_graph.png)<br>\n\nEach node has a vector of information (token embedding) and it gets a weighted sum of all of the nodes that point to it. This is done in a data dependent manner, so it depends on what data is stored in each node at any point in time. Our graph does not look quite like the example.  Instead, our graph has 8 nodes. The first node is pointed to by only itself, our second node is pointed to by the first node and itself, our third node is pointed to by our first and second nodes as well as itself and so on. This structure is common in auto-regressive scenarious. <br>\n\n**Auto-regressive** in this context refers to a type of model that generates sequences by modeling the probability of each item in the sequence given its preceding items. In other words, autoregressive language models generate predictions step by step, where each prediction is dependent on the ones that came before it.<br>\n\nIn principal attention can be applied to any arbitrary directed graph as it is just a communication mechanism between nodes.<br>\n\n2) There is no notion of space or position. Attention simply acts over a set of vectors in this graph. The nodes have no idea of where they are positioned within the space which is why we need to encode them positionally which gives them information that anchors them to a specific position. i.e. inherently the nodes, representing characters in our example, don't know what position they occur in relative to the other nodes which is why we need to positionally encode them. You can contrast this with convolutional neural networks where the data and network inherently are modeled spatially. For example CNN's are regularly used for computer vision applications. In these applications adjacent pixels are fed into the CNN where convolutional filters act in space preserving the spatial information about the data.<br>\n\nAttention, in contrast with CNN's, has no notion of space, so space or position or location need to be encoded into the nodes through some other mechanism, which in our case is a positional encoding vector. This position vector will be added to the token prior to it being processed through the linear layers.\n\n**Additional Notes:** [link](https://youtu.be/kCc8FmEb1nY?t=4420)<br>\n* Each example across `batch` dimensions is processed completely independently. Information from an item in a batch does not affect information in another item within the batch. Different items within a batch never talk to eachother.\n* In an encoder network (block), you do not filter out future tokens, only in a decoder network. This means that in an encoder network, these lines from our previous example would be removed:\n```python\ntril = torch.tril(torch.ones(T,T,dtype=torch.long)) \nweights = weights.masked_fill(tril == 0, float('-inf'))\n```\n> There are many instances where you want all of the nodes to talk to each other, such as in sentiment analysis for example, because later on in the network you are making a simple prediction on whether the text is positive or negative. Another example would be vision transformers where you want all image patches to talk to each other. In these instances you use an encoder block which does not have masking in contrast to the decoder block which is what we have been focusing on here.\n* There are different types of attention. What we're looking at now is self attention. The reason this is self-attention is because the data comes from the same source (`x`). Attention can be much more general than self attention, in that the source of the data can be from a different source. For example in encoder decoder networks, the queries could be produced from `x` but the the keys and values could come from a completely different source, for example from different encoder blocks that we would want to condition on. A real world example of this could be translating from one language to another, where the original or input language comes from an separate encoder network. The encoder network provides the `keys` and `values` and the decoder network provides the `queries`. This is called cross attention and is where there is a separate set of nodes we would like to pull information from into our node. Self attention, again, is where we are pulling `keys`, `queries` and `values` from one set of nodes.\n\nSo far we've implemented most of the attenion equation from the original [Attention is all you need paper](https://arxiv.org/abs/1706.03762).\n![Attention Equation from Attention is all you need paper](./attention_equation1.png)<br>\n\n`Attention(Q,K,V) = softmax((Q*K^T)/(sqrt(dk))*V`<br>\nWhere: Q = Query, K = Key, V = Value, dk = dimension of the Q and K or 'head'.<br>\n\nThe piece we are missing is dividing by `sqrt(dk)` which makes this **'scaled self attention'**. To do this we need to divide `weights` by `sqrt(dk)` or the dimension of the Q,K head. This makes it so when Q,K are unit variance, `weights` will be unit variance too which is important so `softmax` will remain diffused and not be saturated too much, i.e. the dot products betweek Q and K can become very large which pushes the gradients through the softmax to become very small which negatively impact training. This is why we want to scale them first before taking the `softmax`.<br>\n\nLet's look at a real example of this:<br>\n\nWhere `q` and `k` are a gausian or normal distributions so the mean of the values is 0 and the standard deviation is 1. When you compute the matrix multiplication between them you will notice that the variance of `weights` is quite high.\n\nNow if you divide the dot product of `q` and `k` by the square root of the head_size you can see that it returns the variance of `weights` back to 1 instead of approximately 17 prior to scaling.\n\nWe'll create a very basic function to plot the tensors to help visualize the results.\n\nAgain, the reason that scaling `weights` is important is because of the subsequent `softmax` that is applied. When large values are input into `softmax` it causes the gradients to be small and the output of the softmax to converge toward one-hot vectors. First we'll start out with one of the example weights that has already been divided by `math.sqrt(head_size)`.\n\nYou can see the the output of softmax here is diffuse. None of the output values are overly large or small. If you multiply these same values by `math.sqrt(head_size)`, effectively undoing scaling we applied, you will see that the results after softmax are less evenly distributed or diffuse.\n\nIf you push it even further you can see that the second item in the vector continues to grow even though the value of each element, relative to eachother has not changed.\n\nAs the input values to the softmax continue to grow the result of the `softmax` continues to converge to a one-hot encoded vector, which is where one of the values in the vector is 1 and all the rest are 0's. In effect this would make it so 1 node will only draw information from one other node, which is generally not what we want. This is especially a problem during initialization of the network before training, as it can be difficult for the network to recover from this during training.\n\n## Continuing model definition\n\nNow we're going to create a `Head` module where we'll implement a single self attention head which we'll use in our transformer, replacing the bigram model. You can reference the video [link](https://youtu.be/kCc8FmEb1nY?t=4752) here to follow along if you would like.\n\nThe `register_buffer` method is utilized to incorporate the tril matrix as a part of the model's state. This integration ensures that tril is consistently saved and loaded with the model, maintaining uniform behavior across various runs and settings. Crucially, being a buffer, tril is excluded from gradient calculations and is not included as a `parameter` during model optimization, thereby rendering it a non-trainable component of the model.\n\nTo make visualizing the training loss easier we'll create a simple function to plot them.\n\nNow we'll add our new `Head` implementation to the `TransformerLanguageModel` class and train a model to ensure everything is working as well as to get a baseline of the results. Note we are also adding a `token_position_embedding_table` to encode the token positions. This learned looked up value will be added to the `token_embeddings`.\n\nNext we'll add multi-head attention which is just computing multiple attention heads together in parallel and then concatenating the results. \n![Multi head attention formula](./multi_head_attention1.png)\n\nNow let's add our newly created multi-head attention back into our Model.\n\nAs you can see there is quite an improvement in the loss, going from `Validation Loss: tensor(2.4176)` with a single attention head to `Validation Loss: tensor(2.2609)` with our multi-attention head that has 4 heads. Note, these losses may vary somewhat between training runs.  The results are still nonsense, but are looking closer to the training text than previous attempts. The reason that multi-headed attention works better than a single self attention block is that it is helpful to have multiple communication channels between tokens so they can each be looking for different things over different communication channels. As an example one communication channel make be looking back at vowels or consonants while another might be looking for the previous space.<br>\n\nIf you look at this transformer block diagram, you can see that we've implemented quite a bit of it so far.<br>\n![transformer architecture diagram](./transformer_architecture_diagram2.png)<br>\n\nWe've implemented the output embeddings, positional embeddings, (the lower) masked multi-head attention, and the final linear and softmax layers. We are going to skip the multi-head attention block in the middle as that is only needed if your model has an encoder block, which ours does not. This leaves the feed forward network to implement which is just a simple multi layer perceptron. In addition the entire block between the positional encodings and final linear layer can be stacked on top of itself multiple times signified by Nx.<br>\n\nHere is the equation for the feed forward network, which is a simple multi layer perceptron: <br>\n![feed_forward_network_mlp1.png](./feed_forward_network_mlp1.png)\n\nNote: In the equation it defines a (linear layer), (relu), and (linear layer). We'll add the final linear layer later.<br>\nNow let's add our FFN to our Transformer Model.\n\nOur loss has improved again from `Validation Loss: tensor(2.2854)` now to `Validation Loss: tensor(2.2720)` now that we've added the feed forward network.<br>\n\nNext we need to create a Block module that incorporates everthing within the block on the transformer architecture diagram (grey box) which will then allow us to stack them.\n\nNow we can add our new Transformer Block to our model and start stacking it.\n\nAs you can see the accuracy actually got worse. Given our new much more powerful model, this is not something that we want. As the depth of models increase they can become harder to train. Fortunately there are a few things that we can do about that. [link](https://youtu.be/kCc8FmEb1nY?t=5208)<br>\n\nFirst we can implement skip connections, also known as residual connections, which are depicted on the transformer architecture diagram as black lines that bypass the masked multi-head attention block and feed into the add and norm block. You can also see one bypassing the FFN. The idea for these originally came from deep residual networks paper. In this case we are going to add the input data back to the output of the blocks that are being skipped. When you use addition, the gradients are evenly distributed between both the skip branch and the block branch. An alternative that is sometimes used is a simple concatenation of the input and output of the skipped block. <br>\n\nWhen we initialize the network before training we typically want to start off with very low weights for the branches that go through the blocks so the blocks contribute very little to the overall loss. This way the gradient signal makes its way through the entire network. Then during training the network will slowly increase the weights and participation of the blocks.\n\nNow let's implement the skip connections in our TransfomerBlock module.\n\nWe also need to add a projection layer to our MultiHeadAttention module as well as the feed forward network. This is a simple linear layer.\n\nIn the FFN rather than adding the same projection layer parameter we'll simply just add an additional linear layer to the existing sequential module. Also we are going to fan out and then back in by a factor of 4 between the linear layers in the FFN to add additional computation.\n\nNow let's train the network again to see how we end up.\n\nThis looks much better than our last run without the residual layers which had a loss of `Validation Loss: tensor(2.4430)` and it also beats the previous run before that had a los of `Validation Loss: tensor(2.2720)` with a final loss of `Validation Loss: tensor(2.0940)`. Also, as you can see the text output, while still gibberish, is much better than in all previous runs.<br>\n\nThe second trick that helps with training deep neural nets, in addition to residual blocks, is the Norm as depicted on the block which in our case is layer norm. Let's implement and add that. [link](https://youtu.be/kCc8FmEb1nY?t=5709)\n\nSince the original attention is all you need paper came out, it has become more common to apply the norm prior to the blocks instead of after them with the add as is depicted on the transformer architecture diagram. We will follow what common practice is today. Also instead of using the layer norm we developed, we will use the Pytorch version instead.\n\nThese layer norms are applied to each token embedding to ensure they start off having a unit gausian at initialization, but because of the trainable parameters, this may change during training. <br>\n\nWe also need to add a layer norm after the last transformer block and before the last linear layer. Now let's train the model again and see how it does.\n\nThe loss is now down to `Validation Loss: tensor(2.0630)` from `Validation Loss: tensor(2.0940)` during the last run.\n\n# Scaling Up\n\nNow that we have a fully functioning transformer network, to achieve better performance, we need to scale up. We'll be doing a bit of code cleanup and refactoring as we scale up the architecture. To make things easier to follow, I've reset the kernel so we'll be re-declaring everything again from scratch.\n\nAdding dropout\n\nAdding dropout\n\nAdding dropout\n\nThe results are starting to look pretty decent. The loss has dramatically improved. Scaling up the network has made a big difference. <br><br>\n**Debugging Models Aside**: While trying to train the model I realized I had made a mistake in the code. I tried to train this several times but loss would not drop below about 2.4. I went back through my code and nothing obvious was standing out to me that was wrong. After an hour of scouring my code I finally found the issue. It was a subtle change, but made all the difference. This was my code before the fix: <br>\n```python\nweights = (q @ k.transpose(-2,-1)) / self.head_size**-0.5\n```\nand this was my code after:<br>\n```python\nweights = (q @ k.transpose(-2,-1)) * self.head_size**-0.5\n```\nWhen we first implemented dividing the `weights` by the head size to fix the issue with large numbers passing through softmax, I had been using the `.../math.sqrt(head_size)`. To try and make things more consistent with Andrej's code and to remove the requirement to `import math` I switched the implementation over to the way he was doing it which was taking the head_size to the power of 0.5 which is equivalent to the sqrt, but I had missed that it was a -0.5 which is equivalent to 1/sqrt(0.5) so instead of dividing by `self.head_size**-0.5` I should have been multiplying by it. This can be one of the big challenges in deep learning. Often times when you make a mistake, no error is thrown, it just doesn't work. Sometimes it affects the results by a little and other times it affects it by a lot which is what happened in my case. Thankfully I had benchmarks to compare my result against so it was clear that I had an implementation issue, not that the model I was using was incapable of getting better results. \n\n# Conclusion\n\nIn this notebook we have built a transformer model based on the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper following along with Andrej Karpathy's fantastic YouTube video: [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY). While building out the transformer we tried to build an intuition on what makes the transformer work. I hope you found the format of this notebook useful, adding and modifying the code as we went, enabling you to follow along, run the code and see the output with each step.\n","srcMarkdownNoYaml":"\n\n[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/matdmiller/blog/blob/main/posts/2023-06-10_transformers/notebook.ipynb)<br><br>\nIn this blog we're going to walk through creating and training a transformer from scratch. We'll go through each foundational element step by step and explain what is happening along the way. This blog is written in a Jupyter notebook which you can [download](https://github.com/matdmiller/blog/blob/main/posts/2023-06-10_transformers/notebook.ipynb) and use to run the code yourself as you follow along. Running the code as you follow along and changing it to see how the output changes will help you learn the concepts better than reading alone. While this is a lengthy topic, please don't be too alarmed with the length of the notebook or the amount of code. Most of it is copied from previous cells as we build up the transformer. Rather than just showing the code that was changed which would have shortened things up considerably, I chose to copy all required code down to the next cell to allow this entire notebook to be run from top to bottom. This should make it easier to run as well as allow you to experiment with each new concept as we go.\n\nI'll be closely following [Andrej Karpathy's YouTube video 'Let's build GPT: from scratch, in code, spelled out.'](https://www.youtube.com/watch?v=kCc8FmEb1nY). We'll be heavily referencing the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper as we go. If you would rather you can download the final version of the code from [Andrej's repo](https://github.com/karpathy/ng-video-lecture). The dataset we'll be using for this can downloaded [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). You may also find referencing Andrej's [nanoGPT](https://github.com/karpathy/nanoGPT) repo helpful as well.\n\n# Getting started\n\nTo get started we'll first download the mini shakespeare dataset.\n\nLet's open it up and take a peek at what's inside.\n\n# Tokenization\n\nNow that we have our dataset the first thing we need to do is break it down into tokens which will be fed into the model. This is known as tokenization. There are various techniques for tokenization, `sub-word` being used in most of the more modern LLM's. Because our dataset is small for this toy problem, we are instead going to tokenize by each individual character. This makes tokenzation much simpler to implement and will also significantly reduce the number of unique tokens. The first thing we need to do is to go through the dataset and get a list of all unique characters or tokens which is known as the vocab.\n\nThe neural networks are unable to take in characters directly. We must first convert the tokens to integers, which in our case means converting each individual character into integers. Those integers will then be used to index into the set of token embeddings. Token embeddings are learned vectors that represent each token that will be passed into the model. Let's create our character to index (integer) mapping.\n\nNow that we have our tokenizer we can go through and tokenize the dataset. This step may sometimes be done on the fly during training and inference instead of ahead of time with large datasets, but for our small dataset we can easily and quickly do this without significant memory utilization.\n\nWe are going to import pytorch and create a tensor for our encoded dataset. Pytorch is the library we'll be using to create and train the model. \n\nNow that we have the encoded text, we need to split it into a training and validation set. Creating a validation set will allow us to test the performance of our model after each epoch, or at an another interval of our choosing, to ensure that it's training correctly or improving over time. A typical split between training and validation is 90/10 which is what we'll use.\n\nNext we need to choose our context length. The context lenth is the maximum length of the sequence used when training the transformer. This is sometimes also referred to as the block size, which is how Andrej refers to it. When the transformer is trained, it is trained on each combination of tokens up to the maximum context length. For example if the context length was 5 the transformer would be trained on (0,1), (0,1,2), (0,1,2,3), and (0,1,2,3,4). Let's look at a more concrete example in code.\n\n# Data Loader\n\nNow we'll implement a function to get a batch of data from our training and validation datasets. We'll specify which dataset to pull from as a parameter and return the inputs and targets which we're naming x and y respectively. We'll go ahead and print out everything through the process so you can see what's going on.\n\nYou may have noticed that the inputs and targets appear to be the same, just shifted by one, which is correct. To help give you a better understanding on how the inputs and targets are combined, we printed out every combination of input and target. As you can see each input sequence is really multiple sequences starting with the first token in the sequence as the input and the second token in the sequence being the target all the way to the full input sequence being the input and the subsequent character being the target.\n\n# Bigram Model\n\nBefore jumping into using a transformer we'll start with a bigram model. A bigram model predicts the probability of one token following another. For example given the token for the letter 'a' what is the probability of each token in the vocab will be the next token.\n\nNow we'll add a loss function to the forward method. We will use the negative log likelihood, which is known in pytorch as the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html). To be able to use the cross entropy loss we'll need to reshape the output and targets to match the format that it expects. The model output should be a 2D tensor (B\\*T x C) and targets should be a 1D tensor (B\\*T). We need to squash the batch and time dimensions on the model output and the batch and time dimensions of the targets using the `.view` method.\n\nNow we're going to add a generate method to perform character generation from our model. Instead of explaining here what each generation step is doing, detailed code comments have been added before each step. There are also commented out `print` statements that you can uncomment and run to help you understand what's going on as well.<br>\n\nOne pytorch function you may not have seen before is [`torch.multinomial`](https://pytorch.org/docs/stable/generated/torch.multinomial.html). It returns `num_samples`, in our case 1, based on the weighted probability distribution of the predictions for each token.\n\nThe output of this is totally nonsensical noise because the model is not trained.  Next we'll get set up to train the model.\n\nThe first thing we'll do is create an optimizer:\n\nFor more info on the `Adam` optimizer or optimizers in general, check out this great [video](https://youtu.be/nlVOG2Nzc3k?si=Lj7UH14PG7u48F4M&t=995) from fast.ai. <br>\n<br>\nNext, we'll create a basic training loop.\n\nAs you can see the loss is improving, but only a little bit.  Next we'll train for longer.\n\nThe loss seems to be steadily decreasing. Let's train for a lot longer and then see what the results look like.\n\nOk, it looks like the loss has stopped improving. Now we'll see what the output looks like.\n\nThe results are still not great, but they're looking more reasonable than they were at first. There are now things that look like words and sentences. Keep in mind this model is predicting the next token or character solely based on the previous token so it doesn't really have a lot to go by.  Learning about the dataset, training loop and getting a basline with a simplistic model is always a good first step in any AI project. This allows you to see if further improvements are helping as well as gives you a baseline to compare your model with.\n\nBefore moving on, let's do a quick loop through the validation set to see what its loss is so we can use it for comparison in the future.\n\nThe final validation loss appears similar to the training loss which makes sense given how basic this model is. Now we'll move on to creating a transformer and see how it does compared to the our basic bigram model.\n\nClearing out variables from above to start fresh.\n\n# Code re-write in preparation for Transformers\n\nWe're going to re-write some of our code from before to clean things up before jumping into building the transformer.\n\nCreate the token mapping functions\n\nNext we'll tokenize our dataset. \n\nAnd split it into training and validation sets\n\nNext we'll set up a basic data loader to get data in batches\n\nNext we'll create a function to estimate the loss for our model. Typically this is calculated against the training set for each training step and at the end of each epoch for the validation set but to keep things simple we'll just calculate it when called based on the number of steps specified as `eval_iters` and take the mean for the training and validation sets respectively. This also helps smooth out the loss values.\n\nWe're going to keep the model framework from the Bigram model and add in the transfomer parts shortly. For now, we just need to make sure that our updated code still works.\n\n# Previous Token Averages - Building Intuition for Self Attention\n\nAttention was the key discovery that enabled the transformer architecture. The idea is that each token should be able to communicate with or look at each previous token in the sequence but not future tokens. For example given token number 4 in a sequence of 8 tokens, token 4 should be able to access token 1, 2 and 3, but not tokens 5 through 8. We will demonstrate this with a for loop implementation to cement the concept and then show the equivalent calculation using matrix multiplication which is how transformers are implemented in real life because the matrix multiplication is orders of magnitudes faster than basic nested `for` loops.\n\nIn this example we're going to take the average of the previous tokens, just for illustration purposes, not because the average is a good way to represent the data from previous tokens.\n\nEach item in x_bag_of_words should be the cumulative mean of all values in x up to that index. For index 0 you can see the results are the same and for index 1 you can quickly recognize that in fact `x_bag_of_words` is the cumulative mean of x at index 0 and 1.\n\nNext, we'll delve into the basic operations of matrix multiplication. Specifically, we're focusing on the multiplication of two matrices, denoted as `a` and `b`. In Python, matrix multiplication is symbolized using the `@` operator. The process of matrix multiplication entails multiplying the rows of the first matrix (in this case, matrix `a`) by the columns of the second matrix (here, matrix `b`) when dealing with 2-dimensional matrices. After multiplying, the results are summed up to give the final outcome.\n\nLet's consider a practical example to illustrate this: calculating the value of `c[0,0]`. To achieve this, we need to multiply the first row of matrix `a` with the first column of matrix `b`, and then sum the results. \n\nIn mathematical terms, it looks like this: `(a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])`. Using the example values below, the equation becomes `(1 * 1) + (2 * 3) + (3 * 5) = 22`. \n\nTo enhance your understanding and intuition of matrix multiplication, I recommend the website http://matrixmultiplication.xyz/. It provides interactive visualizations of the matrix multiplication process, which can help you to understand the concept.\n\nThe next function we need to learn about is the pytorch [`tril`](https://pytorch.org/docs/stable/generated/torch.tril.html) function. It zeros out the upper right portion of a matrix.  Let's look at a few quick examples to help visualize the concept.<br>\n\nFirst let's apply it to a 3x3 matrix of ones.\n\nNext we'll apply it to a 4x4 matrix of random numbers just to show this can be used with numbers other than ones.\n\nNow let's see what happens when we use the tril function with our matrix multiply from before:\n\nNext we'll switch out our first matrix with a matrix of ones instead.\n\nLet's examine the first row in `c` which contains the values `[1, 2]`. Notice that it is the cumulative sum of the first row of `b` and rows 2 and 3 in `b` are effectively masked out. Row 2 in `c` is the cumulative sum of rows 1 and 2 from `b` and so on. Hopefully you can see that, like in our `for` loop, we are effectively cumulatively summing up `b`.\n\nNow we're going to reproduce our `for` loop results using matrix multiplication.\n\nComparing to `x_bag_of_words`, we can see the values are different.  This is because we have calculated the cumulative sum, but not the mean. This is why at each batch's idx 0 the answer is correct as it's only summing 1 item and the sum divided by 1 is both the mean and sum.\n\nNow we'll divide by the number of items that were summed to get the mean. Just like before we're going to use a matrix operation to keep things fast. First we'll create the matrix:\n\nAnd now we'll divide our cumulative sum by the number of items summed.\n\nAnd check whether it matches our previously calculated bow, which it does.\n\nAs an aside, let's look at how long the x_bag_of_words took to calculate via a for loop vs our matrix version. As the number of calculations is tiny in this case, we're going to increase the complexity so you can more easily see the time differences.<br>\nFirst we'll do the for loop version:\n\nThe for loop version on my PC took 4.3 seconds: `4.33 s ± 24.2 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)`. <br>\nNow we'll test the matrix version:\n\nThe matrix multiplication version on the other hand took 20 ms. `19.8 ms ± 872 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)`<br>\nNow we'll confirm the results are the same\n\nAs you can see the results were the same but the matrix multiplication version finished in ~20ms vs ~4.3s (numbers may vary between runs and machines) for the for loop version which is ~220x faster and that is running the matmul on the cpu instead of gpu which would speed it up even further.\n\nAlternatively we can use Andrej's method which should be even more efficient because we're dividing the mask elements by the number of elements to be summed which is effectively moving the division by `num_toks_summed` out of our final timed calculation.\n\n`20.7 ms ± 1.21 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)` In this case the numbers are very close. This could be influenced by a number of factors, but in general it's best to try and compute things once vs for each iteration if possible.\n\nNow let's look at the 3rd version of calculating this using softmax. It should produce an identical result.\n\n`20.5 ms ± 1.97 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)`\n\nSoftmax is used to create probabilities that previous tokens will interact with the current token. Intuitvely you can think of this as not all previous tokens in a sequence carry equal weight or importance so this allows the model to assign a weigting to all previous tokens. All probabilities output from softmax add up to 1. `-inf` is used to mask out future tokens which should not be accesible to the current token. Running `-inf` through softmax yields a 0 probability. In the transformer instead of the `mask` values being either identical or 0 (ex: `[0.5, 0.5, 0, 0]`) the weighting of past tokens will be learned. i.e. certain tokens can communicate with certain other tokens with a stronger weight based on learned values from the data. Andrej calls this `wei` for 'weights', which is the weighting used to determine which tokens communicate with eachother. This is different that what we're doing now which is forcing all non-0 weights to be the same. What we're doing now is just building up the math and intuition on how attention works.\n\n# Self attention\n\n## Initial Code Setup\n\nTo start with we're going to modify our `BigramLanguageModel` to be a `TransformerLanguageModel` class.\n\nWe're going to add an embedding dimension, change up the token embedding table and modify the token embedding lookup and logits calculation as we work towards modifying this class into a true transformer. We'll iteratively test as we go to make sure it is still able to train correctly. Please read through the below code taking note of the comments explaining the changes being made.\n\nWe need to also encode the token position so we'll need to add another embedding table for that which will be learned as well.\n\n## Building Up To Self Attention\n\nWe'll go through the simple cumulative token average again using matrix multiplication and modify it over time to be self attention. \n\nWhen using the cumulative mean the weights are fixed, but ideally we want the weights to variable and learned so each token can interact with other tokens a varying amount based on learned paramters of what is most important. Some tokens will find other tokens more interesting than others and we want that to be data dependent and learned through training.<br>\n\n[The example Andrej gives](https://youtu.be/kCc8FmEb1nY?t=3820) is \"maybe if I'm a vowel token I am more interested in past consonant tokens and I want the [consonant] data to flow to me, this is the problem that self attention solves\". The way that self attention solves this is that every single node or token will emit 2 vectors, a query and key vector. The query vector roughly represents \"what am I looking for\" and the key vector roughly represents \"what do I contain\". The way we then get the affinities between each token is by performing a dot product (matrix multiplication) of all the query vectors against all of the key vectors. So for a given query vector, the dot product is calculated against all of the key vectors and the results of that become the weights that are used for each token. This is the `weight` variable we used above except now instead of being a fixed average, it varies per token and is learned. If the key and query are aligned they will produce a larger value when the dot product is taken between them which leads to a larger value in the `weight` matrix. \n\n<br>Let's take the above example and modify it to implement self attention.\n\n<br>First we need to define our head size. We will use 16. This will be the side dimension of a matrix where each query and key vector are matrix multiplied together. To get the query and key vectors from the token embeddings we first need to run the token embedding through a linear layer for the query and key which will generate a vector of size `head_size`.\n\nNow we will calculate the affinities (weights) between each token in each sequence by matrix multiplying all of the queries and keys. If we simply try to calculate `query @ key` it will fail because the shapes are not correct to be able to do matrix multiplication. In our case both `key` and `query` are of shape (B,T,head_size) which are incompatible shapes to be matrix multiplied together. We need to transpose, or rotate, the `key` in the T and head_size dimension so they can be matrix multiplied. We cannot simply use the `.T` transpose because it would transpose in the batch dimension as well which we do not want so instead we'll specify which dimensions to transpose which we can do by calling `key.transpose(-2, -1)` which will transpose the last 2 dimensions.\n\nNow we have weights that are calculated based on each token's affinity to every other token. We then apply the same filtering that we did previously with our cumulative mean so we simply remove the line where the `weights` were set to zero. This will allow us to finally apply a learned weighting to each previous token embedding.\n\nYou can see the weights below. Notice they are no longer uniform. They can now be indivual and learned from the data.\n\nNow each token will be able to calculate its affinity to all other tokens. You can see in the example by looking at the bottom row, that the 8th token has a high affinity for the 6th token because it has the highest value: <br>\n\n`[0.1168, 0.1792, 0.0927, 0.1220, 0.1040, `**`0.2227`**`, 0.1242, 0.0384]`<br>\n\nThere is [one more part](https://youtu.be/kCc8FmEb1nY?t=4207) of self attention we need to look at.  That is that when we aggregate the tokens `out = weights @ x` we don't aggregate the tokens exactly, we aggregate the `value`, so in the same way that we calculate `key` and `query` via passing the token embedding through a linear layer, we will do the same to get the `value`.\n\nAnd now instead of calculating the output by matrix multiplying the `weights` by `x` we multiply the weights by `value`.\n\nNotice how the shape of `out` has changed from `torch.Size([32, 8, 32])` to `torch.Size([32, 8, 16])` now that the we are using `value` which is of length 16 instead of the token embedding `x` which was of length 32.<br>\n\nYou can think of the token embedding `x` as private information of the token and it must be passed through the linear layers to get the `query`, `key` and `value`. You can think of it as the token embedding `x` has all the information about the token and:<br>\n\n`query`: represents the things that the token is interested in or wants.<br>\n\n`key`: represents the things the token has.<br>\n\n`value`: represents, if you find the token interesting, the information the token wants to communicate.\n\n**Additional Notes on Attention:** [link](https://youtu.be/kCc8FmEb1nY?t=4298)<br>\n1) Attention is a communication mechanism. You can think of it as if you had nodes in a directed graph:<br>\n\n![Image of a Directed Graph](./directed_graph.png)<br>\n\nEach node has a vector of information (token embedding) and it gets a weighted sum of all of the nodes that point to it. This is done in a data dependent manner, so it depends on what data is stored in each node at any point in time. Our graph does not look quite like the example.  Instead, our graph has 8 nodes. The first node is pointed to by only itself, our second node is pointed to by the first node and itself, our third node is pointed to by our first and second nodes as well as itself and so on. This structure is common in auto-regressive scenarious. <br>\n\n**Auto-regressive** in this context refers to a type of model that generates sequences by modeling the probability of each item in the sequence given its preceding items. In other words, autoregressive language models generate predictions step by step, where each prediction is dependent on the ones that came before it.<br>\n\nIn principal attention can be applied to any arbitrary directed graph as it is just a communication mechanism between nodes.<br>\n\n2) There is no notion of space or position. Attention simply acts over a set of vectors in this graph. The nodes have no idea of where they are positioned within the space which is why we need to encode them positionally which gives them information that anchors them to a specific position. i.e. inherently the nodes, representing characters in our example, don't know what position they occur in relative to the other nodes which is why we need to positionally encode them. You can contrast this with convolutional neural networks where the data and network inherently are modeled spatially. For example CNN's are regularly used for computer vision applications. In these applications adjacent pixels are fed into the CNN where convolutional filters act in space preserving the spatial information about the data.<br>\n\nAttention, in contrast with CNN's, has no notion of space, so space or position or location need to be encoded into the nodes through some other mechanism, which in our case is a positional encoding vector. This position vector will be added to the token prior to it being processed through the linear layers.\n\n**Additional Notes:** [link](https://youtu.be/kCc8FmEb1nY?t=4420)<br>\n* Each example across `batch` dimensions is processed completely independently. Information from an item in a batch does not affect information in another item within the batch. Different items within a batch never talk to eachother.\n* In an encoder network (block), you do not filter out future tokens, only in a decoder network. This means that in an encoder network, these lines from our previous example would be removed:\n```python\ntril = torch.tril(torch.ones(T,T,dtype=torch.long)) \nweights = weights.masked_fill(tril == 0, float('-inf'))\n```\n> There are many instances where you want all of the nodes to talk to each other, such as in sentiment analysis for example, because later on in the network you are making a simple prediction on whether the text is positive or negative. Another example would be vision transformers where you want all image patches to talk to each other. In these instances you use an encoder block which does not have masking in contrast to the decoder block which is what we have been focusing on here.\n* There are different types of attention. What we're looking at now is self attention. The reason this is self-attention is because the data comes from the same source (`x`). Attention can be much more general than self attention, in that the source of the data can be from a different source. For example in encoder decoder networks, the queries could be produced from `x` but the the keys and values could come from a completely different source, for example from different encoder blocks that we would want to condition on. A real world example of this could be translating from one language to another, where the original or input language comes from an separate encoder network. The encoder network provides the `keys` and `values` and the decoder network provides the `queries`. This is called cross attention and is where there is a separate set of nodes we would like to pull information from into our node. Self attention, again, is where we are pulling `keys`, `queries` and `values` from one set of nodes.\n\nSo far we've implemented most of the attenion equation from the original [Attention is all you need paper](https://arxiv.org/abs/1706.03762).\n![Attention Equation from Attention is all you need paper](./attention_equation1.png)<br>\n\n`Attention(Q,K,V) = softmax((Q*K^T)/(sqrt(dk))*V`<br>\nWhere: Q = Query, K = Key, V = Value, dk = dimension of the Q and K or 'head'.<br>\n\nThe piece we are missing is dividing by `sqrt(dk)` which makes this **'scaled self attention'**. To do this we need to divide `weights` by `sqrt(dk)` or the dimension of the Q,K head. This makes it so when Q,K are unit variance, `weights` will be unit variance too which is important so `softmax` will remain diffused and not be saturated too much, i.e. the dot products betweek Q and K can become very large which pushes the gradients through the softmax to become very small which negatively impact training. This is why we want to scale them first before taking the `softmax`.<br>\n\nLet's look at a real example of this:<br>\n\nWhere `q` and `k` are a gausian or normal distributions so the mean of the values is 0 and the standard deviation is 1. When you compute the matrix multiplication between them you will notice that the variance of `weights` is quite high.\n\nNow if you divide the dot product of `q` and `k` by the square root of the head_size you can see that it returns the variance of `weights` back to 1 instead of approximately 17 prior to scaling.\n\nWe'll create a very basic function to plot the tensors to help visualize the results.\n\nAgain, the reason that scaling `weights` is important is because of the subsequent `softmax` that is applied. When large values are input into `softmax` it causes the gradients to be small and the output of the softmax to converge toward one-hot vectors. First we'll start out with one of the example weights that has already been divided by `math.sqrt(head_size)`.\n\nYou can see the the output of softmax here is diffuse. None of the output values are overly large or small. If you multiply these same values by `math.sqrt(head_size)`, effectively undoing scaling we applied, you will see that the results after softmax are less evenly distributed or diffuse.\n\nIf you push it even further you can see that the second item in the vector continues to grow even though the value of each element, relative to eachother has not changed.\n\nAs the input values to the softmax continue to grow the result of the `softmax` continues to converge to a one-hot encoded vector, which is where one of the values in the vector is 1 and all the rest are 0's. In effect this would make it so 1 node will only draw information from one other node, which is generally not what we want. This is especially a problem during initialization of the network before training, as it can be difficult for the network to recover from this during training.\n\n## Continuing model definition\n\nNow we're going to create a `Head` module where we'll implement a single self attention head which we'll use in our transformer, replacing the bigram model. You can reference the video [link](https://youtu.be/kCc8FmEb1nY?t=4752) here to follow along if you would like.\n\nThe `register_buffer` method is utilized to incorporate the tril matrix as a part of the model's state. This integration ensures that tril is consistently saved and loaded with the model, maintaining uniform behavior across various runs and settings. Crucially, being a buffer, tril is excluded from gradient calculations and is not included as a `parameter` during model optimization, thereby rendering it a non-trainable component of the model.\n\nTo make visualizing the training loss easier we'll create a simple function to plot them.\n\nNow we'll add our new `Head` implementation to the `TransformerLanguageModel` class and train a model to ensure everything is working as well as to get a baseline of the results. Note we are also adding a `token_position_embedding_table` to encode the token positions. This learned looked up value will be added to the `token_embeddings`.\n\nNext we'll add multi-head attention which is just computing multiple attention heads together in parallel and then concatenating the results. \n![Multi head attention formula](./multi_head_attention1.png)\n\nNow let's add our newly created multi-head attention back into our Model.\n\nAs you can see there is quite an improvement in the loss, going from `Validation Loss: tensor(2.4176)` with a single attention head to `Validation Loss: tensor(2.2609)` with our multi-attention head that has 4 heads. Note, these losses may vary somewhat between training runs.  The results are still nonsense, but are looking closer to the training text than previous attempts. The reason that multi-headed attention works better than a single self attention block is that it is helpful to have multiple communication channels between tokens so they can each be looking for different things over different communication channels. As an example one communication channel make be looking back at vowels or consonants while another might be looking for the previous space.<br>\n\nIf you look at this transformer block diagram, you can see that we've implemented quite a bit of it so far.<br>\n![transformer architecture diagram](./transformer_architecture_diagram2.png)<br>\n\nWe've implemented the output embeddings, positional embeddings, (the lower) masked multi-head attention, and the final linear and softmax layers. We are going to skip the multi-head attention block in the middle as that is only needed if your model has an encoder block, which ours does not. This leaves the feed forward network to implement which is just a simple multi layer perceptron. In addition the entire block between the positional encodings and final linear layer can be stacked on top of itself multiple times signified by Nx.<br>\n\nHere is the equation for the feed forward network, which is a simple multi layer perceptron: <br>\n![feed_forward_network_mlp1.png](./feed_forward_network_mlp1.png)\n\nNote: In the equation it defines a (linear layer), (relu), and (linear layer). We'll add the final linear layer later.<br>\nNow let's add our FFN to our Transformer Model.\n\nOur loss has improved again from `Validation Loss: tensor(2.2854)` now to `Validation Loss: tensor(2.2720)` now that we've added the feed forward network.<br>\n\nNext we need to create a Block module that incorporates everthing within the block on the transformer architecture diagram (grey box) which will then allow us to stack them.\n\nNow we can add our new Transformer Block to our model and start stacking it.\n\nAs you can see the accuracy actually got worse. Given our new much more powerful model, this is not something that we want. As the depth of models increase they can become harder to train. Fortunately there are a few things that we can do about that. [link](https://youtu.be/kCc8FmEb1nY?t=5208)<br>\n\nFirst we can implement skip connections, also known as residual connections, which are depicted on the transformer architecture diagram as black lines that bypass the masked multi-head attention block and feed into the add and norm block. You can also see one bypassing the FFN. The idea for these originally came from deep residual networks paper. In this case we are going to add the input data back to the output of the blocks that are being skipped. When you use addition, the gradients are evenly distributed between both the skip branch and the block branch. An alternative that is sometimes used is a simple concatenation of the input and output of the skipped block. <br>\n\nWhen we initialize the network before training we typically want to start off with very low weights for the branches that go through the blocks so the blocks contribute very little to the overall loss. This way the gradient signal makes its way through the entire network. Then during training the network will slowly increase the weights and participation of the blocks.\n\nNow let's implement the skip connections in our TransfomerBlock module.\n\nWe also need to add a projection layer to our MultiHeadAttention module as well as the feed forward network. This is a simple linear layer.\n\nIn the FFN rather than adding the same projection layer parameter we'll simply just add an additional linear layer to the existing sequential module. Also we are going to fan out and then back in by a factor of 4 between the linear layers in the FFN to add additional computation.\n\nNow let's train the network again to see how we end up.\n\nThis looks much better than our last run without the residual layers which had a loss of `Validation Loss: tensor(2.4430)` and it also beats the previous run before that had a los of `Validation Loss: tensor(2.2720)` with a final loss of `Validation Loss: tensor(2.0940)`. Also, as you can see the text output, while still gibberish, is much better than in all previous runs.<br>\n\nThe second trick that helps with training deep neural nets, in addition to residual blocks, is the Norm as depicted on the block which in our case is layer norm. Let's implement and add that. [link](https://youtu.be/kCc8FmEb1nY?t=5709)\n\nSince the original attention is all you need paper came out, it has become more common to apply the norm prior to the blocks instead of after them with the add as is depicted on the transformer architecture diagram. We will follow what common practice is today. Also instead of using the layer norm we developed, we will use the Pytorch version instead.\n\nThese layer norms are applied to each token embedding to ensure they start off having a unit gausian at initialization, but because of the trainable parameters, this may change during training. <br>\n\nWe also need to add a layer norm after the last transformer block and before the last linear layer. Now let's train the model again and see how it does.\n\nThe loss is now down to `Validation Loss: tensor(2.0630)` from `Validation Loss: tensor(2.0940)` during the last run.\n\n# Scaling Up\n\nNow that we have a fully functioning transformer network, to achieve better performance, we need to scale up. We'll be doing a bit of code cleanup and refactoring as we scale up the architecture. To make things easier to follow, I've reset the kernel so we'll be re-declaring everything again from scratch.\n\nAdding dropout\n\nAdding dropout\n\nAdding dropout\n\nThe results are starting to look pretty decent. The loss has dramatically improved. Scaling up the network has made a big difference. <br><br>\n**Debugging Models Aside**: While trying to train the model I realized I had made a mistake in the code. I tried to train this several times but loss would not drop below about 2.4. I went back through my code and nothing obvious was standing out to me that was wrong. After an hour of scouring my code I finally found the issue. It was a subtle change, but made all the difference. This was my code before the fix: <br>\n```python\nweights = (q @ k.transpose(-2,-1)) / self.head_size**-0.5\n```\nand this was my code after:<br>\n```python\nweights = (q @ k.transpose(-2,-1)) * self.head_size**-0.5\n```\nWhen we first implemented dividing the `weights` by the head size to fix the issue with large numbers passing through softmax, I had been using the `.../math.sqrt(head_size)`. To try and make things more consistent with Andrej's code and to remove the requirement to `import math` I switched the implementation over to the way he was doing it which was taking the head_size to the power of 0.5 which is equivalent to the sqrt, but I had missed that it was a -0.5 which is equivalent to 1/sqrt(0.5) so instead of dividing by `self.head_size**-0.5` I should have been multiplying by it. This can be one of the big challenges in deep learning. Often times when you make a mistake, no error is thrown, it just doesn't work. Sometimes it affects the results by a little and other times it affects it by a lot which is what happened in my case. Thankfully I had benchmarks to compare my result against so it was clear that I had an implementation issue, not that the model I was using was incapable of getting better results. \n\n# Conclusion\n\nIn this notebook we have built a transformer model based on the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper following along with Andrej Karpathy's fantastic YouTube video: [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY). While building out the transformer we tried to build an intuition on what makes the transformer work. I hope you found the format of this notebook useful, adding and modifying the code as we went, enabling you to follow along, run the code and see the output with each step.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"toc":true,"self-contained":true,"output-file":"notebook.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":{"render-on-save":true},"theme":"lux","title-block-banner":true,"title":"Transformers From Scratch","author":"Mat Miller","date":"01/06/2024","categories":["python","transformers"],"image":"transformer_coding_1.png","card-style":"summary","resources":["*.png","*.jpeg"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}