{"title":"CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks","markdown":{"yaml":{"title":"CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks","author":"Mat Miller","date":"05/31/2024","categories":["python","cuda","numba","massively parallel programming","accelerated computing"],"image":"cuda_mode_1.jpeg","resources":["*.zip","*.png","*.jpeg"],"format":{"html":{"number-sections":true,"toc":true,"code-fold":false,"self-contained":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/matdmiller/blog/blob/main/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.ipynb)\n\nIn this post we're going to create and optimize GPU CUDA kernels to accelerate an embarrasingly parallel workload and achieve a **8.4 million X** speedup over baseline. Everything you need to create and run CUDA kernels is contained within this blog post which is actually a Jupyter notebook that you can run interactively. This is a fully self contained example workload that can be run for free with no setup required on Google Colab. All you need to do is click the **Open in Google Colab** button at the top of this post.\n\nYou should run this notebook as you go in Google Colab or any other properly configured CUDA environment. No extra setup is required if you use Google Colab, but if you run in another environment you are responsible for setting it up which is beyond the scope of this post. Learning by doing is more effective than reading alone. Hacking around with the code and seeing how that affects the outcomes helps cement the concepts being taught and builds intuition.\n\nYou will see several methods for computing the workload covered in this post. We will benchmark each method so you observe how quickly they run and begin to build an intuition on much you might be able to accelerate your own workloads. As a rule of thumb you can typically excpect a 10-1,000x speedup when accelerating workloads on GPU's. Each step will be explained and you will learn tips and tricks throughout the post.\n\nThe problem we will be solving is based on a real problem I ran across recently. We had a 3D model with hundreds of thousands of objects. The objects were divided into sets. We needed to figure out which objects from set1 overlapped with objects in set2. Objects in set1 contained metadata which needed to be transferred to all overlapping objects from set2. Each set contained approximately 200,000 objects. Every object from set1 needed to be checked for overlap against every object in set2 which equated to 200,000 * 200,000 = 40 Billon checks that needed to be completed. We had the min and max x,y,z bounding box coordinates for every object which is what we used to check for overlap. When the problem was brought to me an initial attempt was made to solve it by looping through each set of objects in pandas and checking for overlapping objects. The problem was this approach was going to take almost a month to run. This was a perfect embarassingly parallel workload and happened to perfectly coincide with the [CUDA MODE](https://github.com/cuda-mode/lectures) class I was taking. It was the perfect opportunity to implement my newly learned CUDA skills on real world problem. Code is included in this notebook to create a synthetic dataset that matches this problem.\n\nWe will follow the same path I took when trying out different methods to solve this problem. We'll start out with the basic pandas for loop code that was brought to me initially and I'll show you a simple trick to speed up pandas for loops. Next we'll look at a solution using numpy and broadcasting. This is one of the simplest ways to parallelize workloads on CPU's. This reduced the runtime from 777 hours with our initial pandas for loop to 8 minutes. From a practical perspective this was an acceptable amount of time to wait for the answer, but I was curious how much faster I could make this a custom CUDA kernel. The results were surprising to say the least. The next easy step was to take the numpy broadcasting solution and conver it to pytorch and run it on the GPU. This brought the runtime down to 15 seconds, a 32x improvement. At this point I was skeptical how much better a custom CUDA kernel would really be able to speed this up. After all pytorch is an incredibly optimized library for performing AI training, would I really be able to beat it. It turns out the answer was yes! Not that my CUDA code is better than pytorch's, it's definitely not, however my code that was written for this specific problem ended up being a lot faster than the broadcasting solution in pytorch. I'll show you how to write and run C++ CPU and CUDA code using the pytorch `load_inline` compiler solution. I'll also show you how create compiled CPU and CUDA solutions using numba and compare and contrast the 2 options and explain why I like numba better.\n\nA special thanks to [Mark Saroufim](https://twitter.com/marksaroufim) and [Andreas Köpf](https://twitter.com/neurosp1ke) for creating the [CUDA MODE lecture series](https://www.youtube.com/@CUDAMODE) and [Jeremy Howard](https://twitter.com/jeremyphoward) for beautifully demonstrating 2 dead simple ways to get started with CUDA programming using PyTorch `load_inline` and Numba. \n\n*   PyTorch `load_inline` [lecture](https://youtu.be/4sgKnKbR-WE) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture_003/pmpp.ipynb)\n*   Numba [lecture](https://youtu.be/wVsR-YhaHlM?si=YiYq154tjebPdOZ5) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture_005/matmul_l5.ipynb)\n\nBefore we jump into the code I'll show you the benchmarking results to hopefully get you excited about the speedups you're going to learn how to implement.\n\nAs you can see our runtime went from hours to minutes to milliseconds as we added first CPU and then GPU acceleration. Keep on reading to see the implementations of each implementation.\n\n# Environment Setup\n\n## Colab Setup\n\nGetting started in Google Colab is fortunately very easy! Just load this notebook in Google Colab and select a GPU enabled runtime. A free Nvidia T4 instance will work just fine, though it's a bit slow. All of the base benchmarks in this notebook were obtained using a free T4 instance on Colab.<br>\n<br>\nAs of May 2024 Colab comes with all of the required dependencies such as CUDA, pytorch, numpy, pandas, numba, tqdm, ipywidgets, etc except for Ninja and wurlitzer which you can install by running the command below.\n\n## Local Setup (Not Recommended for Beginners)\n\nThese are the steps I followed to set up my environment for this project. They are not comprehensive and may not be optimal for your situation. You may use them as a reference but please modify them as necessary to fit your situation. If you get stuck, I suggest using Google Colab instead of trying to run this locally because it is easier to get started with.\n\n- Setup new conda environment: `conda create -n numba python=3.9`\n- Activate your new environment: `conda activate numba`\n- `pip install -U setuptools pip ninja`\n- `conda install -c conda-forge cupy cuda-version=12.1`\n- `conda install nvidia::cuda-python`\n- `conda install nvidia/label/cuda-12.1.0::cuda`\n- Install conda prereqs: `conda install numba`\n- Install pytorch: `conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n- Install pip pre-reqs: `pip install pandas tqdm ipywidgets wurlitzer`\n- (Optional) Setup the conda env as a jupyter kernel: `conda install ipykernel` and `ipython kernel install --user --name=numba`\n\n# Generate a Synthetic Dataset\n\nFirst we'll start off by creating a synthetic dataset of 200,000 (pipe) segments and 200,000 (weld) itersections which will generate 40,000,000,000 (40 Billion) checks to be done. This dataset is similar to the original dataset of 3D model object bounding boxes that we needed find ones that were overlapping.\n\nVerify bounding box dims: direction, length, x, y , z:\n\nThis should be: `(2, 3253, 150, 150, 3253)`\n\nNow, we'll preview the sets:\n\n### Shuffle the dataset (Optional)\n\nIf you want to make this 'harder' you can shuffle the dataset. A shuffled dataset would be more representative of the original dataset I had, but since we are checking every object from set 1 vs every object from set 2 it doesn't make a difference performance wise. Also by leaving the dataset sorted as-is, it's easier to inspect the results to see if it's working correctly.\n\nThe last row of shuffled df should be: `121958\t6097\t18\t0\t4895\t289765\t259562\t276875\t289925\t259722\t277035`\n\nThe last row of shuffled df should be: `68680\t3434\t0\t0\t6708\t399431\t242270\t218081\t406139\t242420\t218231`\n\n### Save the Dataset\n\n# Imports\n\nOne of the exciting aspects of writing CUDA kernels is observing the performance improvements. These improvements can vary significantly across different hardware configurations. Below, we extract the runtime hardware information to tag our benchmark results. This tagging is crucial as it provides context about the hardware used, helping you develop a mental model and build intuition about the potential speedups achievable with the various techniques demonstrated.\n\nCurrent hardware:\n\n# Loading the data\n\nTo start off we'll load the data from 2 files containing sets of bounding box coordinates, representing pipes and welds, that we want to check for overlaps between each set.\n\nIf you want to download rather than generate the dataset, you can do so by uncommenting the following code block and running it.\n\nFirst we'll load set1 which represents the welds or intersections between pipe segments.\n\nVerify the size of the weld (intersection) objects in set 1 are all uniform:\n\nNow we'll load set2 which represents the pipe or segments.\n\nNext, we'll create numpy arrays and torch tensors which we'll use later on.\n\n# A naive python approach using Pandas\n\nFirst, we begin with a naive Python pandas for loop. This approach will confirm our logic and serve as a benchmark to measure the speedup achieved with CUDA. Spoiler alert: the improvement is significant!<br>\n<br>\nDue to the inefficiency of pandas for loops, we'll work with a small subset of the data to ensure the code runs in a reasonable amount of time.\n\nTo start off we'll check the first 2 objects from set 1 against set 2.\n\nYou'll notice that I've included the results of running code blocks as static text within the notebook, in addition to the Jupyter cell output. This ensures you can always see the results I obtained, even if the cell output is cleared. Additionally, I've included results from running the code on different GPU instance types to illustrate the performance variations across different hardware.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 20.9 s, sys: 114 ms, total: 21 s\nWall time: 27.9 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 15.4 s, sys: 69.6 ms, total: 15.5 s\nWall time: 15.4 s\n----------------------------------------------------------------------------------------------------\n```\n\nChecking just 2 * 200,000 = 400,000 of the total required 40,000,000,000 (40 billion) checks, or just 0.001% of the data, took 28 seconds on `Hardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab`. Extrapolating this to the full dataset, it would take 777 hours or 32 days to complete. We can do better.\n\nThere is a trick in Pandas to iterate through the rows of a DataFrame more quickly by using the itertuples method instead of iterrows. Let's try that out. Since it's significantly faster, we'll iterate through the first 20 objects from set1 instead of just the first 2 as we did before.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5.29 s, sys: 18.4 ms, total: 5.3 s\nWall time: 5.4 s\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 4.35 s, sys: 17.7 ms, total: 4.37 s\nWall time: 4.35 s\n----------------------------------------------------------------------------------------------------\n```\n\nThis is a significant speedup. Our initial attempt would have taken 32 days to complete, while this method will take about 15 hours—manageable within a day, but still extremely slow. Waiting 15 hours to discover bugs in your code is far from ideal. We can improve this further. Next, let's try vectorizing the operation with NumPy.\n\n# Vectorize With Broadcasting\n\n## Numpy on CPU\n\nVectorizing with NumPy allows the CPU to perform these comparisons in parallel. Additionally, NumPy is implemented in C, which should provide a dramatic speedup.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5.98 s, sys: 3.38 s, total: 9.35 s\nWall time: 9.44 s\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 6.2 s, sys: 1.38 s, total: 7.58 s\nWall time: 7.54 s\n----------------------------------------------------------------------------------------------------\n```\n\nNow we're talking—a 5,932x speedup! We can now process the entire dataset in about 8 minutes, a significant improvement over the original estimate of 777 hours. For this particular dataset, we only need to perform the analysis once, so running in 8 minutes is sufficient. However, if each set were 10 times larger, the processing time would increase 100-fold, bringing us back to an impractical duration again. Let's see how much of a speedup we can achieve by moving these arrays to the GPU while using the same algorithm.\n\n## Torch on GPU\n\nWe'll start off by rewriting our `check_overlap` function in pytorch.\n\nWe'll run it on the first 4,000 items from set 1 against all items in set 2 just like we did with the numpy arrays on CPU to get an apple to apple comparison.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n304 ms ± 2.35 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n56.3 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nOk, this is another huge speedup. We're now at a 184,211x speedup over the orignal implementation and a 31x speedup over the comparable CPU implementation. Since it's so fast now, let's actually compute this against the entire dataset.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 14.5 s, sys: 50.4 ms, total: 14.5 s\nWall time: 16 s\n[(0, 0), (0, 35920), (1, 0), (1, 1), (2, 1)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3.17 s, sys: 29.4 ms, total: 3.2 s\nWall time: 3.19 s\n----------------------------------------------------------------------------------------------------\n```\n\nYou might be wondering why we're processing this in chunks of 4,000 rows instead of all at once. This approach is more memory-efficient and results in a faster runtime. Without chunking, you'd run out of memory.<br>\n<br>\nNext, we'll create a matrix using these results, formatted for future custom CUDA kernel examples. This matrix will have dimensions len(set1) x 6. Each row in this matrix corresponds to an index from set1, and we store up to six matches, with each integer value representing the index of a matching row from set2. We will use output_test_tt from now on to validate the correctness of the results produced by future kernels.\n\n# C++ Implementation With Torch load_inline\n\n## Setup and Boilerplate\n\nNow we'll write our own custom CUDA kernel and compile it using PyTorch's `load_inline` function. We'll be using the fantastic CUDA example from [Jeremy Howard's](https://twitter.com/jeremyphoward/) CUDA MODE [Lecture 3 presentation](https://youtu.be/4sgKnKbR-WE) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture3/pmpp.ipynb). For a more detailed introduction to CUDA with PyTorch, I recommend checking out these resources.<br>\n<br>\nFirst we'll enable `CUDA_LAUNCH_BLOCKING` to get better compilation error messages and load the `wurlitzer` plugin, which allows c++ compilation messages to be surfaced up to python and into the notebook.\n\n`load_cuda` is a helper function for compiling custom cuda kernel's inline using torch's [`load_inline`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline) function.\n\n`cuda_begin` contains the boilerplate imports and utility functions we'll use for compiling our custom CUDA kernels. We import the PyTorch `torch extension` for integrating torch in our C++ code, stdio for printing, and CUDAException for error handling and reporting. Additionally, we create a few helper functions to ensure that the tensors passed to our custom CUDA kernel are on the GPU and contiguous in memory. We also include a simple helper function for ceiling division to calculate the number of blocks to launch.\n\n## C++ CUDA Kernel on GPU\n\nThe main advantage of using GPUs over CPUs is their ability to perform a large amount of work in parallel and largely independently. While CPU cores are typically faster than GPU cores, a powerful modern CPU might have 64 cores or 128 threads, whereas a modern GPU has thousands of 'cores' and can execute thousands of threads in parallel. This parallelism allows GPUs to outperform CPUs significantly on embarrassingly parallel workloads. The speedup varies depending on the workload and the specific CPU and GPU used, but generally, you can expect a speedup ranging from 10x to 1000x. <br>\n<br>\nWhen designing your CUDA program, you divide your workload into blocks and threads. A block is a group of threads, and blocks can be defined as 1, 2, or 3 dimensional to map to your workload for easier comprehension. Utilizing additional block dimensions does not affect performance. We will use a single block dimension, with a maximum of 2^31 blocks for dimension 0 (x) and 2^16 for dimensions 1 (y) and 2 (z). Each block can have a maximum of 1024 threads. It is typically beneficial to use a multiple of 32 when assigning the number of threads per block, as this is the size of a 'warp' in modern Nvidia GPUs. While the explanation of what a warp is is beyond the scope of this notebook, know that using a thread count divisible by 32 is almost always faster than not using a multiple of 32. <br>\n<br>\nYou might be wondering why 'blocks' are important and why Nvidia introduced this concept in CUDA. Blocks are crucial for several reasons. All threads within a block are executed on the same streaming multiprocessor (SM) and have access to shared memory, which is approximately 10 times faster than global memory. For example, an RTX 3090 has 48KB of shared memory per block and 82 SMs, with a total of 10,496 CUDA cores. Similarly, a T4 has 48KB of shared memory per block and 40 SMs, with a total of 2,560 CUDA cores.<br>\n<br>\nWith that brief introduction, let's dive into the code. First, we need to define our custom CUDA kernel. The `__global__` keyword specifies that the function is a CUDA kernel to be executed on the GPU. CUDA kernels are executed in parallel on the GPU and do not return anything. Instead, you need to pass a variable to the kernel where the output can be stored. In our function, we need to pass pointers to set1 (`s1`), set2 (`s2`), and the output tensor (`out`), as well as the lengths of `s1` and `s2`, and the maximum number of `s2` indices that can be stored per `s1` item in out. Please take a moment to read through the thoroughly and verbosely commented CUDA kernel below to understand its functionality.\n\nAdd the `find_overlap` c++ function declaration to the cpp_sources in the torch `load_inline` function. The function logic is defined above in the `cuda_src` and is the c++ function run on the CPU that calls our custom CUDA kernel.\n\nCompile the CUDA kernel using our `load_cuda` utility function, which calls PyTorch's `load_inline` with some predefined defaults. We'll time the compilation step, which typically takes 1-2 minutes. When writing your own CUDA kernel, this wait time can significantly slow down your iteration speed. Later in this post, we'll demonstrate other methods that compile much faster and allow for quicker iterations, which is highly recommended. It took me many iterations and a lot of waiting for compilation to get this code right.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.01 s, sys: 105 ms, total: 1.11 s\nWall time: 1min 33s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/inline_ext...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext.so\nLoading extension module inline_ext...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 680 ms, sys: 109 ms, total: 788 ms\nWall time: 1min 15s\n----------------------------------------------------------------------------------------------------\n```\n\nNow we'll inspect the newly compiled module. We can see that it has our newly defined `find_overlap` function, which is now available to call from Python. While inline_compile is relatively slow, it simplifies writing and running C++ CUDA code from a Python Jupyter notebook, which is quite convenient!\n\n```\n['__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'find_overlap']\n```\n\nWe'll quickly inspect the type, shape, and layout of the `set1` and `set2` tensors that we'll use to test our new CUDA kernel. It's always a good idea to examine your inputs and outputs when writing code, and Jupyter notebooks make this process easy.\n\n\n```\n(torch.float32,\n torch.float32,\n torch.Size([200000, 6]),\n torch.Size([200000, 6]),\n torch.strided,\n torch.strided)\n ```\n\nFinally, we'll time our new CUDA kernel. It takes 369 ms to run which is a 7,600,000 (7.6 Millon) times speedup over our original implementation and a 41 x improvement over our previous broadcasted pure pytorch CUDA implementation.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n369 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n85.3 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nFinally, we'll calculate the results one last time and store them in the `out` variable. This will allow us to compare against our previously calculated results to ensure they match and confirm that we have not introduced any logic errors.<br>\n<br>\nChecking the output is a crucial step. While implementing some kernels for this post, I observed large speedups that initially seemed promising. However, I later realized that these 'performance gains' were due to omitting a significant portion of the computation. This underscores the importance of thorough validation.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\nFinally, we'll check to make sure the results match, which they do.\n\n```\ntensor(True, device='cuda:0')\n```\n\n## C++ Single Threaded For Loop on CPU\n\nAfter implementing a working custom CUDA kernel, I was curious to see how fast a naive single-threaded (nested for loop) CPU-only C++ implementation would be. My aim with this implementation is purely for benchmarking purposes, to help build an intuition on the types of speedups you might expect with different implementations.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.05 s, sys: 110 ms, total: 1.16 s\nWall time: 1min 36s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 1 and re-building as inline_ext_v1...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v1...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v1.so\nLoading extension module inline_ext_v1...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 702 ms, sys: 108 ms, total: 810 ms\nWall time: 1min 13s\n----------------------------------------------------------------------------------------------------\n```\n\nFor our CPU implementation, we need to create tensors on the CPU instead of using the GPU tensors we used previously for our CUDA kernel.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 39s, sys: 1.17 s, total: 3min 40s\nWall time: 3min 47s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 30s, sys: 1.51 s, total: 3min 32s\nWall time: 3min 28s\n----------------------------------------------------------------------------------------------------\n```\n\nAnd finally test that the output is correct.\n\n```\ntensor(True)\n```\n\n## C++ OMP Parallelization on CPU\n\nNext, we'll try parallelizing on the CPU using C++ OMP (OpenMP) parallelization to see how fast we can get on the CPU. The runtimes will vary **SIGNIFICANTLY** depending on the number of cores available and their speed.<br>\n<br>\nTo add OMP parallelization, we need to include a few additional compiler flags in our `load_cuda` function, which is why we're rewriting it here.\n\n\nChoose the number of threads that makes sense for your CPU. I have added a quick function below to determine the number of threads your CPU supports if you don't already know.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 598 ms, sys: 54.3 ms, total: 652 ms\nWall time: 53 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 2 and re-building as inline_ext_v2...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v2...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v2.so\nLoading extension module inline_ext_v2...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on different CPU's with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5min 41s, sys: 1.35 s, total: 5min 42s\nWall time: 3min 32s\n3min 32s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n8.67 s - 18 threads\n6.91 s - 36 threads\n9.28 s - # threads not set\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 37s, sys: 927 ms, total: 3min 38s\nWall time: 1min 48s\n1min 48s - 2 threads\nTODO: Try 12 threads\n```\n\n```\ntensor(True)\n```\n\nA *trick* I discovered in later experiments when writing CUDA kernels was to load the values from the set pointers into dedicated variables before performing the comparisons. This yielded faster runtimes. I applied the same technique to the CPU OMP implementation, and it improved performance here as well by loading the `s1` values into variables first.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 594 ms, sys: 62.5 ms, total: 656 ms\nWall time: 54 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 3 and re-building as inline_ext_v3...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v3...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v3.so\nLoading extension module inline_ext_v3...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 292 ms, sys: 56.7 ms, total: 349 ms\nWall time: 32.2 s\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on different CPU's with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 27s, sys: 1.11 s, total: 3min 28s\nWall time: 2min 15s\n2min 15s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n6.45 s - 18 threads\n4.58 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 2min 26s, sys: 504 ms, total: 2min 27s\nWall time: 1min 12s\n1min 12s - 2 threads\n----------------------------------------------------------------------------------------------------\n```\n\n```\ntensor(True)\n```\n\nAs you can see, setting the `set1` pointer values to variables before performing the comparisons leads to a significant performance improvement!<br>\n<br>\nNow, let's try the same with the `set2` values that are being compared against. This is unlikely to improve performance, as it doesn't decrease the number of lookups like it does with `set1` in the outer loop.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 601 ms, sys: 59.6 ms, total: 661 ms\nWall time: 52.8 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 4 and re-building as inline_ext_v4...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v4...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v4.so\nLoading extension module inline_ext_v4...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 309 ms, sys: 47.6 ms, total: 357 ms\nWall time: 31.5 s\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on various CPUs with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 9min 30s, sys: 2.43 s, total: 9min 33s\nWall time: 5min 31s\n5min 31s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n10.7 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 5min 34s, sys: 1.4 s, total: 5min 35s\nWall time: 2min 46s\n2min 46s - 2 threads\nTODO Try 12 threads\n----------------------------------------------------------------------------------------------------\n```\nAs you can see caching the set2 values actually made the performance worse.\n\n```\ntensor(True)\n```\n\n# Numba\n\nWaiting for these kernels to compile significantly slows down iteration speed, which is very important! Numba allows you to compile Python inline as well, but it is approximately 100 times faster than `torch`'s compilation via `load_inline`. Numba also allows you to run on both the CPU and GPU. I recommend starting with Numba instead of `torch`'s `load_inline` because it is easier to use and enables much faster iteration speed. Once your kernel is working and tuned, you can easily convert it to a C++ CUDA kernel if needed.\n\nWe'll start by importing the required Numba packages.\n\n\n`njit` allows you to compile CPU functions that don't rely on the Python interpreter, making it much faster than `jit`, which allows falling back to the Python interpreter. You almost always want to use `njit` if possible because it is much faster. `cuda.jit` is similar to `njit` but compiles your code into a CUDA kernel and runs it on the GPU.\n\n\n## CPU\n\nFirst, we'll start with a CPU implementation in Numba to establish a baseline. We'll use `float32` numbers initially and explore whether using different numeric types can improve performance for this problem.\n\n### float32\n\nConvert the previously loaded `int32` Numpy arrays to `float32`.\n\nVerify the shape and types of the new arrays.\n\nNow we'll create our CPU implementation. Notice that we are defining the types of the input and output parameters for the function. By specifying these types, Numba can compile this function to be much faster than a standard Python function.<br>\n<br>\nTake note of the three different comparison implementations below. I've commented out the first two iterations so you can see what I started with. These initial implementations were much slower than the final version. Implementation details can significantly impact your function's performance, so it's worth experimenting with different options.\n\n\nBenchmark Results:\n```\nImplementations:\n\nIteration 1\nif (s1l[:3] <= s2[j, 3:]).all() and (s1l[3:] >= s2[j, :3]).all():\n\nIteration 2\nif s1l0 <= s2l[3] and s1l1 <= s2l[4] and s1l2 <= s2l[5] and s1l3 >= s2l[0] and s1l4 >= s2l[1] and s1l5 >= s2l[2]:\n\nIteration 3\ns2l0 = s2l[0]\ns2l1 = s2l[1]\ns2l2 = s2l[2]\ns2l3 = s2l[3]\ns2l4 = s2l[4]\ns2l5 = s2l[5]\n\nif s1l0 <= s2l3 and s1l1 <= s2l4 and s1l2 <= s2l5 and s1l3 >= s2l0 and s1l4 >= s2l1 and s1l5 >= s2l2:\n\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 42s, sys: 193 ms, total: 1min 42s\nWall time: 1min 10s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n\nIteration 1\nCPU times: user 1h 4min 37s, sys: 798 ms, total: 1h 4min 38s<br>\nWall time: 2min 16s\n\nIteration 2\nCPU times: user 7min 29s, sys: 95.5 ms, total: 7min 29s<br>\nWall time: 14.4 s\n\nIteration 3\nCPU times: user 52.2 s, sys: 12.2 ms, total: 52.2 s<br>\nWall time: 1.99 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 55s, sys: 53.8 ms, total: 1min 55s\nWall time: 10.1 s\n----------------------------------------------------------------------------------------------------\n```\n\nAs you can see, the three successive implementation iterations yielded a compounded speedup of 9.4x and 7x. I believe the speedup between implementations 1 and 2 is because the NumPy library does not have to be called, but I'm unsure why there was such a significant speedup between implementations 2 and 3. I suspect it has to do with caching, but I'm not certain.\n\nAs always, we check out output to make sure it's correct.\n\n```\ntensor(True)\n```\n\n### int32\n\nNext we'll try out using the same exact logic only swapping `float32` for `int32` to see if it makes a difference.<br>\n<br>\nFirst we'll create our `int32` numpy arrays.\n\nAnd compile a new function that has the exact same logic as before, but the new type for the inputs as we discussed.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1min 47s, sys: 265 ms, total: 1min 48s\nWall time: 1min 1s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\nCPU times: user 49.1 s, sys: 16.1 ms, total: 49.1 s<br>\nWall time: 2.02 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 1min 51s, sys: 24.1 ms, total: 1min 51s\nWall time: 9.51 s\n```\nSwapping input types did not make any difference in the runtime.\n\n```\ntensor(True)\n```\n\n## GPU\n\n### Plotting\n\nWe need to start performing sweeps of hyperparameters, such as the number of threads per block, to determine the optimal values. Plotting the results will help us visualize which settings work best and how much impact changing these settings has. We'll create a function to plot the number of threads per block on the x-axis and runtime on the y-axis. Due to the significant variance in runtimes, we've added a parameter to limit the y-axis range to better visualize the best results. Additionally, we may want to visualize other variables besides threads per block, so if we specify a series key, it will create a separate plot color for each value, such as the number of items per thread.\n\n### Basic CUDA Kernel\n\nNow let's implement our first CUDA kernel with Numba. When defining the Numba CUDA kernel, use the `@cuda.jit` decorator. When calling this function, you need to pass the required CUDA kernel arguments in square brackets before passing the function arguments in parentheses: `kernel_func[grid_dim, block_dim, stream, dyn_shared_mem_size](standard_function_arguments)`. `grid_dim` and `block_dim` are always required, but additional CUDA arguments, such as shared memory size, can also be passed, which we'll cover shortly. For further information, please refer to the [Numba documentation](https://numba.readthedocs.io/en/stable/cuda/memory.html#dynamic-shared-memory).<br>\n<br>\nThis function should look familiar compared to the C++ CUDA kernel implementations from before. You'll notice that you can utilize some Pythonic implementation styles, which helps make the code more compact and readable.\n\nNext, we'll define our Python wrapper function to handle tasks similar to those managed by the C++ CPU functions earlier, such as determining the number of blocks.\n\nNext we'll create a simple helper function to create an initialized output array. We'll typically keep this initialization outside of the benchmark runs to make it easier to compare just the kernel runtimes.\n\nNow let's run our new numba kernel and see how well it performs.\n\nIsn't it nice that we don't have to wait long for it to compile? This allows for much faster iteration speed. Next, I'll implement some more advanced techniques, which would have been much more challenging to implement using `torch.load_inline` due to the slow iteration speed. It took me quite a few tries to get the more advanced implementations right.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n400 ms ± 4.68 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\nTODO XXXX\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n143 ms ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nNow we'll take a look at the output and check that it's correct.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\n```\ntensor(True, device='cuda:0')\n```\n\nLet's try another experiment with 1,024 threads per block.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n401 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 83.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\nIt looks like it doesn't make any difference.\n\nNow let's do a sweep to confirm what the optimal number of threads per block is.\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_0.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_0_zoom.png)\n\n### CUDA Kernel Using Shared Memory v1\n\nNext, we'll utilize shared memory to try and speed up our CUDA kernel. Shared memory is ultra-fast memory shared between all threads in a block, and it is approximately 10 times faster than global GPU memory. The Nvidia T4 and RTX 3090 have 48KB of shared memory available per thread block, compared to 16GB and 24GB, respectively, of global memory VRAM in each card.\n\nIn our CUDA kernel implementation, each thread computes all comparisons for one element from `set1` against all elements of `set2` by looping over all elements in `set2`. This means that in our original CUDA kernel implementation, each thread has to load all elements of `set2` from global memory. To speed this up, we can break `set2` into chunks so that each chunk fits into shared memory and each element in the chunk can be fetched from global memory only once per block by a single thread.\n\nWe'll add another loop to our kernel to loop through each `set2` chunk. In the outer loop, each thread will load a single value of `set2` into shared memory, and then in the inner loop, we'll compare the element from `set1` with each element from `set2` in shared memory. If our kernel is memory-bound, this should speed up our overall runtime.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n345 ms ± 15.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n129 ms ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n399 ms ± 3.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n114 ms ± 38.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n338 ms ± 5.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 120 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% XXXXXXXXXXXXXXXXXXXXXX 64/64 [04:09<00:00,  3.55s/it]\n```\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_1.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_1_zoom.png)\n\nAs you can see using shared memory gave us a nice 20% speed improvement over our previous numba kernel that did not utilize shared memory.\n\n### CUDA Kernel Using Shared Memory v2\n\nNext we'll try a slight variation where we test if loading multiple set2 elements into shared memory per thread is faster (`cache_items_per_thread`) than only loading a single element per thread as in our v1 example.\n\nFirst we'll test it out to make sure it works and produces the correct result.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n417 ms ± 17.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n152 ms ± 16.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [38:27<00:00,  7.17s/it]\n100% 32/32 [07:05<00:00, 23.28s/it]\n64 1 361 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n64 2 353 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 25/25 [04:32<00:00, 18.24s/it]\n100% 21/21 [03:18<00:00, 15.38s/it]\n100% 18/18 [02:31<00:00, 13.07s/it]\n100% 16/16 [02:04<00:00, 11.74s/it]\n128 1 343 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 14/14 [01:41<00:00, 10.31s/it]\n100% 12/12 [01:17<00:00,  9.04s/it]\n100% 11/11 [01:09<00:00,  8.48s/it]\n100% 10/10 [00:57<00:00,  7.53s/it]\n100% 9/9 [00:50<00:00,  7.15s/it]\n100% 9/9 [00:47<00:00,  6.76s/it]\n100% 8/8 [00:42<00:00,  6.49s/it]\n100% 8/8 [00:40<00:00,  6.18s/it]\n256 1 339 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 7/7 [00:34<00:00,  5.68s/it]\n100% 7/7 [00:35<00:00,  5.81s/it]\n100% 6/6 [00:28<00:00,  5.36s/it]\n100% 6/6 [00:27<00:00,  5.11s/it]\n100% 6/6 [00:27<00:00,  5.07s/it]\n100% 5/5 [00:23<00:00,  5.07s/it]\n100% 5/5 [00:22<00:00,  4.87s/it]\n100% 5/5 [00:21<00:00,  4.76s/it]\n100% 5/5 [00:21<00:00,  4.71s/it]\n100% 4/4 [00:16<00:00,  4.24s/it]\n100% 4/4 [00:16<00:00,  4.23s/it]\n100% 4/4 [00:15<00:00,  4.15s/it]\n100% 4/4 [00:16<00:00,  4.37s/it]\n100% 4/4 [00:16<00:00,  4.34s/it]\n100% 4/4 [00:16<00:00,  4.37s/it]\n100% 4/4 [00:15<00:00,  4.10s/it]\n512 1 335 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 3/3 [00:13<00:00,  4.50s/it]\n100% 3/3 [00:13<00:00,  4.47s/it]\n100% 3/3 [00:12<00:00,  4.24s/it]\n100% 3/3 [00:12<00:00,  4.23s/it]\n100% 3/3 [00:12<00:00,  4.25s/it]\n100% 3/3 [00:12<00:00,  4.24s/it]\n100% 3/3 [00:12<00:00,  4.12s/it]\n100% 3/3 [00:11<00:00,  3.94s/it]\n100% 3/3 [00:12<00:00,  4.06s/it]\n100% 3/3 [00:12<00:00,  4.04s/it]\n100% 2/2 [00:08<00:00,  4.08s/it]\n100% 2/2 [00:08<00:00,  4.05s/it]\n100% 2/2 [00:07<00:00,  3.76s/it]\n100% 2/2 [00:07<00:00,  3.75s/it]\n100% 2/2 [00:07<00:00,  3.80s/it]\n100% 2/2 [00:07<00:00,  3.82s/it]\n100% 2/2 [00:07<00:00,  3.90s/it]\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nbest result 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n```\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_2.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_2_zoom.png)\n\nAs you can see there are a few results that are very close but the best one was where there was 1 cache item per thread which is effectively the same as our v1 algorithm.\n\n### CUDA Kernel Using Shared Memory v3 **fp16**\n\nNow we'll try the same algorithm but with `fp16` instead of `fp32`, which should cut our memory usage and bandwidth in half. Given the lower precision, we need to carefully check that the results are the same, as we're losing a significant portion (half) of our floating-point resolution.\n\nWhen declaring the size of the shared memory in this implementation, I initially forgot that it is specified in bytes when the kernel is called. However, when accessed from within the kernel, the bytes per object are taken into account. I was declaring the shared memory size without considering this, which led to continual invalid memory access errors that took quite a while to debug.\n\nNow we need to create fp16 torch tensors from our fp32 versions that we were using previously.\n\nNow we'll do a quick sanity check to ensure the results are correct.\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nOutput is correct? tensor(False, device='cuda:0')\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [ 41295,  41296, 199998, 199999,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\nAs you can see the output is not correct. The question is did we make a mistake in our implementation or is the loss of precision when switching to float16 causing the problem.<br>\n<br>\nLet's compare the results with our test array `output_test_tt` and see if we initially spot any differences.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]])\n```\nAs you can see the in last index of set1 the results are different. There are additional matches `41295,  41296`. Let's look at those values.\n\nAs you can see they are different between the float16 and float32 versions:<br>\n```\n(tensor([348.5000, 306.7500, 239.3750, 348.5000, 307.0000, 239.5000],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.2500, 298.7500, 239.2500, 348.5000, 307.0000, 239.3750],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.4000, 306.8360, 239.3870, 348.5600, 306.9960, 239.5470],\n        device='cuda:0'),\n tensor([348.2370, 298.6260, 239.2440, 348.3870, 307.0380, 239.3940],\n        device='cuda:0'))\n```\nNow let's use the check overlap function we built for the PyTorch GPU vectorize implementation, passing in values from both the `float16` and `float32` inputs to see if it produces different results. The `check_overlap_torch` function takes two 2-dimensional arrays and returns a list of tuples containing the indices of a match. Since we only want to compare the numbers in question, we need to expand the first dimension of our test elements, which we can do by indexing into them with `None`. This converts the 1D vector into a 1x6 array, which is required for this function.\n\n```\n[(0, 0)]\n```\n\n```\n[]\n```\n\nAs you can see, there is a match when using the `float16` values but not with the `float32` values. This indicates that `float16` is not suitable for our implementation and this use case. Half precision is very common in deep learning workloads and works fine in most cases, so it's something we should check. Although it is not suitable for our use case because it returns incorrect results, I am curious how it performs compared to the `float32` implementations, so we'll run it anyway to see how it works. This will allow you to compare from a benchmarking perspective, but it's a good lesson that it might not work in all cases and it's important to validate our outputs as we go.<br>\n<br>\nThis will take longer to run, as there are significantly more possible `cache_items_per_thread` to check because each element only takes up half the amount of shared memory, allowing us to effectively have twice as many elements in shared memory.\n\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [39:45<00:00,  7.12s/it]\n100% 64/64 [07:12<00:00, 12.09s/it]\n64 1 377 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 2 355 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 3 354 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 51/51 [04:44<00:00,  9.63s/it]\n100% 42/42 [03:20<00:00,  8.20s/it]\n96 1 349 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n96 3 347 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 36/36 [02:33<00:00,  7.05s/it]\n100% 32/32 [02:03<00:00,  6.32s/it]\n128 1 336 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n128 3 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 28/28 [01:42<00:00,  5.65s/it]\n100% 25/25 [01:20<00:00,  5.05s/it]\n100% 23/23 [01:12<00:00,  4.70s/it]\n100% 21/21 [01:01<00:00,  4.32s/it]\n100% 19/19 [00:53<00:00,  4.01s/it]\n100% 18/18 [00:48<00:00,  3.79s/it]\n100% 17/17 [00:44<00:00,  3.61s/it]\n100% 16/16 [00:40<00:00,  3.43s/it]\n100% 15/15 [00:37<00:00,  3.28s/it]\n100% 14/14 [00:33<00:00,  3.12s/it]\n100% 13/13 [00:31<00:00,  3.03s/it]\n100% 12/12 [00:26<00:00,  2.80s/it]\n100% 12/12 [00:27<00:00,  2.74s/it]\n100% 11/11 [00:25<00:00,  2.70s/it]\n100% 11/11 [00:24<00:00,  2.61s/it]\n100% 10/10 [00:21<00:00,  2.45s/it]\n100% 10/10 [00:22<00:00,  2.51s/it]\n100% 9/9 [00:18<00:00,  2.35s/it]\n100% 9/9 [00:18<00:00,  2.33s/it]\n100% 9/9 [00:18<00:00,  2.31s/it]\n100% 8/8 [00:16<00:00,  2.22s/it]\n100% 8/8 [00:16<00:00,  2.21s/it]\n100% 8/8 [00:16<00:00,  2.21s/it]\n100% 8/8 [00:15<00:00,  2.08s/it]\n512 1 331 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 7/7 [00:15<00:00,  2.28s/it]\n100% 7/7 [00:15<00:00,  2.27s/it]\n100% 7/7 [00:14<00:00,  2.14s/it]\n100% 7/7 [00:15<00:00,  2.14s/it]\n100% 6/6 [00:12<00:00,  2.14s/it]\n100% 6/6 [00:12<00:00,  2.13s/it]\n100% 6/6 [00:12<00:00,  2.08s/it]\n100% 6/6 [00:11<00:00,  1.99s/it]\n100% 6/6 [00:12<00:00,  2.04s/it]\n100% 6/6 [00:12<00:00,  2.03s/it]\n100% 5/5 [00:10<00:00,  2.04s/it]\n100% 5/5 [00:10<00:00,  2.03s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.87s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.95s/it]\n100% 5/5 [00:09<00:00,  1.93s/it]\n100% 5/5 [00:09<00:00,  1.97s/it]\n100% 4/4 [00:07<00:00,  1.88s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.78s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.85s/it]\n100% 4/4 [00:07<00:00,  1.85s/it]\n100% 4/4 [00:07<00:00,  1.87s/it]\n100% 4/4 [00:07<00:00,  1.87s/it]\n100% 4/4 [00:07<00:00,  1.90s/it]\n100% 4/4 [00:07<00:00,  1.89s/it]\n100% 4/4 [00:06<00:00,  1.68s/it]\n100% 4/4 [00:06<00:00,  1.68s/it]\n```\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_fp16_3.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_fp16_3_zoom.png)\n\nSurpringly this kernel is not faster than the float32 kernel. I would have expected this to be a memory bandwidth bound kernel but it does not seem to be.\n\n# Conclusion\n\nIn this notebook, we learned how to create CUDA kernels using several different methods, including PyTorch's `load_inline` function to compile a C++ CUDA kernel and Numba's `cuda.jit`. We also created several accelerated CPU implementations. We recorded the execution time for each option using a free Google Colab T4 instance and plotted the results. We achieved an 8.4 million times speedup over our initial pandas implementation. However, even more interesting was the speedup we achieved over the broadcast vectorized examples, which are likely among the fastest common Python implementations. These vectorized operations call highly optimized code, yet we managed to surpass them with our custom kernels.<br>\n<br>\nThe key takeaway from these experiments is that if your use case doesn't quite fit within existing optimized libraries, you can achieve significant speedups with custom CUDA kernels. Additionally, writing basic CUDA kernels is easier than you might have initially thought.\n\n","srcMarkdownNoYaml":"\n\n# Introduction\n\n[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/matdmiller/blog/blob/main/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/notebook.ipynb)\n\nIn this post we're going to create and optimize GPU CUDA kernels to accelerate an embarrasingly parallel workload and achieve a **8.4 million X** speedup over baseline. Everything you need to create and run CUDA kernels is contained within this blog post which is actually a Jupyter notebook that you can run interactively. This is a fully self contained example workload that can be run for free with no setup required on Google Colab. All you need to do is click the **Open in Google Colab** button at the top of this post.\n\nYou should run this notebook as you go in Google Colab or any other properly configured CUDA environment. No extra setup is required if you use Google Colab, but if you run in another environment you are responsible for setting it up which is beyond the scope of this post. Learning by doing is more effective than reading alone. Hacking around with the code and seeing how that affects the outcomes helps cement the concepts being taught and builds intuition.\n\nYou will see several methods for computing the workload covered in this post. We will benchmark each method so you observe how quickly they run and begin to build an intuition on much you might be able to accelerate your own workloads. As a rule of thumb you can typically excpect a 10-1,000x speedup when accelerating workloads on GPU's. Each step will be explained and you will learn tips and tricks throughout the post.\n\nThe problem we will be solving is based on a real problem I ran across recently. We had a 3D model with hundreds of thousands of objects. The objects were divided into sets. We needed to figure out which objects from set1 overlapped with objects in set2. Objects in set1 contained metadata which needed to be transferred to all overlapping objects from set2. Each set contained approximately 200,000 objects. Every object from set1 needed to be checked for overlap against every object in set2 which equated to 200,000 * 200,000 = 40 Billon checks that needed to be completed. We had the min and max x,y,z bounding box coordinates for every object which is what we used to check for overlap. When the problem was brought to me an initial attempt was made to solve it by looping through each set of objects in pandas and checking for overlapping objects. The problem was this approach was going to take almost a month to run. This was a perfect embarassingly parallel workload and happened to perfectly coincide with the [CUDA MODE](https://github.com/cuda-mode/lectures) class I was taking. It was the perfect opportunity to implement my newly learned CUDA skills on real world problem. Code is included in this notebook to create a synthetic dataset that matches this problem.\n\nWe will follow the same path I took when trying out different methods to solve this problem. We'll start out with the basic pandas for loop code that was brought to me initially and I'll show you a simple trick to speed up pandas for loops. Next we'll look at a solution using numpy and broadcasting. This is one of the simplest ways to parallelize workloads on CPU's. This reduced the runtime from 777 hours with our initial pandas for loop to 8 minutes. From a practical perspective this was an acceptable amount of time to wait for the answer, but I was curious how much faster I could make this a custom CUDA kernel. The results were surprising to say the least. The next easy step was to take the numpy broadcasting solution and conver it to pytorch and run it on the GPU. This brought the runtime down to 15 seconds, a 32x improvement. At this point I was skeptical how much better a custom CUDA kernel would really be able to speed this up. After all pytorch is an incredibly optimized library for performing AI training, would I really be able to beat it. It turns out the answer was yes! Not that my CUDA code is better than pytorch's, it's definitely not, however my code that was written for this specific problem ended up being a lot faster than the broadcasting solution in pytorch. I'll show you how to write and run C++ CPU and CUDA code using the pytorch `load_inline` compiler solution. I'll also show you how create compiled CPU and CUDA solutions using numba and compare and contrast the 2 options and explain why I like numba better.\n\nA special thanks to [Mark Saroufim](https://twitter.com/marksaroufim) and [Andreas Köpf](https://twitter.com/neurosp1ke) for creating the [CUDA MODE lecture series](https://www.youtube.com/@CUDAMODE) and [Jeremy Howard](https://twitter.com/jeremyphoward) for beautifully demonstrating 2 dead simple ways to get started with CUDA programming using PyTorch `load_inline` and Numba. \n\n*   PyTorch `load_inline` [lecture](https://youtu.be/4sgKnKbR-WE) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture_003/pmpp.ipynb)\n*   Numba [lecture](https://youtu.be/wVsR-YhaHlM?si=YiYq154tjebPdOZ5) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture_005/matmul_l5.ipynb)\n\nBefore we jump into the code I'll show you the benchmarking results to hopefully get you excited about the speedups you're going to learn how to implement.\n\nAs you can see our runtime went from hours to minutes to milliseconds as we added first CPU and then GPU acceleration. Keep on reading to see the implementations of each implementation.\n\n# Environment Setup\n\n## Colab Setup\n\nGetting started in Google Colab is fortunately very easy! Just load this notebook in Google Colab and select a GPU enabled runtime. A free Nvidia T4 instance will work just fine, though it's a bit slow. All of the base benchmarks in this notebook were obtained using a free T4 instance on Colab.<br>\n<br>\nAs of May 2024 Colab comes with all of the required dependencies such as CUDA, pytorch, numpy, pandas, numba, tqdm, ipywidgets, etc except for Ninja and wurlitzer which you can install by running the command below.\n\n## Local Setup (Not Recommended for Beginners)\n\nThese are the steps I followed to set up my environment for this project. They are not comprehensive and may not be optimal for your situation. You may use them as a reference but please modify them as necessary to fit your situation. If you get stuck, I suggest using Google Colab instead of trying to run this locally because it is easier to get started with.\n\n- Setup new conda environment: `conda create -n numba python=3.9`\n- Activate your new environment: `conda activate numba`\n- `pip install -U setuptools pip ninja`\n- `conda install -c conda-forge cupy cuda-version=12.1`\n- `conda install nvidia::cuda-python`\n- `conda install nvidia/label/cuda-12.1.0::cuda`\n- Install conda prereqs: `conda install numba`\n- Install pytorch: `conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n- Install pip pre-reqs: `pip install pandas tqdm ipywidgets wurlitzer`\n- (Optional) Setup the conda env as a jupyter kernel: `conda install ipykernel` and `ipython kernel install --user --name=numba`\n\n# Generate a Synthetic Dataset\n\nFirst we'll start off by creating a synthetic dataset of 200,000 (pipe) segments and 200,000 (weld) itersections which will generate 40,000,000,000 (40 Billion) checks to be done. This dataset is similar to the original dataset of 3D model object bounding boxes that we needed find ones that were overlapping.\n\nVerify bounding box dims: direction, length, x, y , z:\n\nThis should be: `(2, 3253, 150, 150, 3253)`\n\nNow, we'll preview the sets:\n\n### Shuffle the dataset (Optional)\n\nIf you want to make this 'harder' you can shuffle the dataset. A shuffled dataset would be more representative of the original dataset I had, but since we are checking every object from set 1 vs every object from set 2 it doesn't make a difference performance wise. Also by leaving the dataset sorted as-is, it's easier to inspect the results to see if it's working correctly.\n\nThe last row of shuffled df should be: `121958\t6097\t18\t0\t4895\t289765\t259562\t276875\t289925\t259722\t277035`\n\nThe last row of shuffled df should be: `68680\t3434\t0\t0\t6708\t399431\t242270\t218081\t406139\t242420\t218231`\n\n### Save the Dataset\n\n# Imports\n\nOne of the exciting aspects of writing CUDA kernels is observing the performance improvements. These improvements can vary significantly across different hardware configurations. Below, we extract the runtime hardware information to tag our benchmark results. This tagging is crucial as it provides context about the hardware used, helping you develop a mental model and build intuition about the potential speedups achievable with the various techniques demonstrated.\n\nCurrent hardware:\n\n# Loading the data\n\nTo start off we'll load the data from 2 files containing sets of bounding box coordinates, representing pipes and welds, that we want to check for overlaps between each set.\n\nIf you want to download rather than generate the dataset, you can do so by uncommenting the following code block and running it.\n\nFirst we'll load set1 which represents the welds or intersections between pipe segments.\n\nVerify the size of the weld (intersection) objects in set 1 are all uniform:\n\nNow we'll load set2 which represents the pipe or segments.\n\nNext, we'll create numpy arrays and torch tensors which we'll use later on.\n\n# A naive python approach using Pandas\n\nFirst, we begin with a naive Python pandas for loop. This approach will confirm our logic and serve as a benchmark to measure the speedup achieved with CUDA. Spoiler alert: the improvement is significant!<br>\n<br>\nDue to the inefficiency of pandas for loops, we'll work with a small subset of the data to ensure the code runs in a reasonable amount of time.\n\nTo start off we'll check the first 2 objects from set 1 against set 2.\n\nYou'll notice that I've included the results of running code blocks as static text within the notebook, in addition to the Jupyter cell output. This ensures you can always see the results I obtained, even if the cell output is cleared. Additionally, I've included results from running the code on different GPU instance types to illustrate the performance variations across different hardware.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 20.9 s, sys: 114 ms, total: 21 s\nWall time: 27.9 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 15.4 s, sys: 69.6 ms, total: 15.5 s\nWall time: 15.4 s\n----------------------------------------------------------------------------------------------------\n```\n\nChecking just 2 * 200,000 = 400,000 of the total required 40,000,000,000 (40 billion) checks, or just 0.001% of the data, took 28 seconds on `Hardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab`. Extrapolating this to the full dataset, it would take 777 hours or 32 days to complete. We can do better.\n\nThere is a trick in Pandas to iterate through the rows of a DataFrame more quickly by using the itertuples method instead of iterrows. Let's try that out. Since it's significantly faster, we'll iterate through the first 20 objects from set1 instead of just the first 2 as we did before.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5.29 s, sys: 18.4 ms, total: 5.3 s\nWall time: 5.4 s\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 4.35 s, sys: 17.7 ms, total: 4.37 s\nWall time: 4.35 s\n----------------------------------------------------------------------------------------------------\n```\n\nThis is a significant speedup. Our initial attempt would have taken 32 days to complete, while this method will take about 15 hours—manageable within a day, but still extremely slow. Waiting 15 hours to discover bugs in your code is far from ideal. We can improve this further. Next, let's try vectorizing the operation with NumPy.\n\n# Vectorize With Broadcasting\n\n## Numpy on CPU\n\nVectorizing with NumPy allows the CPU to perform these comparisons in parallel. Additionally, NumPy is implemented in C, which should provide a dramatic speedup.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5.98 s, sys: 3.38 s, total: 9.35 s\nWall time: 9.44 s\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 6.2 s, sys: 1.38 s, total: 7.58 s\nWall time: 7.54 s\n----------------------------------------------------------------------------------------------------\n```\n\nNow we're talking—a 5,932x speedup! We can now process the entire dataset in about 8 minutes, a significant improvement over the original estimate of 777 hours. For this particular dataset, we only need to perform the analysis once, so running in 8 minutes is sufficient. However, if each set were 10 times larger, the processing time would increase 100-fold, bringing us back to an impractical duration again. Let's see how much of a speedup we can achieve by moving these arrays to the GPU while using the same algorithm.\n\n## Torch on GPU\n\nWe'll start off by rewriting our `check_overlap` function in pytorch.\n\nWe'll run it on the first 4,000 items from set 1 against all items in set 2 just like we did with the numpy arrays on CPU to get an apple to apple comparison.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n304 ms ± 2.35 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n56.3 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nOk, this is another huge speedup. We're now at a 184,211x speedup over the orignal implementation and a 31x speedup over the comparable CPU implementation. Since it's so fast now, let's actually compute this against the entire dataset.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 14.5 s, sys: 50.4 ms, total: 14.5 s\nWall time: 16 s\n[(0, 0), (0, 35920), (1, 0), (1, 1), (2, 1)]\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3.17 s, sys: 29.4 ms, total: 3.2 s\nWall time: 3.19 s\n----------------------------------------------------------------------------------------------------\n```\n\nYou might be wondering why we're processing this in chunks of 4,000 rows instead of all at once. This approach is more memory-efficient and results in a faster runtime. Without chunking, you'd run out of memory.<br>\n<br>\nNext, we'll create a matrix using these results, formatted for future custom CUDA kernel examples. This matrix will have dimensions len(set1) x 6. Each row in this matrix corresponds to an index from set1, and we store up to six matches, with each integer value representing the index of a matching row from set2. We will use output_test_tt from now on to validate the correctness of the results produced by future kernels.\n\n# C++ Implementation With Torch load_inline\n\n## Setup and Boilerplate\n\nNow we'll write our own custom CUDA kernel and compile it using PyTorch's `load_inline` function. We'll be using the fantastic CUDA example from [Jeremy Howard's](https://twitter.com/jeremyphoward/) CUDA MODE [Lecture 3 presentation](https://youtu.be/4sgKnKbR-WE) and [notebook](https://github.com/cuda-mode/lectures/blob/main/lecture3/pmpp.ipynb). For a more detailed introduction to CUDA with PyTorch, I recommend checking out these resources.<br>\n<br>\nFirst we'll enable `CUDA_LAUNCH_BLOCKING` to get better compilation error messages and load the `wurlitzer` plugin, which allows c++ compilation messages to be surfaced up to python and into the notebook.\n\n`load_cuda` is a helper function for compiling custom cuda kernel's inline using torch's [`load_inline`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline) function.\n\n`cuda_begin` contains the boilerplate imports and utility functions we'll use for compiling our custom CUDA kernels. We import the PyTorch `torch extension` for integrating torch in our C++ code, stdio for printing, and CUDAException for error handling and reporting. Additionally, we create a few helper functions to ensure that the tensors passed to our custom CUDA kernel are on the GPU and contiguous in memory. We also include a simple helper function for ceiling division to calculate the number of blocks to launch.\n\n## C++ CUDA Kernel on GPU\n\nThe main advantage of using GPUs over CPUs is their ability to perform a large amount of work in parallel and largely independently. While CPU cores are typically faster than GPU cores, a powerful modern CPU might have 64 cores or 128 threads, whereas a modern GPU has thousands of 'cores' and can execute thousands of threads in parallel. This parallelism allows GPUs to outperform CPUs significantly on embarrassingly parallel workloads. The speedup varies depending on the workload and the specific CPU and GPU used, but generally, you can expect a speedup ranging from 10x to 1000x. <br>\n<br>\nWhen designing your CUDA program, you divide your workload into blocks and threads. A block is a group of threads, and blocks can be defined as 1, 2, or 3 dimensional to map to your workload for easier comprehension. Utilizing additional block dimensions does not affect performance. We will use a single block dimension, with a maximum of 2^31 blocks for dimension 0 (x) and 2^16 for dimensions 1 (y) and 2 (z). Each block can have a maximum of 1024 threads. It is typically beneficial to use a multiple of 32 when assigning the number of threads per block, as this is the size of a 'warp' in modern Nvidia GPUs. While the explanation of what a warp is is beyond the scope of this notebook, know that using a thread count divisible by 32 is almost always faster than not using a multiple of 32. <br>\n<br>\nYou might be wondering why 'blocks' are important and why Nvidia introduced this concept in CUDA. Blocks are crucial for several reasons. All threads within a block are executed on the same streaming multiprocessor (SM) and have access to shared memory, which is approximately 10 times faster than global memory. For example, an RTX 3090 has 48KB of shared memory per block and 82 SMs, with a total of 10,496 CUDA cores. Similarly, a T4 has 48KB of shared memory per block and 40 SMs, with a total of 2,560 CUDA cores.<br>\n<br>\nWith that brief introduction, let's dive into the code. First, we need to define our custom CUDA kernel. The `__global__` keyword specifies that the function is a CUDA kernel to be executed on the GPU. CUDA kernels are executed in parallel on the GPU and do not return anything. Instead, you need to pass a variable to the kernel where the output can be stored. In our function, we need to pass pointers to set1 (`s1`), set2 (`s2`), and the output tensor (`out`), as well as the lengths of `s1` and `s2`, and the maximum number of `s2` indices that can be stored per `s1` item in out. Please take a moment to read through the thoroughly and verbosely commented CUDA kernel below to understand its functionality.\n\nAdd the `find_overlap` c++ function declaration to the cpp_sources in the torch `load_inline` function. The function logic is defined above in the `cuda_src` and is the c++ function run on the CPU that calls our custom CUDA kernel.\n\nCompile the CUDA kernel using our `load_cuda` utility function, which calls PyTorch's `load_inline` with some predefined defaults. We'll time the compilation step, which typically takes 1-2 minutes. When writing your own CUDA kernel, this wait time can significantly slow down your iteration speed. Later in this post, we'll demonstrate other methods that compile much faster and allow for quicker iterations, which is highly recommended. It took me many iterations and a lot of waiting for compilation to get this code right.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.01 s, sys: 105 ms, total: 1.11 s\nWall time: 1min 33s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu121/inline_ext...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext.so\nLoading extension module inline_ext...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 680 ms, sys: 109 ms, total: 788 ms\nWall time: 1min 15s\n----------------------------------------------------------------------------------------------------\n```\n\nNow we'll inspect the newly compiled module. We can see that it has our newly defined `find_overlap` function, which is now available to call from Python. While inline_compile is relatively slow, it simplifies writing and running C++ CUDA code from a Python Jupyter notebook, which is quite convenient!\n\n```\n['__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'find_overlap']\n```\n\nWe'll quickly inspect the type, shape, and layout of the `set1` and `set2` tensors that we'll use to test our new CUDA kernel. It's always a good idea to examine your inputs and outputs when writing code, and Jupyter notebooks make this process easy.\n\n\n```\n(torch.float32,\n torch.float32,\n torch.Size([200000, 6]),\n torch.Size([200000, 6]),\n torch.strided,\n torch.strided)\n ```\n\nFinally, we'll time our new CUDA kernel. It takes 369 ms to run which is a 7,600,000 (7.6 Millon) times speedup over our original implementation and a 41 x improvement over our previous broadcasted pure pytorch CUDA implementation.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n369 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n85.3 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nFinally, we'll calculate the results one last time and store them in the `out` variable. This will allow us to compare against our previously calculated results to ensure they match and confirm that we have not introduced any logic errors.<br>\n<br>\nChecking the output is a crucial step. While implementing some kernels for this post, I observed large speedups that initially seemed promising. However, I later realized that these 'performance gains' were due to omitting a significant portion of the computation. This underscores the importance of thorough validation.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\nFinally, we'll check to make sure the results match, which they do.\n\n```\ntensor(True, device='cuda:0')\n```\n\n## C++ Single Threaded For Loop on CPU\n\nAfter implementing a working custom CUDA kernel, I was curious to see how fast a naive single-threaded (nested for loop) CPU-only C++ implementation would be. My aim with this implementation is purely for benchmarking purposes, to help build an intuition on the types of speedups you might expect with different implementations.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1.05 s, sys: 110 ms, total: 1.16 s\nWall time: 1min 36s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 1 and re-building as inline_ext_v1...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v1...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v1.so\nLoading extension module inline_ext_v1...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 702 ms, sys: 108 ms, total: 810 ms\nWall time: 1min 13s\n----------------------------------------------------------------------------------------------------\n```\n\nFor our CPU implementation, we need to create tensors on the CPU instead of using the GPU tensors we used previously for our CUDA kernel.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 39s, sys: 1.17 s, total: 3min 40s\nWall time: 3min 47s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 30s, sys: 1.51 s, total: 3min 32s\nWall time: 3min 28s\n----------------------------------------------------------------------------------------------------\n```\n\nAnd finally test that the output is correct.\n\n```\ntensor(True)\n```\n\n## C++ OMP Parallelization on CPU\n\nNext, we'll try parallelizing on the CPU using C++ OMP (OpenMP) parallelization to see how fast we can get on the CPU. The runtimes will vary **SIGNIFICANTLY** depending on the number of cores available and their speed.<br>\n<br>\nTo add OMP parallelization, we need to include a few additional compiler flags in our `load_cuda` function, which is why we're rewriting it here.\n\n\nChoose the number of threads that makes sense for your CPU. I have added a quick function below to determine the number of threads your CPU supports if you don't already know.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 598 ms, sys: 54.3 ms, total: 652 ms\nWall time: 53 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 2 and re-building as inline_ext_v2...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v2...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v2.so\nLoading extension module inline_ext_v2...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on different CPU's with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 5min 41s, sys: 1.35 s, total: 5min 42s\nWall time: 3min 32s\n3min 32s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n8.67 s - 18 threads\n6.91 s - 36 threads\n9.28 s - # threads not set\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 3min 37s, sys: 927 ms, total: 3min 38s\nWall time: 1min 48s\n1min 48s - 2 threads\nTODO: Try 12 threads\n```\n\n```\ntensor(True)\n```\n\nA *trick* I discovered in later experiments when writing CUDA kernels was to load the values from the set pointers into dedicated variables before performing the comparisons. This yielded faster runtimes. I applied the same technique to the CPU OMP implementation, and it improved performance here as well by loading the `s1` values into variables first.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 594 ms, sys: 62.5 ms, total: 656 ms\nWall time: 54 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 3 and re-building as inline_ext_v3...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v3...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v3.so\nLoading extension module inline_ext_v3...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 292 ms, sys: 56.7 ms, total: 349 ms\nWall time: 32.2 s\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on different CPU's with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 3min 27s, sys: 1.11 s, total: 3min 28s\nWall time: 2min 15s\n2min 15s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n6.45 s - 18 threads\n4.58 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 2min 26s, sys: 504 ms, total: 2min 27s\nWall time: 1min 12s\n1min 12s - 2 threads\n----------------------------------------------------------------------------------------------------\n```\n\n```\ntensor(True)\n```\n\nAs you can see, setting the `set1` pointer values to variables before performing the comparisons leads to a significant performance improvement!<br>\n<br>\nNow, let's try the same with the `set2` values that are being compared against. This is unlikely to improve performance, as it doesn't decrease the number of lookups like it does with `set1` in the outer loop.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 601 ms, sys: 59.6 ms, total: 661 ms\nWall time: 52.8 s\n\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\nThe input conditions for extension module inline_ext have changed. Bumping to version 4 and re-building as inline_ext_v4...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/inline_ext/build.ninja...\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nBuilding extension module inline_ext_v4...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -fopenmp -c /root/.cache/torch_extensions/py310_cu121/inline_ext/main.cpp -o main.o\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py310_cu121/inline_ext/cuda.cu -o cuda.cuda.o\n[3/3] c++ main.o cuda.cuda.o -shared -fopenmp -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v4.so\nLoading extension module inline_ext_v4...\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 309 ms, sys: 47.6 ms, total: 357 ms\nWall time: 31.5 s\n----------------------------------------------------------------------------------------------------\n```\n\nBelow are the results of several different runs on various CPUs with different numbers of threads.<br>\n<br>\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 9min 30s, sys: 2.43 s, total: 9min 33s\nWall time: 5min 31s\n5min 31s - 2 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n10.7 s - 36 threads\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 5min 34s, sys: 1.4 s, total: 5min 35s\nWall time: 2min 46s\n2min 46s - 2 threads\nTODO Try 12 threads\n----------------------------------------------------------------------------------------------------\n```\nAs you can see caching the set2 values actually made the performance worse.\n\n```\ntensor(True)\n```\n\n# Numba\n\nWaiting for these kernels to compile significantly slows down iteration speed, which is very important! Numba allows you to compile Python inline as well, but it is approximately 100 times faster than `torch`'s compilation via `load_inline`. Numba also allows you to run on both the CPU and GPU. I recommend starting with Numba instead of `torch`'s `load_inline` because it is easier to use and enables much faster iteration speed. Once your kernel is working and tuned, you can easily convert it to a C++ CUDA kernel if needed.\n\nWe'll start by importing the required Numba packages.\n\n\n`njit` allows you to compile CPU functions that don't rely on the Python interpreter, making it much faster than `jit`, which allows falling back to the Python interpreter. You almost always want to use `njit` if possible because it is much faster. `cuda.jit` is similar to `njit` but compiles your code into a CUDA kernel and runs it on the GPU.\n\n\n## CPU\n\nFirst, we'll start with a CPU implementation in Numba to establish a baseline. We'll use `float32` numbers initially and explore whether using different numeric types can improve performance for this problem.\n\n### float32\n\nConvert the previously loaded `int32` Numpy arrays to `float32`.\n\nVerify the shape and types of the new arrays.\n\nNow we'll create our CPU implementation. Notice that we are defining the types of the input and output parameters for the function. By specifying these types, Numba can compile this function to be much faster than a standard Python function.<br>\n<br>\nTake note of the three different comparison implementations below. I've commented out the first two iterations so you can see what I started with. These initial implementations were much slower than the final version. Implementation details can significantly impact your function's performance, so it's worth experimenting with different options.\n\n\nBenchmark Results:\n```\nImplementations:\n\nIteration 1\nif (s1l[:3] <= s2[j, 3:]).all() and (s1l[3:] >= s2[j, :3]).all():\n\nIteration 2\nif s1l0 <= s2l[3] and s1l1 <= s2l[4] and s1l2 <= s2l[5] and s1l3 >= s2l[0] and s1l4 >= s2l[1] and s1l5 >= s2l[2]:\n\nIteration 3\ns2l0 = s2l[0]\ns2l1 = s2l[1]\ns2l2 = s2l[2]\ns2l3 = s2l[3]\ns2l4 = s2l[4]\ns2l5 = s2l[5]\n\nif s1l0 <= s2l3 and s1l1 <= s2l4 and s1l2 <= s2l5 and s1l3 >= s2l0 and s1l4 >= s2l1 and s1l5 >= s2l2:\n\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 42s, sys: 193 ms, total: 1min 42s\nWall time: 1min 10s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\n\nIteration 1\nCPU times: user 1h 4min 37s, sys: 798 ms, total: 1h 4min 38s<br>\nWall time: 2min 16s\n\nIteration 2\nCPU times: user 7min 29s, sys: 95.5 ms, total: 7min 29s<br>\nWall time: 14.4 s\n\nIteration 3\nCPU times: user 52.2 s, sys: 12.2 ms, total: 52.2 s<br>\nWall time: 1.99 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n\nIteration 3\nCPU times: user 1min 55s, sys: 53.8 ms, total: 1min 55s\nWall time: 10.1 s\n----------------------------------------------------------------------------------------------------\n```\n\nAs you can see, the three successive implementation iterations yielded a compounded speedup of 9.4x and 7x. I believe the speedup between implementations 1 and 2 is because the NumPy library does not have to be called, but I'm unsure why there was such a significant speedup between implementations 2 and 3. I suspect it has to do with caching, but I'm not certain.\n\nAs always, we check out output to make sure it's correct.\n\n```\ntensor(True)\n```\n\n### int32\n\nNext we'll try out using the same exact logic only swapping `float32` for `int32` to see if it makes a difference.<br>\n<br>\nFirst we'll create our `int32` numpy arrays.\n\nAnd compile a new function that has the exact same logic as before, but the new type for the inputs as we discussed.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\nCPU times: user 1min 47s, sys: 265 ms, total: 1min 48s\nWall time: 1min 1s\n\narray([[     0,  35920,     -1,     -1,     -1,     -1],\n       [     0,      1,     -1,     -1,     -1,     -1],\n       [     1,      2,     -1,     -1,     -1,     -1],\n       ...,\n       [199996, 199997,     -1,     -1,     -1,     -1],\n       [199997, 199998,     -1,     -1,     -1,     -1],\n       [199998, 199999,     -1,     -1,     -1,     -1]], dtype=int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\n(Device: 18 Cores / 36 Threads)\nCPU times: user 49.1 s, sys: 16.1 ms, total: 49.1 s<br>\nWall time: 2.02 s\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\nCPU times: user 1min 51s, sys: 24.1 ms, total: 1min 51s\nWall time: 9.51 s\n```\nSwapping input types did not make any difference in the runtime.\n\n```\ntensor(True)\n```\n\n## GPU\n\n### Plotting\n\nWe need to start performing sweeps of hyperparameters, such as the number of threads per block, to determine the optimal values. Plotting the results will help us visualize which settings work best and how much impact changing these settings has. We'll create a function to plot the number of threads per block on the x-axis and runtime on the y-axis. Due to the significant variance in runtimes, we've added a parameter to limit the y-axis range to better visualize the best results. Additionally, we may want to visualize other variables besides threads per block, so if we specify a series key, it will create a separate plot color for each value, such as the number of items per thread.\n\n### Basic CUDA Kernel\n\nNow let's implement our first CUDA kernel with Numba. When defining the Numba CUDA kernel, use the `@cuda.jit` decorator. When calling this function, you need to pass the required CUDA kernel arguments in square brackets before passing the function arguments in parentheses: `kernel_func[grid_dim, block_dim, stream, dyn_shared_mem_size](standard_function_arguments)`. `grid_dim` and `block_dim` are always required, but additional CUDA arguments, such as shared memory size, can also be passed, which we'll cover shortly. For further information, please refer to the [Numba documentation](https://numba.readthedocs.io/en/stable/cuda/memory.html#dynamic-shared-memory).<br>\n<br>\nThis function should look familiar compared to the C++ CUDA kernel implementations from before. You'll notice that you can utilize some Pythonic implementation styles, which helps make the code more compact and readable.\n\nNext, we'll define our Python wrapper function to handle tasks similar to those managed by the C++ CPU functions earlier, such as determining the number of blocks.\n\nNext we'll create a simple helper function to create an initialized output array. We'll typically keep this initialization outside of the benchmark runs to make it easier to compare just the kernel runtimes.\n\nNow let's run our new numba kernel and see how well it performs.\n\nIsn't it nice that we don't have to wait long for it to compile? This allows for much faster iteration speed. Next, I'll implement some more advanced techniques, which would have been much more challenging to implement using `torch.load_inline` due to the slow iteration speed. It took me quite a few tries to get the more advanced implementations right.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n400 ms ± 4.68 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 36 Core Intel(R) Xeon(R) CPU E5-2697 v4, GPU: RTX 3090, Runtime Location: Other\nTODO XXXX\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n143 ms ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nNow we'll take a look at the output and check that it's correct.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\n```\ntensor(True, device='cuda:0')\n```\n\nLet's try another experiment with 1,024 threads per block.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n401 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 83.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\nIt looks like it doesn't make any difference.\n\nNow let's do a sweep to confirm what the optimal number of threads per block is.\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_0.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_0_zoom.png)\n\n### CUDA Kernel Using Shared Memory v1\n\nNext, we'll utilize shared memory to try and speed up our CUDA kernel. Shared memory is ultra-fast memory shared between all threads in a block, and it is approximately 10 times faster than global GPU memory. The Nvidia T4 and RTX 3090 have 48KB of shared memory available per thread block, compared to 16GB and 24GB, respectively, of global memory VRAM in each card.\n\nIn our CUDA kernel implementation, each thread computes all comparisons for one element from `set1` against all elements of `set2` by looping over all elements in `set2`. This means that in our original CUDA kernel implementation, each thread has to load all elements of `set2` from global memory. To speed this up, we can break `set2` into chunks so that each chunk fits into shared memory and each element in the chunk can be fetched from global memory only once per block by a single thread.\n\nWe'll add another loop to our kernel to loop through each `set2` chunk. In the outer loop, each thread will load a single value of `set2` into shared memory, and then in the inner loop, we'll compare the element from `set1` with each element from `set2` in shared memory. If our kernel is memory-bound, this should speed up our overall runtime.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n345 ms ± 15.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n129 ms ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n399 ms ± 3.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n114 ms ± 38.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n338 ms ± 5.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n122 ms ± 120 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% XXXXXXXXXXXXXXXXXXXXXX 64/64 [04:09<00:00,  3.55s/it]\n```\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_1.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_1_zoom.png)\n\nAs you can see using shared memory gave us a nice 20% speed improvement over our previous numba kernel that did not utilize shared memory.\n\n### CUDA Kernel Using Shared Memory v2\n\nNext we'll try a slight variation where we test if loading multiple set2 elements into shared memory per thread is faster (`cache_items_per_thread`) than only loading a single element per thread as in our v1 example.\n\nFirst we'll test it out to make sure it works and produces the correct result.\n\nBenchmark Results:\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: Tesla T4, Runtime Location: Google Colab\n417 ms ± 17.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n----------------------------------------------------------------------------------------------------\nHardware Runtime - CPU: 12 Core Intel(R) Xeon(R) CPU @ 2.20GHz, GPU: NVIDIA A100-SXM4-40GB, Runtime Location: Google Colab\n152 ms ± 16.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n----------------------------------------------------------------------------------------------------\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [38:27<00:00,  7.17s/it]\n100% 32/32 [07:05<00:00, 23.28s/it]\n64 1 361 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n64 2 353 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 25/25 [04:32<00:00, 18.24s/it]\n100% 21/21 [03:18<00:00, 15.38s/it]\n100% 18/18 [02:31<00:00, 13.07s/it]\n100% 16/16 [02:04<00:00, 11.74s/it]\n128 1 343 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 14/14 [01:41<00:00, 10.31s/it]\n100% 12/12 [01:17<00:00,  9.04s/it]\n100% 11/11 [01:09<00:00,  8.48s/it]\n100% 10/10 [00:57<00:00,  7.53s/it]\n100% 9/9 [00:50<00:00,  7.15s/it]\n100% 9/9 [00:47<00:00,  6.76s/it]\n100% 8/8 [00:42<00:00,  6.49s/it]\n100% 8/8 [00:40<00:00,  6.18s/it]\n256 1 339 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 7/7 [00:34<00:00,  5.68s/it]\n100% 7/7 [00:35<00:00,  5.81s/it]\n100% 6/6 [00:28<00:00,  5.36s/it]\n100% 6/6 [00:27<00:00,  5.11s/it]\n100% 6/6 [00:27<00:00,  5.07s/it]\n100% 5/5 [00:23<00:00,  5.07s/it]\n100% 5/5 [00:22<00:00,  4.87s/it]\n100% 5/5 [00:21<00:00,  4.76s/it]\n100% 5/5 [00:21<00:00,  4.71s/it]\n100% 4/4 [00:16<00:00,  4.24s/it]\n100% 4/4 [00:16<00:00,  4.23s/it]\n100% 4/4 [00:15<00:00,  4.15s/it]\n100% 4/4 [00:16<00:00,  4.37s/it]\n100% 4/4 [00:16<00:00,  4.34s/it]\n100% 4/4 [00:16<00:00,  4.37s/it]\n100% 4/4 [00:15<00:00,  4.10s/it]\n512 1 335 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n100% 3/3 [00:13<00:00,  4.50s/it]\n100% 3/3 [00:13<00:00,  4.47s/it]\n100% 3/3 [00:12<00:00,  4.24s/it]\n100% 3/3 [00:12<00:00,  4.23s/it]\n100% 3/3 [00:12<00:00,  4.25s/it]\n100% 3/3 [00:12<00:00,  4.24s/it]\n100% 3/3 [00:12<00:00,  4.12s/it]\n100% 3/3 [00:11<00:00,  3.94s/it]\n100% 3/3 [00:12<00:00,  4.06s/it]\n100% 3/3 [00:12<00:00,  4.04s/it]\n100% 2/2 [00:08<00:00,  4.08s/it]\n100% 2/2 [00:08<00:00,  4.05s/it]\n100% 2/2 [00:07<00:00,  3.76s/it]\n100% 2/2 [00:07<00:00,  3.75s/it]\n100% 2/2 [00:07<00:00,  3.80s/it]\n100% 2/2 [00:07<00:00,  3.82s/it]\n100% 2/2 [00:07<00:00,  3.90s/it]\n```\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nbest result 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n```\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_2.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_2_zoom.png)\n\nAs you can see there are a few results that are very close but the best one was where there was 1 cache item per thread which is effectively the same as our v1 algorithm.\n\n### CUDA Kernel Using Shared Memory v3 **fp16**\n\nNow we'll try the same algorithm but with `fp16` instead of `fp32`, which should cut our memory usage and bandwidth in half. Given the lower precision, we need to carefully check that the results are the same, as we're losing a significant portion (half) of our floating-point resolution.\n\nWhen declaring the size of the shared memory in this implementation, I initially forgot that it is specified in bytes when the kernel is called. However, when accessed from within the kernel, the bytes per object are taken into account. I was declaring the shared memory size without considering this, which led to continual invalid memory access errors that took quite a while to debug.\n\nNow we need to create fp16 torch tensors from our fp32 versions that we were using previously.\n\nNow we'll do a quick sanity check to ensure the results are correct.\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\nOutput is correct? tensor(False, device='cuda:0')\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [ 41295,  41296, 199998, 199999,     -1,     -1]], device='cuda:0',\n       dtype=torch.int32)\n```\n\nAs you can see the output is not correct. The question is did we make a mistake in our implementation or is the loss of precision when switching to float16 causing the problem.<br>\n<br>\nLet's compare the results with our test array `output_test_tt` and see if we initially spot any differences.\n\n```\ntensor([[     0,  35920,     -1,     -1,     -1,     -1],\n        [     0,      1,     -1,     -1,     -1,     -1],\n        [     1,      2,     -1,     -1,     -1,     -1],\n        ...,\n        [199996, 199997,     -1,     -1,     -1,     -1],\n        [199997, 199998,     -1,     -1,     -1,     -1],\n        [199998, 199999,     -1,     -1,     -1,     -1]])\n```\nAs you can see the in last index of set1 the results are different. There are additional matches `41295,  41296`. Let's look at those values.\n\nAs you can see they are different between the float16 and float32 versions:<br>\n```\n(tensor([348.5000, 306.7500, 239.3750, 348.5000, 307.0000, 239.5000],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.2500, 298.7500, 239.2500, 348.5000, 307.0000, 239.3750],\n        device='cuda:0', dtype=torch.float16),\n tensor([348.4000, 306.8360, 239.3870, 348.5600, 306.9960, 239.5470],\n        device='cuda:0'),\n tensor([348.2370, 298.6260, 239.2440, 348.3870, 307.0380, 239.3940],\n        device='cuda:0'))\n```\nNow let's use the check overlap function we built for the PyTorch GPU vectorize implementation, passing in values from both the `float16` and `float32` inputs to see if it produces different results. The `check_overlap_torch` function takes two 2-dimensional arrays and returns a list of tuples containing the indices of a match. Since we only want to compare the numbers in question, we need to expand the first dimension of our test elements, which we can do by indexing into them with `None`. This converts the 1D vector into a 1x6 array, which is required for this function.\n\n```\n[(0, 0)]\n```\n\n```\n[]\n```\n\nAs you can see, there is a match when using the `float16` values but not with the `float32` values. This indicates that `float16` is not suitable for our implementation and this use case. Half precision is very common in deep learning workloads and works fine in most cases, so it's something we should check. Although it is not suitable for our use case because it returns incorrect results, I am curious how it performs compared to the `float32` implementations, so we'll run it anyway to see how it works. This will allow you to compare from a benchmarking perspective, but it's a good lesson that it might not work in all cases and it's important to validate our outputs as we go.<br>\n<br>\nThis will take longer to run, as there are significantly more possible `cache_items_per_thread` to check because each element only takes up half the amount of shared memory, allowing us to effectively have twice as many elements in shared memory.\n\n\n```\nHardware Runtime - CPU: 2 Core Intel(R) Xeon(R) CPU @ 2.00GHz, GPU: Tesla T4, Runtime Location: Google Colab\n100% 61/61 [39:45<00:00,  7.12s/it]\n100% 64/64 [07:12<00:00, 12.09s/it]\n64 1 377 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 2 355 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n64 3 354 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 51/51 [04:44<00:00,  9.63s/it]\n100% 42/42 [03:20<00:00,  8.20s/it]\n96 1 349 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n96 3 347 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 36/36 [02:33<00:00,  7.05s/it]\n100% 32/32 [02:03<00:00,  6.32s/it]\n128 1 336 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n128 3 333 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 28/28 [01:42<00:00,  5.65s/it]\n100% 25/25 [01:20<00:00,  5.05s/it]\n100% 23/23 [01:12<00:00,  4.70s/it]\n100% 21/21 [01:01<00:00,  4.32s/it]\n100% 19/19 [00:53<00:00,  4.01s/it]\n100% 18/18 [00:48<00:00,  3.79s/it]\n100% 17/17 [00:44<00:00,  3.61s/it]\n100% 16/16 [00:40<00:00,  3.43s/it]\n100% 15/15 [00:37<00:00,  3.28s/it]\n100% 14/14 [00:33<00:00,  3.12s/it]\n100% 13/13 [00:31<00:00,  3.03s/it]\n100% 12/12 [00:26<00:00,  2.80s/it]\n100% 12/12 [00:27<00:00,  2.74s/it]\n100% 11/11 [00:25<00:00,  2.70s/it]\n100% 11/11 [00:24<00:00,  2.61s/it]\n100% 10/10 [00:21<00:00,  2.45s/it]\n100% 10/10 [00:22<00:00,  2.51s/it]\n100% 9/9 [00:18<00:00,  2.35s/it]\n100% 9/9 [00:18<00:00,  2.33s/it]\n100% 9/9 [00:18<00:00,  2.31s/it]\n100% 8/8 [00:16<00:00,  2.22s/it]\n100% 8/8 [00:16<00:00,  2.21s/it]\n100% 8/8 [00:16<00:00,  2.21s/it]\n100% 8/8 [00:15<00:00,  2.08s/it]\n512 1 331 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n100% 7/7 [00:15<00:00,  2.28s/it]\n100% 7/7 [00:15<00:00,  2.27s/it]\n100% 7/7 [00:14<00:00,  2.14s/it]\n100% 7/7 [00:15<00:00,  2.14s/it]\n100% 6/6 [00:12<00:00,  2.14s/it]\n100% 6/6 [00:12<00:00,  2.13s/it]\n100% 6/6 [00:12<00:00,  2.08s/it]\n100% 6/6 [00:11<00:00,  1.99s/it]\n100% 6/6 [00:12<00:00,  2.04s/it]\n100% 6/6 [00:12<00:00,  2.03s/it]\n100% 5/5 [00:10<00:00,  2.04s/it]\n100% 5/5 [00:10<00:00,  2.03s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.87s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.88s/it]\n100% 5/5 [00:09<00:00,  1.95s/it]\n100% 5/5 [00:09<00:00,  1.93s/it]\n100% 5/5 [00:09<00:00,  1.97s/it]\n100% 4/4 [00:07<00:00,  1.88s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.78s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.79s/it]\n100% 4/4 [00:07<00:00,  1.85s/it]\n100% 4/4 [00:07<00:00,  1.85s/it]\n100% 4/4 [00:07<00:00,  1.87s/it]\n100% 4/4 [00:07<00:00,  1.87s/it]\n100% 4/4 [00:07<00:00,  1.90s/it]\n100% 4/4 [00:07<00:00,  1.89s/it]\n100% 4/4 [00:06<00:00,  1.68s/it]\n100% 4/4 [00:06<00:00,  1.68s/it]\n```\n\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_fp16_3.png)\n![image](https://blog.matdmiller.com/posts/2024-02-15_custom_cuda_kernel_intro_and_benchmarks/benchmark_numba_gpu_smem_fp16_3_zoom.png)\n\nSurpringly this kernel is not faster than the float32 kernel. I would have expected this to be a memory bandwidth bound kernel but it does not seem to be.\n\n# Conclusion\n\nIn this notebook, we learned how to create CUDA kernels using several different methods, including PyTorch's `load_inline` function to compile a C++ CUDA kernel and Numba's `cuda.jit`. We also created several accelerated CPU implementations. We recorded the execution time for each option using a free Google Colab T4 instance and plotted the results. We achieved an 8.4 million times speedup over our initial pandas implementation. However, even more interesting was the speedup we achieved over the broadcast vectorized examples, which are likely among the fastest common Python implementations. These vectorized operations call highly optimized code, yet we managed to surpass them with our custom kernels.<br>\n<br>\nThe key takeaway from these experiments is that if your use case doesn't quite fit within existing optimized libraries, you can achieve significant speedups with custom CUDA kernels. Additionally, writing basic CUDA kernels is easier than you might have initially thought.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"toc":true,"self-contained":true,"output-file":"notebook.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":{"render-on-save":true},"theme":"lux","title-block-banner":true,"title":"CUDA MODE - Accelerate your code with massively parallel programming plus some other tricks","author":"Mat Miller","date":"05/31/2024","categories":["python","cuda","numba","massively parallel programming","accelerated computing"],"image":"cuda_mode_1.jpeg","resources":["*.zip","*.png","*.jpeg"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}